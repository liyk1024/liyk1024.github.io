<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Oracle存档日志空间占满处理]]></title>
    <url>%2F2020%2F09%2F10%2FOracle%E5%AD%98%E6%A1%A3%E6%97%A5%E5%BF%97%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[oracle链接报错 1# oracle无法连接，报错oracle：archiver error. Connect internal only, until freed 连接到超级管理员 1sqlplus / as sysdba 检查flash recovery area的使用情况 12SQL&gt; select * from V$FLASH_RECOVERY_AREA_USAGE;# 查看ARCHIVELOG的 PERCENT_SPACE_USED是否已经99.xx 计算flash recovery area已经占用的空间 12SQL&gt; select sum(percent_space_used)*3/100 from v$flash_recovery_area_usage;# 单位G 查看db_recovery_file_dest_size的空间 1SQL&gt; show parameter recover; 修改db_recovery_file_dest_size的空间 123# 大小根据硬盘空间ALTER SYSTEM SET DB_RECOVERY_FILE_DEST_SIZE=10g;# 退出登录即可 清理一下archivelog归档日志 123456789101112# cmd下，进入目标库rman target /远程：rman target sys/*****@orcl# 清理所有的归档日志crosscheck archivelog all;delete expired archivelog all;或者清理指定时间之前的日志（archivelog）：DELETE ARCHIVELOG ALL COMPLETED BEFORE &apos;SYSDATE-7&apos;;(指定删除7天前的归档日志)；或者直接如下：RMAN target sys/*****@orcl;DELETE NOPROMPT ARCHIVELOG ALL COMPLETED BEFORE &apos;SYSDATE-7&apos;; 12345# 查看归档日志列表：list archivelog all;# 查看失效的归档日志列表：list expired archivelog all;]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PVC扩容]]></title>
    <url>%2F2020%2F09%2F05%2FPVC%2F</url>
    <content type="text"><![CDATA[导读1本文介绍k8s中部署ceph-csi，并实现动态扩容pvc的操作 环境版本12345678[root@master kubernetes]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster Ready master 40d v1.18.0node1 Ready node 40d v1.18.0node2 Ready node 40d v1.18.0[root@master kubernetes]# ceph versionceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e) 配置ceph-csi 创建新存储池 1234567ceph osd pool create kube# 查看所有pool[root@master kubernetes]# ceph osd pool lsrbdkube# 或者使用 ceph osd lspools 新建用户 1234567891011121314# 为kube和ceph-csi创建用户ceph auth get-or-create client.kube mon &apos;profile rbd&apos; osd &apos;profile rbd pool=kube&apos; mgr &apos;profile rbd pool=kube&apos;[client.kube] key = AQBnz11fclr********zmr8ifftAHQbTw== # 后面配置参数中用到的key获取ceph auth get client.kube[root@master kubernetes]# ceph auth get client.kubeexported keyring for client.kube[client.kube] key = AQB/8jlfbtSk*******04A/Xp/eWOEx67pw== caps mon = &quot;allow r&quot; caps osd = &quot;allow class-read object_prefix rbd_children, allow rwx pool=kube&quot; 获取ceph集群信息 123456789[root@master kubernetes]# ceph mon dumpdumped monmap epoch 1epoch 1fsid 98564ee8-31bc-4ed6-9c31-cee****a8clast_changed 2020-08-16 22:11:42.371294created 2020-08-16 22:11:42.3712940: 192.168.100.11:6789/0 mon.master# 需要用到fsid：ceph集群的ID，监控阶段信息， 192.168.100.11:6789 拉取ceph-csi 1git clone https://github.com/ceph/ceph-csi 修改cm 123456789101112131415161718192021cd ceph-csi/deploy/rbd/kubernetes[root@master kubernetes]# cat csi-config-map.yaml---apiVersion: v1kind: ConfigMapdata: config.json: |- [ &#123; &quot;clusterID&quot;: &quot;98564ee8-31bc-4ed6-9c31-cee***ca8c&quot;, &quot;monitors&quot;: [ &quot;192.168.100.11:6789&quot; ] &#125; ]metadata: name: ceph-csi-config # 根据自身需要是否需要单独创建ns，本例全是默认ns下kubectl apply -f csi-config-map.yaml 新建secret 1234567891011# 创建csi-rbd-secret.yaml[root@master kubernetes]# cat csi-rbd-secret.yaml apiVersion: v1kind: Secretmetadata: name: csi-rbd-secretstringData: userID: kube userKey: AQB/8jlfbtSkIxAAb*******/Xp/eWOEx67pw== # kubectl apply -f csi-rbd-secret.yaml RBAC授权 1234# 如果有自定义ns，则在此修改$ sed -i &quot;s/namespace: default/namespace: ceph-csi/g&quot; $(grep -rl &quot;namespace: default&quot; ./)$ sed -i -e &quot;/^kind: ServiceAccount/&#123;N;N;a\ namespace: ceph-csi # 输入到这里的时候需要按一下回车键，在下一行继续输入 &#125;&quot; $(egrep -rl &quot;^kind: ServiceAccount&quot; ./) 123# 创建必须的 ServiceAccount 和 RBAC ClusterRole/ClusterRoleBinding 资源对象$ kubectl create -f csi-provisioner-rbac.yaml$ kubectl create -f csi-nodeplugin-rbac.yaml 123# 创建 PodSecurityPolicy$ kubectl create -f csi-provisioner-psp.yaml$ kubectl create -f csi-nodeplugin-psp.yaml 部署CSI sidecar1234567891011121314151617# 将 csi-rbdplugin-provisioner.yaml 和 csi-rbdplugin.yaml 中的 kms 部分配置注释1、csi-rbdplugin-provisioner.yaml注释如下：137 #- name: ceph-csi-encryption-kms-config138 # mountPath: /etc/ceph-csi-encryption-kms-config/178 #- name: ceph-csi-encryption-kms-config179 # configMap:180 # name: ceph-csi-encryption-kms-config2、csi-rbdplugin.yaml注释如下：89 #- name: ceph-csi-encryption-kms-config90 # mountPath: /etc/ceph-csi-encryption-kms-config/ 153 #- name: ceph-csi-encryption-kms-config154 # configMap:155 # name: ceph-csi-encryption-kms-config 部署csi-rbdplugin-provisioner 12kubectl create -f csi-rbdplugin-provisioner.yaml# 这里面包含了 6 个 Sidecar 容器: 包括 external-provisioner、external-attacher、csi-resizer、 和 csi-rbdplugin 。 部署rbd csi driver 12kubectl ceph-csi create -f csi-rbdplugin.yaml# Pod 中包含两个容器：CSI node-driver-registrar 和CSI RBD Driver 123456[root@master kubernetes]# kubectl get pod |grep csi-rbdplugincsi-rbdplugin-c28gp 3/3 Running 0 8hcsi-rbdplugin-provisioner-7dfbc5fc7d-frwxz 6/6 Running 0 8hcsi-rbdplugin-provisioner-7dfbc5fc7d-q5ljg 6/6 Running 0 8hcsi-rbdplugin-provisioner-7dfbc5fc7d-v8v49 6/6 Running 0 8hcsi-rbdplugin-x5rkk 3/3 Running 0 8h 创建Storageclass 123456789101112131415161718192021222324252627[root@master kubernetes]# cat storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: csi-rbd-scprovisioner: rbd.csi.ceph.comparameters: clusterID: 98564ee8-31bc-4ed6-9c31-cee1e15eca8c pool: kube imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret csi.storage.k8s.io/provisioner-secret-namespace: default csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret csi.storage.k8s.io/controller-expand-secret-namespace: default csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret csi.storage.k8s.io/node-stage-secret-namespace: default csi.storage.k8s.io/fstype: ext4reclaimPolicy: DeleteallowVolumeExpansion: truemountOptions: - discard# 这里的clusterID对应上面查询的fsid# imageFeatures用来确定创建的image特性，如不指定就会使用RBD内核中的特征列表，但linux不一定支持所有特征所以这里需要限制下。kubectl apply -f storgeclass.yaml[root@master kubernetes]# kubectl get sc |grep csicsi-rbd-sc rbd.csi.ceph.com Delete Immediate true 8h 试用ceph-csiKubernetes通过persistentVolume子系统为用户和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来，其中PV（PersistentVolume）是实际的存储，pvc（PersistentVolumeClaim） 是用户对存储的请求。 1234567891011121314151617181920212223242526272829303132333435363738# 试用ceph-csi仓库自带的示例来演示cd ceph-csi/examples/rbd# 创建pvc，自动申请对应pvkubectl apply -f pvc.yaml[root@master rbd]# cat pvc.yaml ---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: rbd-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: csi-rbd-sc# 创建podkubectl apply -f pod.yaml[root@master rbd]# cat pod.yaml ---apiVersion: v1kind: Podmetadata: name: csi-rbd-demo-podspec: containers: - name: web-server image: nginx volumeMounts: - name: mypvc mountPath: /var/lib/www/html volumes: - name: mypvc persistentVolumeClaim: claimName: rbd-pvc readOnly: false 12# 查看pvc和申请成功的pvkubectl get pvc,pv pod测试 12345678kubectl exec -it csi-rbd-demo-pod bashroot@csi-rbd-demo-pod:/# cd /var/lib/www/root@csi-rbd-demo-pod:/var/lib/www# ls -ltotal 4drwxrwxrwx 3 root root 4096 Sep 14 09:09 htmlroot@csi-rbd-demo-pod:/var/lib/www# echo &quot;https://fuckcloudnative.io&quot; &gt; sealos.txtroot@csi-rbd-demo-pod:/var/lib/www# cat sealos.txthttps://fuckcloudnative.io 查看ceph pool中的rbd images 123456789101112131415161718192021222324252627282930[root@master rbd]# rbd ls -p kubecsi-vol-60e0cf1b-fd5d-11ea-82ec-c2a930d7d7e0csi-vol-928bae1f-fd5e-11ea-82ec-c2a930d7d7e0# 2个的原因是之前扩容占用了个[root@master rbd]# rbd info csi-vol-60e0cf1b-fd5d-11ea-82ec-c2a930d7d7e0 -p kuberbd image &apos;csi-vol-60e0cf1b-fd5d-11ea-82ec-c2a930d7d7e0&apos;: size 2048 MB in 512 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.25a0fa6552204 format: 2 features: layering flags: [root@master rbd]# rbd info csi-vol-928bae1f-fd5e-11ea-82ec-c2a930d7d7e0 -p kuberbd image &apos;csi-vol-928bae1f-fd5e-11ea-82ec-c2a930d7d7e0&apos;: size 3072 MB in 768 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.25a0fb1419ae9 format: 2 features: layering flags:# 可以看到对 image 的特征限制生效了，这里只有layering。# 一个2G，一个3G# 实际上这个 image 会被挂载到 node 中作为一个块设备，可以通过 rbd 命令查看映射信息：[root@node1 ~]# rbd showmappedid pool image snap device 0 kube csi-vol-928bae1f-fd5e-11ea-82ec-c2a930d7d7e0 - /dev/rbd0 [root@node1 ~]# lsblk -l |grep rbdrbd0 253:0 0 3G 0 disk /var/lib/kubelet/pods/a6d918de-526b-43c0-ae2e-1a271dd4d471/volumes/kubernetes.io~csi/pvc-ca05d200-f7f7-4b1a-ac05-7be3eb71bd6f/mount 扩容pvc 扩展pvc 12# 直接修改yaml文件，将存储大小数字修改后apply# 或者kubectl edit pvc csi-rbd-pvc,编辑pvc.spec.resource.requests.storage，修改完保存退出。 扩容生效 1# csi driver会自动在存储端进行扩容，观察pvc状态变化，一般得耗时几秒钟 12# 如上图，可看到pvc成功扩展，describe pvc可查看到pvc resize# 也可进入pod里查看，挂载点是否已扩容生效]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[配置管理-ConfigMap和Secret]]></title>
    <url>%2F2020%2F08%2F20%2F%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86-ConfigMap%E5%92%8CSecret%2F</url>
    <content type="text"><![CDATA[配置管理需解决的问题： 可变的配置参数如何打包到镜像中 敏感信息防止泄露，如密码、密钥等 每次配置更新都要重新打包，镜像版本过多也给镜像存储管理带来负担 定制化太严重、可扩展能力差，且不容易复用 ConfiMap 作用：主要用来保存一些非敏感数据，如环境变量、命令行参数或挂载到存储卷中。 ConfigMap通过键值对来存储信息，是个namespace级别的资源，kubectl操作时可简写成cm 元数据使用data: 后跟相关键值 支持键值对（值是数字需用双引号,即需用字符串来表示）、多行的文本（键后使用| 跟多行文本） 12345678910111213141516171819202122232425262728$ cat cm-demo-mix.yamlapiVersion: v1kind: ConfigMapmetadata: name: cm-demo-mix # 对象名字 namespace: demo # 所在的命名空间data: # 这是跟其他对象不太一样的地方，其他对象这里都是spec # 每一个键都映射到一个简单的值 player_initial_lives: &quot;3&quot; # 注意这里的值如果数字的话，必须用字符串来表示 ui_properties_file_name: &quot;user-interface.properties&quot; # 也可以来保存多行的文本 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true $ cat cm-demo-all-env.yamlapiVersion: v1kind: ConfigMapmetadata: name: cm-demo-all-env namespace: demodata: SPECIAL_LEVEL: very SPECIAL_TYPE: charm 可以通过kubectl create cm基于目录、文件或者字面值来创建。123kubectl create configmap &lt;map-name&gt; &lt;data-source&gt;# &lt;map-name&gt; 是要设置的ConfigMap名称，&lt;data-source&gt; 是要从中提取数据的目录、文件或者字面值。# 可以使用kubectl describe 或者 kubectl get 获取有关 ConfigMap 的信息。 基于目录创建ConfigMap123使用 kubectl create configmap 基于同一目录中的多个文件创建 ConfigMap。 当你基于目录来创建 ConfigMap 时，kubectl 识别目录下基本名可以作为合法键名的 文件，并将这些文件打包到新的 ConfigMap 中。普通文件之外的所有目录项都会被 忽略（例如，子目录、符号链接、设备、管道等等）。 12345678910111213141516171819202122232425262728293031323334# 创建本地目录mkdir -p configure-pod-container/configmap/# 将实例文件下载到 `configure-pod-container/configmap/` 目录wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.propertieswget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties# 创建 configmapkubectl create configmap game-config --from-file=configure-pod-container/configmap/# 以上命令将 configure-pod-container/configmap 目录下的所有文件，也就是 game.properties 和 ui.properties 打包到 game-config ConfigMap 中。kubectl describe configmaps game-configName: game-configNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====game.properties:----enemies=alienslives=3enemies.cheat=trueenemies.cheat.level=noGoodRottensecret.code.passphrase=UUDDLRLRBABASsecret.code.allowed=truesecret.code.lives=30ui.properties:----color.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNice 123456789101112131415161718192021222324kubectl get configmaps game-config -o yamlapiVersion: v1kind: ConfigMapmetadata: creationTimestamp: 2016-02-18T18:52:05Z name: game-config namespace: default resourceVersion: &quot;516&quot; selfLink: /api/v1/namespaces/default/configmaps/game-config uid: b4952dc3-d670-11e5-8cd0-68f728db1985data: game.properties: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice 基于文件创建ConfigMap12# 可以使用 kubectl create configmap 基于单个文件或多个文件创建 ConfigMap。kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties Pod 必须和 ConfigMap 在同一个 namespace 下面;在创建 Pod 之前，请务必保证 ConfigMap 已经存在，否则 Pod 创建会报错 ConfigMap的几大使用场景 命令行参数 环境变量，可以只注入部分变量，也可全部注入 挂载文件，可以是单个文件也可以是所有键值对，用每个键值作为文件名。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ cat cm-demo-pod.yamlapiVersion: v1kind: Podmetadata: name: cm-demo-pod namespace: demospec: containers: - name: demo image: busybox:1.28 command: - &quot;bin/sh&quot; - &quot;-c&quot; - &quot;echo PLAYER_INITIAL_LIVES=$PLAYER_INITIAL_LIVES &amp;&amp; sleep 10000&quot; env: # 定义环境变量 - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的 valueFrom: configMapKeyRef: name: cm-demo-mix # 这个值来自ConfigMap key: player_initial_lives # 需要取值的键 - name: UI_PROPERTIES_FILE_NAME valueFrom: configMapKeyRef: name: cm-demo-mix key: ui_properties_file_name envFrom: # 可以将 configmap 中的所有键值对都通过环境变量注入容器中 - configMapRef: name: cm-demo-all-env volumeMounts: - name: full-config # 这里是下面定义的 volume 名字 mountPath: &quot;/config&quot; # 挂载的目标路径 readOnly: true - name: part-config mountPath: /etc/game/ readOnly: true volumes: # 您可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中 - name: full-config # 这是 volume 的名字 configMap: name: cm-demo-mix # 提供你想要挂载的 ConfigMap 的名字 - name: part-config configMap: name: cm-demo-mix items: # 我们也可以只挂载部分的配置 - key: game.properties path: properties Secret 用secret保存一些敏感数据信息，比如密码、密钥、token等 用法和ConfigMap基本一致，都可以用作环境变量或者文件挂载 123456# 使用kubectl来创建secret，支持三种类型kubectl create secret -hAvailable Commands: docker-registry Create a secret for use with a Docker registry generic Create a secret from a local file, directory or literal value tls Create a TLS secret 123456789101112131415# 创建一个secret来保存私有容器仓库的身份信息$ kubectl create secret -n demo docker-registry regcred \ --docker-server=yourprivateregistry.com \ --docker-username=liyk \ --docker-password=mypassw0rd \--docker-email=liyk@example.com secret/regcred created $ kubectl get secret -n demo regcred NAME TYPE DATA AGE regcred kubernetes.io/dockerconfigjson 1 28s # 可看出创建出来的secret类型是：kubernetes.io/dockerconfigjson$ kubectl get secret regcred -n demo --output=&quot;jsonpath=&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode&#123;&quot;auths&quot;:&#123;&quot;yourprivateregistry.com&quot;:&#123;&quot;username&quot;:&quot;allen&quot;,&quot;password&quot;:&quot;mypassw0rd&quot;,&quot;email&quot;:&quot;allen@example.com&quot;,&quot;auth&quot;:&quot;YWxsZW46bXlwYXNzdzByZA==&quot;&#125;&#125;&#125;# Secret 保存的数据都是通过 base64 加密后的数据。 12345678910111213141516171819202122# 平时使用较为广泛的还有另外一种Opaque类型的 Secret$ cat secret-demo.yamlapiVersion: v1kind: Secretmetadata: name: dev-db-secret namespace: demotype: Opaquedata: # 这里的值都是 base64 加密后的 password: UyFCXCpkJHpEc2I9 username: ZGV2dXNlcg==# 也可以通过如下等价的 kubectl 命令来创建出来$ kubectl create secret generic dev-db-secret -n demo \ --from-literal=username=devuser \ --from-literal=password=&apos;S!B\*d$zDsb=&apos; # 或通过文件来创建对象$ echo -n &apos;username=devuser&apos; &gt; ./db_secret.txt$ echo -n &apos;password=S!B\*d$zDsb=&apos; &gt;&gt; ./db_secret.txt$ kubectl create secret generic dev-db-secret -n demo \ --from-file=./db_secret.txt 12345678910111213141516# 在pod中使用secret$ cat pod-secret.yamlapiVersion: v1kind: Podmetadata: name: secret-test-pod namespace: demospec: containers: - name: demo-container image: busybox:1.28 command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] envFrom: - secretRef: name: dev-db-secret restartPolicy: Never 写在最后123ConfigMap 和 Secret 是 Kubernetes 常用的保存配置数据的对象，你可以根据需要选择合适的对象存储数据。通过 Volume 方式挂载到 Pod 内的，kubelet 都会定期进行更新。但是通过环境变量注入到容器中，这样无法感知到 ConfigMap 或 Secret 的内容更新。 如果业务自身支持reload，可直接定期进行reload（可以配合readlinessProbe一起使用）。比如nginx -s reload可以通过inotify感知到文件更新。 如果业务没上述能力，可以采用滚动升级的方式。借助开源工具Reloader，一旦发现configmap和secret更新就自动触发对deployment或statefulset等工作负载对象进行滚动升级。 （https://github.com/stakater/Reloader）]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Oracle修改表空间大小]]></title>
    <url>%2F2020%2F08%2F19%2FOracle%E4%BF%AE%E6%94%B9%E8%A1%A8%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[oracle表空间满了就影响正常操作 12oracle数据库被划分成称为表空间的逻辑区域，形成oracle数据库的逻辑结构。一个oracle数据库能有1个或多个表空间，而1个表空间则能对应1个或多个物理的数据库文件。表空间是oracle数据库恢复的最小单位，容纳着许多数据库实体、视图、索引、聚簇、回退段和临时段等。 查看当前库的db_block_size大小1234表空间数据文件容量与db_block_size大小有关，在初始建库时db_block_size要根据实际情况设置，一般设置为4k或者8k。设置太大话一次读出的数据有部分是没用的，会拖慢数据库的读写时间，同时增加无必要的IO操作。而对于数据仓库和ERP方面的应用，每个事务处理的数据量很大，所以DB_BLOCK_SIZE一般设置得比较大，一般为8K，16K或者32K，此时如果DB_BLOCK_SIZE小的话，那么I/O自然就多，消耗太大。ORACLE的物理文件最大只允许4194304个数据块 12# 查看db_block_size值select value from v$parameter where name=&apos;db_block_size&apos;; 2种方法增加表空间大小 扩容当前表空间大小（只有表空间没达上限可直接扩容） 123456# 查询表空间数据文件select FILE_NAME,BYTES/1024/1024,MAXBYTES/1024/1024 fsize from dba_data_files where tablespace_name like &apos;FLUX_WMS&apos;;# 修改表空间大小alter database datafle &apos;D:\FLUX_TABLESPACE\FLUX_WMS.DBF&apos; resize 30720M;# 设置自动扩展alter database datafile &apos;D:\FLUX_TABLESPACE\FLUX_WMS.DBF&apos;AUTOEXTEND ON NEXT 1024M; 新增表空间文件 1234# 查看表空间详情select f.* from dba_data_files f where f.tablespace_name=&apos;FLUX_WMS&apos;;# 新增FLUX_WMS_2.DBF初始10G,并设置自动扩容每次扩容1G，上限32Galter tablespace FLUX_WMS add datafile &apos;D:\FLUX_TABLESPACE\FLUX_WMS_2.DBF&apos; size 10240M autoextend on next 1024M maxsize 30720M; 查看当前所有表空间情况123456789101112131415select a.tablespace_name,c.allocation_type,c.segment_space_management, case mod(c.initial_extent,1024*1024) when 0 then c.initial_extent/1024/1024||&apos;M&apos; else c.initial_extent/1024||&apos;K&apos; end initial_extent, a.total_Mbytes,a.total_Mbytes - b.free_Mbytes used_Mbytes,b.free_Mbytes, trunc(b.free_Mbytes/a.total_Mbytes * 100,2) pct_free,null dummyfrom ( select tablespace_name,sum(bytes)/1024/1024 total_MBytes from dba_data_files group by tablespace_name) a, ( select tablespace_name,sum(bytes)/1024/1024 free_Mbytes from dba_free_space group by tablespace_name) b, dba_tablespaces cwhere a.tablespace_name = b.tablespace_name(+) and a.tablespace_name = c.tablespace_name(+);]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s+ceph实现动态pvc]]></title>
    <url>%2F2020%2F08%2F10%2Fk8s-ceph%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81pv%2F</url>
    <content type="text"><![CDATA[将现有ceph集群用作k8s动态创建持久性存储动态pvc配置步骤 所有节点安装ceph-common 123yum install -y ceph-common# 一般使用ceph-deploy install会安装此软件包# 需要使用kubelet使用rbd命令map附加rbd创建的image 创建osd pool 123456789101112ceph osd create pool $&#123;poolname&#125; $&#123;pg_num&#125; $&#123;pgp_num&#125;pg_num是指定创建的pg的个数，会有一组编号，然后pgp_num就是控制pg到osd的映射分布。一般最好将pgp_num设置成一样。这里强制选择pg_num和pgp_num，因为ceph集群不能自动计算pg数量。下面有一些官方建议的pg使用数量：小于5个osd设置pg_num为1285到10个osd设置pg_num为51210到50个osd设置pg_num为1024如果超过50个osd你需要自己明白权衡点，并且能自行计算pg_num的数量 (OSDs * 100) Total PGs = ------------- pool size 12345678910# 创建ceph osd pool create kube 128 128# 查看ceph osd pool ls 或者 ceph osd lspools# 删除ceph osd pool delete kube 128 128# 重命名ceph osd pool rename kube new-name# 查看使用情况，空间ceph df 使用扩展存储卷插件 123456789# 由于kube-controller-manager使用容器运行，此容器不包含rbd，因此kube-controller-manager在创建pv时无法调用erbd。# 要么添加ceph-common到hyperkube image中；要么使用扩展存储卷插件。git clone https://github.com/kubernetes-incubator/external-storage.gitcd external-storage/ceph/rbd/deploy/# 提供rbac和no-rbac两种方式，因为我们一般搭建k8s集群都开启了rbac认证，所以裁员rbac方式来创建该deployment# 此处ClusterRoleBinding 默认绑定 namespace: default，如果要修改为其他 namespace，对应的 storageClass 中的adminSecretNamespace 也需要对应修改kubectl apply -f rbac/ 创建密钥，用于k8s认证 1234567891011121314151617# 创建osd pool，前面已创建此处可忽略ceph osd pool create kube 128 128# 创建k8s访问ceph的用户在ceph的mon或者admin节点ceph auth get-or-create client.kube mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=kube&apos; -o ceph.client.kube.keyring# 查看admin和kube的key# 直接使用kubect create创建就不用base64编码。如果是写在yaml文件里则需要“| base64”编码后的字符串ceph auth get-key client.adminceph auth get-key client.kube# 创建admin secretkubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=&apos;AQCfPjlfTFHBChAAMU/oht*****igQTg==&apos; \--namespace=kube-system# 创建user secret,用于创建pvc访问ceph的secretkubectl create secret generic ceph-user-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=&apos;AQBZK3VbTN/QOBAAIYi******lunOg==&apos; \--namespace=ceph 123456789# 或者使用yaml文件创建secret[root@master liyk]# cat ceph-secret-admin.yamlapiVersion: v1kind: Secretmetadata: name: ceph-secret-admintype: &quot;kubernetes.io/rbd&quot; data: key: QVFDZlBqbGZURkhCQ2hBQU1VL29odHpJ********c9PQ== 12 # 创建k8s访问ceph用户在ceph的mon或者admin节点ceph auth get-or-create client.kube mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=kube&apos; -o ceph.client.kube.keyring 配置storageclass 12345678910111213141516171819[root@master liyk]# cat rbd-storageClass.yaml kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: rbd#provisioner: kubernetes.io/rbdprovisioner: ceph.com/rbdallowVolumeExpansion: true # 开启pvc扩容parameters: monitors: 192.168.100.11:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret-admin fsType: ext4 imageFormat: &quot;2&quot; imageFeatures: layering 创建pvc 123456789101112[root@master liyk]# cat rbd-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ceph-rbd-pvcspec: accessModes: - ReadWriteOnce storageClassName: rbd resources: requests: storage: 1Gi 查看pvc和自动创建的pv 123456[root@master liyk]# kubectl get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/pvc-ca6c5df5-d91d-40e5-b481-c4416c4dfc8b 1Gi RWO Delete Bound default/ceph-rbd-pvc rbd 9m43sNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/ceph-rbd-pvc Bound pvc-ca6c5df5-d91d-40e5-b481-c4416c4dfc8b 1Gi RWO rbd 9m44s]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[FastApi基础]]></title>
    <url>%2F2020%2F07%2F23%2FFastApi%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[简介FastAPI是一个用于构建API的现代、快速(高性能)的web框架，使用python3.6+并给予标准的类型提示。 关于特性 快速：可与 NodeJS 和 Go 比肩的极高性能（归功于 Starlette 和 Pydantic）。最快的 Python web 框架之一。 高效编码：提高功能开发速度约 200％至 300％。 更少bug：减少约 40％ 的人为（开发者）导致错误。(自动补全相关) 智能：极佳的编辑器支持。处处皆可自动补全，减少调试时间。 简单：设计的易于使用和学习，阅读文档的时间更短。 简短：使代码重复最小化。通过不同的参数声明实现丰富功能。bug 更少。 健壮：生产可用级别的代码。还有自动生成的交互式文档。 标准化：基于（并完全兼容）API 的相关开放标准：OpenAPI (以前被称为 Swagger) 和 JSON Schema。 安装123pip install fastapi# ASGI服务器pip install uvcorn 示例1234567# main1.pyfrom fastapi import FastAPIliyk = FastAPI()@liyk.get(&quot;/&quot;)def index(): return &quot;哈哈123&quot; 运行1234uvicorn main1:liyk --reload --port=9000 --host=0.0.0.0# main1:：main1.py文件# liyk：在main1.py文件中通过liyk = FastAPI()创建的对象# --reload：让服务器在更新代码后重新启动，仅在开发时使用该选项。 交互式API文档12访问：http://127.0.0.1:8000/docs就会有自动生成的交互式API文档 URL规则定义 url中定义参数 1234567# fastapi中使用&#123;参数名&#125;的方式来表示参数from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/book/&#123;book_id&#125;&quot;)def book_detail(book_id): return &#123;&quot;book_id&quot;: book_id&#125; 有类型的参数 123456789# 在定义参数的时候还可以指定具体的类型@app.get(&quot;/book/&#123;book_id&#125;&quot;)def read_book(book_id: int): return &#123;&quot;book_id&quot;: book_id&#125;# 作用1.函数中参数根int,指明这个参数是什么类型2.fastapi，约束参数类型3.fastapi，会自动将book_id参数从字符串类型 预设值 1234567891011from fastapi import FastAPIfrom enum import Enum # 设定预设值app = FastAPI()class GenderEnum(str, Enum): male = &quot;male&quot; female = &quot;female&quot;@app.get(&quot;/gender/&#123;gender_name&#125;&quot;)def gender(gender_name: GenderEnum): return &#123;&quot;性别&quot;: gender_name&#125;]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Oracle归档模式的打开与关闭]]></title>
    <url>%2F2020%2F07%2F16%2FOracle%E5%BD%92%E6%A1%A3%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%89%93%E5%BC%80%E4%B8%8E%E5%85%B3%E9%97%AD%2F</url>
    <content type="text"><![CDATA[oracle默认存档模式为关闭状态，要实现数据的热备份需要打开存档模式。并且数据库的存档模式要在mount实例中进行，且数据库不能处理open状态。 打开归档模式 使用dba角色登录 1sqlplus / as sysdba 查看当前数据库的存档模式 1archive log list 操作打开123456789# 使用SQL# 修改系统的日志方式为存档模式alter system set log_archive_start=true scope=spfile;# 关闭数据库，因为不能在open状态下操作shutdown immediate;# 启动mount实例，但是不启动数据库startup mount;# 更改数据库未存档模式alter database archivelog; 查看当前归档状态1archive log list 打开数据库1alter database open; 关闭归档模式 存档模式需在mount实例中进行，且数据库要关闭状态。关闭存档模式需先查看数据库当前状态，关闭数据库启动mount实例，才可进行操作。 查看当前数据库状态1select status from v$instance; 关闭相关操作 12345678910# 关闭数据库，因为数据库当前状态为openshutdown immediate;# 启动mount实例startup mount;# 改变数据库存档模式为非存档模式alter database noarchivelog;# 查看当前归档状态，已更改为非存档模式archive log list# 查看当前数据库状态，为mount状态select status from v$instance; 打开数据库 123alter database open;# 再次查看数据库状态，已为open状态select status from v$instance; oracle几种启动关闭方式区别1234567startup # 等于一下三个命令startup nomountalter database mountalter database open# 如果数据库以startup mount方式启动，那只需一条命令即可打开数据库alter database open; 1234# 有三种关闭方式shutdown normal # 正常方式关闭数据库shutdown immediate # 立即方式关闭数据库，当使用shutdown不能关闭数据库时，shutdown immediate可完成数据库关闭的操作。shutdown abort # 直接关闭数据库，正在访问数据库的会话会被突然终止。如果数据库中有大量操作正在执行，这时执行shutdown abort后，重新启动数据库需要很长时间。]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Oracle修改最大连接数]]></title>
    <url>%2F2020%2F07%2F10%2FOracle%E4%BF%AE%E6%94%B9%E6%9C%80%E5%A4%A7%E8%BF%9E%E6%8E%A5%E6%95%B0%2F</url>
    <content type="text"><![CDATA[有时遇到数据库连接数够，客户端连接间歇性失败。会报错ORA-12519 TNS:no appropriate service handler found，此时我们需要修改oracle的最大会话数。 DBA角色登录数据库 1sqlplus / as sysdba 查看当前数据库连接数 123456# 查看当前数据库进程的连接数select count(*) from v$process;# 查看当前session连接数select count(*) from v$session;# 查看当前并发连接数select count(*) from v$session where status=&apos;ACTIVE&apos;; 查看数据库允许的最大连接数 123select value from v$parameter where name =&apos;processes&apos;;# 查看processes汇总的信息show parameter processes 修改数据库最大连接数 12# 此处修改4000，根据情况设置alter system set processes = 4000 scope = spfile; 重启数据库生效 1234shutdown immediate; # 关闭数据库startup # 重启数据库show parameter processes # 查看修改后的参数 查看当前哪些用户正在使用数据 1234select osuser, a.username, cpu_time/executions/1000000||&apos;s&apos;, b.sql_text, machinefrom v$session a, v$sqlarea bwhere a.sql_address =b.address order by cpu_time/executions desc;]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django中使用celery]]></title>
    <url>%2F2020%2F06%2F28%2FDjango%E4%B8%AD%E4%BD%BF%E7%94%A8celery%2F</url>
    <content type="text"><![CDATA[项目目录结构1234# 创建django项目django-admin startproject django_celery# pycharm打开django_celery项目，创建apppython manage.py startapp myapps 配置文件 123# mycelery/config.pybroker_url = &apos;redis://:xxx@111@43.254.x.x:6379/14&apos;result_backend = &apos;redis://:xxx@111@43.254.x.x:6379/15&apos; 任务文件 12345678910111213141516171819# mycelery/sms/tasks.pyfrom mycelery.main import appimport timeimport logginglog = logging.getLogger(&quot;django&quot;)@app.task # name表示设置任务的名称，如果不填写，则默认使用函数名做为任务名def send_sms(mobile): &quot;&quot;&quot;发送短信&quot;&quot;&quot; print(&quot;向手机号%s发送短信成功!&quot;%mobile) time.sleep(5) return &quot;send_sms OK&quot;@app.task # name表示设置任务的名称，如果不填写，则默认使用函数名做为任务名def send_sms2(mobile): print(&quot;向手机号%s发送短信成功2!&quot; % mobile) time.sleep(5) return &quot;send_sms2 OK&quot; 主程序 1234567891011121314151617181920# mycelery/main.pyimport osfrom celery import Celery# 创建celery实例对象app = Celery(&quot;djcelery&quot;)# 把celery和django进行组合，识别和加载django的配置文件os.environ.setdefault(&apos;DJANGO_SETTINGS_MODULE&apos;, &apos;celeryPros.settings.dev&apos;)# 通过app对象加载配置app.config_from_object(&quot;mycelery.config&quot;)# 加载任务# 参数必须必须是一个列表，里面的每一个任务都是任务的路径名称# app.autodiscover_tasks([&quot;任务1&quot;,&quot;任务2&quot;])app.autodiscover_tasks([&quot;mycelery.sms&quot;,])# 启动Celery的命令# 强烈建议切换目录到mycelery同级目录下启动# celery -A mycelery.main worker --loglevel=info django视图 12345678# django_celery/urls.pyfrom django.urls import pathfrom myapps import viewsurlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), path(&apos;test/&apos;, views.test),] 12345678910111213141516171819202122# myapps/views.pyfrom django.shortcuts import render,HttpResponsefrom mycelery.sms.tasks import send_sms,send_sms2from datetime import datetime,timedelta# Create your views here.def test(request): # 异步任务 # 声明一个和celery一样的任务函数，可以用导包来解决 # send_sms.delay(&quot;111&quot;) # send_sms2.delay(&quot;112&quot;) # 定时任务 ctime = datetime.now() # 默认用utc时间 utc_time = datetime.utcfromtimestamp(ctime.timestamp()) time_delay = timedelta(seconds=10) task_time = utc_time + time_delay result = send_sms.apply_async([&quot;113&quot;,], eta=task_time) print(result.id) return HttpResponse(&quot;ok&quot;) 总结 12访问127.0.0.1/test 即可执行myapps的视图的test函数，执行celery任务。执行test函数是异步的，不会等celery任务执行完才返回OK]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[K8s备份与迁移]]></title>
    <url>%2F2020%2F06%2F23%2FK8s%E5%A4%87%E4%BB%BD%E4%B8%8E%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[etcd备份 本例使用kubeadm安装k8s集群，采用镜像pod方式部署的etcd，所以操作etcd需要使用etcd镜像提供的etcdctl工具。如果非镜像方式部署etcd则可直接使用etcdctl命令备份。 查看etcd配置参数文件 123456789101112131415161718192021222324# 默认etcd使用的是host-network网络，然后把系统参数和数据都映射到了宿主机目录。# 配置参数在/var/lib/etcd,证书文件位于/etc/kubernetes/pki/etcd# etcd服务所产生的快照文件[root@master]# ll /var/lib/etcd/member/snap/total 4668-rw-r--r-- 1 root root 8588 Aug 19 11:07 0000000000000006-000000000007c863.snap-rw-r--r-- 1 root root 8588 Aug 19 11:59 0000000000000006-000000000007ef74.snap-rw-r--r-- 1 root root 8588 Aug 19 12:51 0000000000000006-0000000000081685.snap-rw-r--r-- 1 root root 8588 Aug 19 13:43 0000000000000006-0000000000083d96.snap-rw-r--r-- 1 root root 8588 Aug 19 14:35 0000000000000006-00000000000864a7.snap-rw------- 1 root root 4714496 Aug 19 14:45 db# 查看etcd的证书文件[root@master]# ll /etc/kubernetes/pki/etcd/total 32-rw-r--r-- 1 root root 1017 Aug 14 10:01 ca.crt-rw------- 1 root root 1679 Aug 14 10:01 ca.key-rw-r--r-- 1 root root 1094 Aug 14 10:01 healthcheck-client.crt-rw------- 1 root root 1675 Aug 14 10:01 healthcheck-client.key-rw-r--r-- 1 root root 1127 Aug 14 10:01 peer.crt-rw------- 1 root root 1675 Aug 14 10:01 peer.key-rw-r--r-- 1 root root 1127 Aug 14 10:01 server.crt-rw------- 1 root root 1675 Aug 14 10:01 server.key 访问etcd服务 123kubectl describe -n kube-system pod etcd-master# 进入etcd的podkubectl exec -it -n kube-system etcd-master -- /bin/sh 备份 123ETCDCTL_API=3 etcdctl --endpoints https://127.0.0.1:2379 snapshot save &quot;/home/supermap/k8s-backup/data/etcd-snapshot/$(date +%Y%m%d_%H%M%S)_snapshot.db&quot;# 如果需要添加证书则在后面跟上--cert=&quot;/etc/kubernetes/pki/etcd/server.crt&quot; --key=&quot;/etc/kubernetes/pki/etcd/server.key&quot; --cacert=&quot;/etc/kubernetes/pki/etcd/ca.crt&quot; velero备份迁移k8s安装velero12345678910111213# 下载最新版https://github.com/vmware-tanzu/velero/releases?spm=a2c6h.12873639.0.0.717d1f2bjLi5Bxwget https://github.com/vmware-tanzu/velero/releases/download/v1.4.2/velero-v1.4.2-linux-amd64.tar.gztar -zxvf velero-v1.4.2-linux-amd64.tar.gzcp -a velero-v1.4.2-linux-amd64/velero /usr/local/bin/velero# 查看版本[root@master minio-yaml]# velero versionClient: Version: v1.4.2 Git commit: 56a08a4d695d893f0863f697c2f926e27d70c0c5&lt;error getting server version: the server could not find the requested resource (post serverstatusrequests.velero.io)&gt; 安装minio123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 上面下载的velero里含有minio部署cd velero-v1.4.2-linux-amd64/examples/minio/# 默认是ClusterIP，我将其直接改为NodePort[root@master minio]# cat 00-minio-deployment.yaml ---apiVersion: v1kind: Namespacemetadata: name: velero---apiVersion: apps/v1kind: Deploymentmetadata: namespace: velero name: minio labels: component: miniospec: strategy: type: Recreate selector: matchLabels: component: minio template: metadata: labels: component: minio spec: volumes: - name: storage emptyDir: &#123;&#125; - name: config emptyDir: &#123;&#125; containers: - name: minio image: minio/minio:latest imagePullPolicy: IfNotPresent args: - server - /storage - --config-dir=/config env: - name: MINIO_ACCESS_KEY value: &quot;minio&quot; - name: MINIO_SECRET_KEY value: &quot;minio123&quot; ports: - containerPort: 9000 volumeMounts: - name: storage mountPath: &quot;/storage&quot; - name: config mountPath: &quot;/config&quot;---apiVersion: v1kind: Servicemetadata: namespace: velero name: minio labels: component: miniospec: # ClusterIP is recommended for production environments. # Change to NodePort if needed per documentation, # but only if you run Minio in a test/trial environment, for example with Minikube. #type: ClusterIP type: NodePort ports: - port: 9000 targetPort: 9000 nodePort: 31045 protocol: TCP selector: component: minio---apiVersion: batch/v1kind: Jobmetadata: namespace: velero name: minio-setup labels: component: miniospec: template: metadata: name: minio-setup spec: restartPolicy: OnFailure volumes: - name: config emptyDir: &#123;&#125; containers: - name: mc image: minio/mc:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - &quot;mc --config-dir=/config config host add velero http://minio:9000 minio minio123 &amp;&amp; mc --config-dir=/config mb -p velero/velero&quot; volumeMounts: - name: config mountPath: &quot;/config&quot; 登录minio 12http://45.192.x.x:30100/minio/velero/# 账户密码，默认为minio、minio123 创建密钥 12345cat &gt; credentials-velero &lt;&lt;EOF[default]aws_access_key_id = minioaws_secret_access_key = minio123EOF 安装velero 1234567velero install \ --provider aws \ --plugins velero/velero-plugin-for-aws:v1.0.0 \ --bucket velero \ --secret-file ./credentials-velero \ --use-volume-snapshots=false \ --backup-location-config region=minio,s3ForcePathStyle=&quot;true&quot;,s3Url=http://minio.velero.svc:9000 12345[root@master velero-v1.4.2-linux-amd64]# kubectl get pod -n veleroNAME READY STATUS RESTARTS AGEminio-fdd868c5-xv52k 1/1 Running 0 56mminio-setup-hktjb 0/1 Completed 0 56mvelero-56fbc5d69c-8v2q7 1/1 Running 0 32m 备份演练 创建测试资源 12# velero里已准备好测试demokubectl apply -f examples/nginx-app/base.yaml 创建备份 12velero backup create nginx-backup --include-namespaces nginx-examplevelero backup create devops-backup --include-namespaces devops 查看备份 12345[root@master examples]# velero backup getNAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTORdevops-backup Completed 0 0 2020-08-20 16:27:18 +0800 CST 29d default &lt;none&gt;devops-backup1 Completed 0 0 2020-08-20 16:29:57 +0800 CST 29d default &lt;none&gt;nginx-backup2 Completed 0 0 2020-08-20 16:08:55 +0800 CST 29d default &lt;none&gt; 恢复测试 1234# 直接删除namespacekubectl delete -n nginx-example# 使用velero恢复，创建后会在velero桶中创建一个restores目录velero restore create --from-backup nginx-backup2 --wait 相关命令 1234velero backup get #备份查看velero schedule get #查看定时备份velero restore get #查看可恢复备份velero delete backup xxxx(备份名) #删除备份 123456789101112131415velero create backup NAME# 剔除namespace--exclude-namespaces stringArray# 剔除资源类型--exclude-resources stringArray# 包含集群资源类型 --include-cluster-resources optionalBool[=true]# 包含 namespace--include-namespaces stringArray # 对指定标签的资源进行备份-l, --selector labelSelector# 对 PV 创建快照--snapshot-volumes optionalBool[=true] # 备份数据多久删掉--ttl duration 定期备份 12345678# 每日1点进行备份velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;0 1 * * *&quot;# 每日1点进行备份，备份保留72小时velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;0 1 * * *&quot; --ttl 72h# 每5小时进行一次备份velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;@every 5h&quot;# 每日对 指定 namespace 进行一次备份 （如panshi-qtc-dev）velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;@every 24h&quot; --include-namespaces panshi-qtc-dev 利用velero备份到阿里云oss 准备好oss的bucket和相关api密钥 12bucket建议创建为低频存储，省钱api密钥建议新建个相关低权限的 下载velero插件 1git clone https://github.com/AliyunContainerService/velero-plugin 修改配置文件 1修改 install/credentials-velero文件，将新建用户中获得的 `AccessKeyID` 和 `AccessKeySecret`填入 1234567# vim install/01-velero.yaml# OSS 所在可用区REGION=cn-hangzhou# OSS Bucket 名称BUCKET=app-velero# bucket 名字prefix=velero 创建ns以及secret 12kubectl create namespace velerokubectl create secret generic cloud-credentials --namespace velero --from-file cloud=install/credentials-velero 创建资源 123456# 部署velerokubectl apply -f install/# 查看位置[root@master velero-plugin]# velero get backup-locationsNAME PROVIDER BUCKET/PREFIX ACCESS MODEdefault alibabacloud devops-k8s-backup/velero ReadWrite 备份及恢复 12velero backup create nginx-example-oss --include-namespaces nginx-examplevelero restore create --from-backup nginx-example-oss 注意事项 12341、在velero备份的时候，备份过程中创建的对象是不会被备份的。2、velero restore恢复不会覆盖`已有的资源`，只恢复当前集群中`不存在的资源`。已有的资源不会回滚到之前的版本，如需要回滚，需在restore之前提前删除现有的资源。3、后期可以做velero作为一个crontjob来运行，定期备份数据。4、在高版本1.16.x中，报错`error: unable to recognize &quot;filebeat.yml&quot;: no matches for kind &quot;DaemonSet&quot; in version &quot;extensions/v1beta1&quot;` ,将yml配置文件内的api接口修改为 apps/v1 ，导致原因为之间使用的kubernetes 版本是1.14.x版本，1.16.x 版本放弃部分API支持！]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Celery执行异步任务]]></title>
    <url>%2F2020%2F06%2F22%2FCelery%E5%B8%B8%E8%A7%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[消费者12345678910111213141516# celery_task1.pyimport celeryimport time# 定义redis信息backend = &apos;redis://:xxx@111@43.254.x.x:6379/1&apos;broker = &apos;redis://:xxx@111@43.254.x.x:6379/2&apos;cel = celery.Celery(&apos;haha&apos;, backend=backend, broker=broker)@celery.taskdef test_send_email(name): print(&apos;向%s发送邮件...&apos; %name) time.sleep(3) print(&apos;向%s发送邮件完成在!&apos; %name) return &quot;ok&quot; 生产者123456# produce_task.pyfrom celery_task import test_send_emailresult1 = test_send_email.delay(&quot;liyk&quot;)print(result1.id)result2 = test_send_email.delay(&quot;mz&quot;)print(result2.id) 执行12345celery worker -A celery_task1 -l info -P eventlet# 需安装eventlet，pip install eventlet或者：celery worker -A celery_task1 --pool=solo -l info# work是开启的进程; -A 执行的文件名; -l 显示日志，显示info日志# 执行命令后做的事情：用celery连接中间件（redis）、创建一个队列并监听它、再启动多个worker去监听任务 12先命令行celery worker xxx启动监听运行produce_task.py 多任务结构 123456789101112131415# celery.pyfrom celery import Celerycel = Celery(&apos;celery_demo&apos;, backend=&apos;redis://:xxxx@111@43.254.x.x:6379/1&apos;, broker = &apos;redis://:xxx@111@43.254.x.x:6379/2&apos;, # 包含以下两个任务文件，去相应的py文件中找任务，对多个任务做分类 include=[&apos;celery_tasks.task01&apos;, &apos;celery_tasks.task02&apos; ])# 时区cel.conf.timezone = &apos;Asia/Shanghai&apos;# 是否使用UTCcel.conf.enable_utc = False 12345678910111213141516171819# task01.pyimport timefrom .celery import cel@cel.taskdef send_email(res): print(&quot;完成向%s发送邮件任务&quot; %res) time.sleep(5) return &quot;邮件发送完成&quot;# task02.pyimport timefrom .celery import cel@cel.taskdef send_msg(name): print(&quot;完成向%s发送短信任务&quot;%name) time.sleep(5) return &quot;短信发送完成!&quot; 12345678910111213141516171819202122232425262728# produce_task.pyfrom celery_tasks.task01 import send_emailfrom celery_tasks.task02 import send_msg# 立即告知celery去执行test_celery任务，并传入一个参数result1 = send_email.delay(&apos;liyk&apos;)print(result1.id)result2 = send_msg.delay(&apos;mz&apos;)print(result2.id)# result.pyfrom celery.result import AsyncResultfrom celery_task import celasync_result=AsyncResult(id=&quot;0ec55a47-e91a-41c1-8428-3da4bb54a669&quot;, app=cel)if async_result.successful(): result = async_result.get() print(result) # result.forget() # 将结果删除elif async_result.failed(): print(&apos;执行失败&apos;)elif async_result.status == &apos;PENDING&apos;: print(&apos;任务等待中被执行&apos;)elif async_result.status == &apos;RETRY&apos;: print(&apos;任务异常后正在重试&apos;)elif async_result.status == &apos;STARTED&apos;: print(&apos;任务已经开始被执行&apos;) 执行12# 在celery_tasks目录外执行celery worker -A celery_tasks -l info -P eventlet celery执行定时任务123456789101112131415161718192021222324# 设置时间让celery执行一个定时任务# produce_task.pyfrom celery_task import send_emailfrom datetime import datetime# 方式一# v1 = datetime(2020, 7, 27, 16, 20, 00)# print(v1)# v2 = datetime.utcfromtimestamp(v1.timestamp())# print(v2)# result = send_email.apply_async(args=[&quot;egon&quot;,], eta=v2)# print(result.id)# 方式二ctime = datetime.now()# 默认用utc时间utc_ctime = datetime.utcfromtimestamp(ctime.timestamp())from datetime import timedeltatime_delay = timedelta(seconds=10)task_time = utc_ctime + time_delay# 使用apply_async并设定时间result = send_email.apply_async(args=[&quot;egon&quot;], eta=task_time)print(result.id) 123456789101112131415161718192021222324252627282930313233343536# 多任务结构中celery.pyfrom datetime import timedeltafrom celery import Celeryfrom celery.schedules import crontabcel = Celery(&apos;tasks&apos;, broker=&apos;redis://127.0.0.1:6379/1&apos;, backend=&apos;redis://127.0.0.1:6379/2&apos;, include=[ &apos;celery_tasks.task01&apos;, &apos;celery_tasks.task02&apos;,])cel.conf.timezone = &apos;Asia/Shanghai&apos;cel.conf.enable_utc = Falsecel.conf.beat_schedule = &#123; # 名字随意命名 &apos;add-every-10-seconds&apos;: &#123; # 执行tasks1下的test_celery函数 &apos;task&apos;: &apos;celery_tasks.task01.send_email&apos;, # 每隔2秒执行一次 # &apos;schedule&apos;: 1.0, # &apos;schedule&apos;: crontab(minute=&quot;*/1&quot;), &apos;schedule&apos;: timedelta(seconds=6), # 传递参数 &apos;args&apos;: (&apos;张三&apos;,) &#125;, # &apos;add-every-12-seconds&apos;: &#123; # &apos;task&apos;: &apos;celery_tasks.task01.send_email&apos;, # 每年4月11号，8点42分执行 # &apos;schedule&apos;: crontab(minute=42, hour=8, day_of_month=11, month_of_year=4), # &apos;args&apos;: (&apos;张三&apos;,) # &#125;,&#125;# 启动Beat程序。$ celery beat -A proj&lt;br&gt;# Celery Beat进程会读取配置文件的内容，周期性的将配置中到期需要执行的任务发送给任务队列# 之后启动 worker 进程.$ celery -A proj worker -l info 或者$ celery -B -A proj worker -l info]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Celery基础]]></title>
    <url>%2F2020%2F06%2F17%2FCelery%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[celery简介：celery是一个简单、灵活且可靠的，处理大量消息的分布式系统，专注于实时处理的异步任务队列，同时也支持任务调度。 组成：由3部分组成 消息中间件：Celery本身不提供消息服务，但是可以方便的和第三方提供的消息中间件集成。包括，RabbitMQ, Redis等等 任务执行单元：Worker是Celery提供的任务执行的单元，worker并发的运行在分布式的系统节点中。 任务结果存储：Task result store用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括AMQP, redis等1另外celery还支持不同的并发和序列化的手段 使用场景：通常来实现异步任务(async task)和定时任务(crontab) 异步任务：将耗时操作任务提交给celery去异步执行，比如发送短信/邮件、消息推送、音视频处理等 定时任务：定时执行某件事情，比如每天数据统计 优点 simple（简单）：celery使用和维护都非常简单，并且不需要配置文件。 highly available（高可用）：woker和client会在网络连接丢失或者失败时，自动进行重试。并且有的brokers 也支持“双主”或者“主／从”的方式实现高可用。 fast（快速）：单个的Celery进程每分钟可以处理百万级的任务，并且只需要毫秒级的往返延迟（使用 RabbitMQ, librabbitmq和优化设置时） flexible（灵活）：Celery几乎每个部分都可以扩展使用，自定义池实现、序列化、压缩方案、日志记录、调度器、消费者、生产者、broker传输等等。 安装123pip install -U celery# 或者sudo easy_install celery]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s搭建lnmp(Discuz)]]></title>
    <url>%2F2020%2F06%2F06%2Fk8s%E6%90%AD%E5%BB%BAlnmp-Discuz%2F</url>
    <content type="text"><![CDATA[下载公共镜像12345# 下载nginx-php-fpm镜像docker pull richarvey/nginx-php-fpm# 下载mysql 5.7镜像docker pull mysql:5.7# 如果有自己镜像仓库，如harbor可将镜像上传上去 安装NFS(测试使用的NFS，生产中建议使用ceph等存储) 创建web和DB目录 12# 有单独的服务器或者安装在master上mkdir -p /data/k8s/&#123;web,db&#125; 安装nfs 1yum -y install nfs-utils 设置共享目录 12vim /etc/exports/data/k8s 192.168.100.0/24(sync,rw,no_root_squash) 客户端挂载 12345# node节点上yum install nfs-utils -ymount -t nfs 192.168.100.11:/data/k8s /mnt# 建议加入开机自启echo &quot;192.168.100.11:/data/k8s /mnt nfs defaults 0 0&quot; &gt;&gt;/etc/fstab 启动服务 123# nfs服务端systemctl start nfssystemctl enable nfs 安装mysql 创建yaml文件目录 1mkdir -p /data/k8s-yaml/lnmp 创建secret,存储mysql密码 1234kubectl create secret generic mysql-pass --from-literal=password=xxzx@789# 使用base64加解密echo -n xxzx@789 |base64echo eHh6eEA3ODk= | base64 -d 创建mysql pv,pvc 123456789101112131415161718192021222324252627# mysql-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: mysql-pvspec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: path: /data/k8s/db #该参数指定你的NFS端的共享目录 server: 192.168.100.11# mysql-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-claim labels: app: discuzspec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi 查看pv，pvc信息1kubectl get pv,pvc 创建mysql-deploy 1234567891011121314151617181920212223242526272829303132333435363738394041# mysql-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: dz-mysql labels: app: discuzspec: selector: matchLabels: app: discuz tier: mysql strategy: type: Recreate template: metadata: labels: app: discuz tier: mysql spec: imagePullSecrets: - name: my-secret containers: - image: mysql:5.7 ##此处指定你的harbor需要拉取的镜像 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass ##此模块是调用secret中存放的mysql密码 key: password ports: - containerPort: 3306 name: dz-mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: ##此模块参数调用定义创建的pvc名称 claimName: mysql-claim 创建mysql-service 12345678910111213# mysql-service.yamlapiVersion: v1kind: Servicemetadata: name: dz-mysql labels: app: discuzspec: ports: - port: 3306 selector: app: discuz tier: mysql 安装nginx 创建nginx-pv,nginx-pvc 123456789101112131415161718192021222324252627# nginx-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: path: /data/k8s/web server: 192.168.100.11# nginx-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-claim labels: app: discuzspec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi 创建nginx-deploy 1234567891011121314151617181920212223242526272829303132333435# nginx-dp.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: dz-web labels: app: discuzspec: replicas: 1 selector: matchLabels: app: discuz tier: nginx-php template: metadata: labels: app: discuz tier: nginx-php spec: imagePullSecrets: - name: my-secret containers: - image: richarvey/nginx-php-fpm:latest name: dz-web ports: - containerPort: 9000 - containerPort: 80 name: dz-web volumeMounts: - name: mysql-persistent-storage mountPath: /var/www/html/ volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: web-claim 创建nginx-service 123456789101112131415# nginx-svc.yamlapiVersion: v1kind: Servicemetadata: name: dz-web labels: app: discuzspec: type: NodePort ports: - port: 80 nodePort: 30001 selector: app: discuz tier: nginx-php 安装Discuz 下载discuz 123456# NFS服务器上操作cd /data/k8sgit clone https://gitee.com/ComsenzDiscuz/DiscuzX.gitcp -a DiscuzX/upload/* web/cd web/chmod 774 data uc_server/data uc_client/data config mysql上创建dz库并授权用户 123kubectl exec -it dz-mysql-xxx -- mysql -uroot -pcreate database dz default character set utf8;grant all on dz.* to &apos;dz&apos;@&apos;%&apos; identified by &apos;abc123&apos;; 浏览器访问安装 1x.x.x.x:30001]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[K8s基础操作和名称解释]]></title>
    <url>%2F2020%2F04%2F26%2FK8s%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%E5%92%8C%E5%90%8D%E7%A7%B0%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[pod1234567891011121314151617181920212223242526272829303132kubectl run nginx1 --image nginx --port 80 #创建pod[root@master ~]# kubectl get pod # 查看podNAME READY STATUS RESTARTS AGEnginx-554b9c67f9-244gh 1/1 Running 1 2d10hnginx1-5fd6585797-fxdgz 1/1 Running 0 55s[root@master ~]# kubectl get pod -w # 动态刷新 --watchNAME READY STATUS RESTARTS AGEnginx-554b9c67f9-244gh 1/1 Running 1 2d10hnginx1-5fd6585797-fxdgz 0/1 ContainerCreating 0 22snginx1-5fd6585797-fxdgz 1/1 Running 0 32s[root@master ~]# kubectl get deployment #(简写deploy)NAME READY UP-TO-DATE AVAILABLE AGEnginx 1/1 1 1 2d10hnginx1 1/1 1 1 81s[root@master ~]# kubectl get deploy/nginxNAME READY UP-TO-DATE AVAILABLE AGEnginx 1/1 1 1 2d10h[root@master ~]# kubectl get deploy/nginx1 -o wide #列出指定pod信息NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx1 1/1 1 1 2m28s nginx1 nginx run=nginx1[root@master ~]# kubectl get pod -owide # 列出所有pod详细信息NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-554b9c67f9-244gh 1/1 Running 1 2d10h 10.244.1.4 node1 &lt;none&gt; &lt;none&gt;nginx1-5fd6585797-fxdgz 1/1 Running 0 2m50s 10.244.2.2 node2 &lt;none&gt; &lt;none&gt;kubectl scale deployment nginx1 --replicas=2 #扩展pod下容器个数 生成deployment的yaml文件 1kubectl create deployment nginx --image=nginx --dry-run -o yaml &gt; deployment.yaml 生成svc的yaml文件 1kubectl expose deployment nginx --port=80 --target-port=80 --type=NodePort --dry-run -o yaml &gt; service.yaml 修改已运行的pod、svc 12kubectl edit svc svc-namekubectl edit deployment deployment-name namespace1234# kubectl可以通过–namespace或者-n选项指定namespace。如果不指定，默认为default。kubectl get namespaces # 缩写ns，查看命名空间# 注意：namespace包含两种状态”Active”和”Terminating”。在namespace删除过程中，namespace状态被设置成”Terminating”。 1234567891011# 命令行直接创建kubectl create namespace new-namespace# 通过文件创建$ cat my-namespace.yamlapiVersion: v1kind: Namespacemetadata: name: new-namespace$ kubectl create -f ./my-namespace.yaml 12345# 删除$ kubectl delete namespaces new-namespace# 注意：删除一个namespace会自动删除所有属于该namespace的资源。default和kube-system命名空间不可删除。 Replication Controller12就像一个进程管理器，监管着不同&apos;node&apos;上的多个pod。保持特定数量的pod副本运行。Replication Controller只会对那些RestartPolicy = Always的Pod的生效，（RestartPolicy的默认值就是Always）,Replication Controller不会去管理那些有不同启动策略pod。 Scaling（缩放）12Replication Controller让我们更容易的控制pod的副本的数量，不管我们是手动控制还是通过其它的自动管理的工具.最简单的：修改replicas的值 Rolling updates（动态更新）12Replication Controller可以支持动态更新，当我们更新一个服务的时候，它可以允许我们一个一个的替换pod Node12&apos;Node&apos;是Pod真正运行的主机，可以物理机，也可以是虚拟机。为了管理Pod，每个node节点上至少要运行container runtime（比如docker或者rkt）、kubelet和kube-proxy服务。 Node管理12不像其他的资源（如Pod和Namespace），Node本质上不是Kubernetes来创建的，Kubernetes只是管理Node上的资源。虽然可以通过Manifest创建一个Node对象（如下json所示），但Kubernetes也只是去检查是否真的是有这么一个Node，如果检查失败，也不会往上调度Pod。 ReplicaSets1234ReplicaSet是下一代复本控制器。ReplicaSet和 Replication Controller之间的唯一区别是现在的选择器支持。Replication Controller只支持基于等式的selector（env=dev或environment!=qa）;但ReplicaSet还支持新的，基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)）。 Services123456# 相当于负载均衡，后面是pod。Kubernete Service 是一个定义了一组Pod的策略的抽象，我们也有时候叫做宏观服务。每个pod都由自己的ip，这些IP也随着时间的变化也不能持续依赖。这样就引发了一个问题：如果一些Pods（让我们叫它作后台，后端）提供了一些功能供其它的Pod使用（让我们叫作前台），在kubernete集群中是如何实现让这些前台能够持续的追踪到这些后台的？Service如果你选择了“NodePort”，那么 Kubernetes master 会分配一个区域范围内，（默认是30000-32767），并且，每一个node，都会代理（proxy）这个端口到你的服务中，我们可以在spec.ports[*].nodePort 找到具体的值 Volumes12345# 解决pod的持久化存储数据，以及数据文件共享Kubernetes volume 支持多种类型，任何容器都可以使用多个Kubernetes volume。它的核心，一个 volume 就是一个目录，可能包含一些数据，这些数据对pod中的所有容器都是可用的，这个目录怎么使用，什么类型，由什么组成都是由特殊的volume 类型决定的想要使用一个volume，Pod必须指明Pod提供了那些磁盘，并且说明如何挂在到容器中 PV/PVC/StorageClass123PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 1234PV是集群中的资源。 PVC是对这些资源的请求，也是对资源的索赔检查。PV和PVC之间的相互作用遵循这个生命周期:Provisioning ——-&gt; Binding ——–&gt;Using——&gt;Releasing——&gt;RecyclingProvisioning Deployment123456Deployment为Pod和ReplicaSet提供了一个声明式定义(declarative)方法.用来替代以前的ReplicationController来方便的管理应用。 - 定义Deployment来创建Pod和ReplicaSet - 滚动升级和回滚应用 - 扩容和缩容 - 暂停和继续Deployment 123Deployment为Pod和Replica Set（下一代Replication Controller）提供声明式更新。你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Helm v3]]></title>
    <url>%2F2020%2F04%2F26%2FHelm-v3%2F</url>
    <content type="text"><![CDATA[背景当一个项目的微服务有几十个甚至上百个，传统的yaml已不适用。需要诞生一个管理工具，helm解决了这问题。 1231. 如何将这些yaml文件作为一个整体管理？2. 这些yaml文件如何高效复用？3. 不支持应用级别的版本管理？ helm1231. helm 一个命令行管理工具2. chart 把yaml打包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等。3. release 在Kubernetes 集群上运行的 Chart 的一个实例，版本 helm v3特性 1231. 架构改变，去掉Tiller，直接helm通过kubeconfig连接apiserver2. release名称可以在不同命令空间重名3. chart支持放到docker镜像仓库 安装 12# 下载解压https://github.com/helm/helm/releases 常用命令 1234567891011121314# 查看当前chart源仓库地址helm repo list# 添加chart源helm repo add stable http://mirror.azure.cn/kubernetes/charts# 搜索helm search repo weave# 安装/升级helm install ui-xx stable/weave-scopehelm upgraded ui-xx stable/weave-scope# 查看部署的情况helm list# 创建chart，里含有基础模版helm create mychartcd mychart helm chart包一套yaml部署多个应用 123451. 资源名称2. 镜像3. 标签4. 副本数5. 端口 用法 12345678910111213141516171819201. 编辑values.yamlreplicas: 1image: liyk/hahatag: latestlabel: liyk-nginxport: 80802. deployment、svc里用变量替换。replicas: &#123;&#123; .Values.replicas &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;-svc# 查看chart模板渲染情况helm install --dry-run xxx mychart# 增加副本数/镜像 imagehelm upgrade web1 --set replicas=3 mychar/# 回滚helm history web1helm rollback web1 1]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s镜像仓库迁移]]></title>
    <url>%2F2020%2F04%2F26%2Fk8s%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[背景123在k8s集群迁移场景中，镜像仓库之间迁移和同步是基本需求。在迁移中当镜像个数较少时可以通过docker pull/push完成镜像迁移，如果涉及到成百上千个镜像，甚至上TB级别的镜像仓库数据时，迁移过程就变得非常漫长，并且可能丢失数据。本文通过阿里云开源工具image-syncer进行镜像迁移。 image-syncer简介和特点 image-syncer 123456789101112image-syncer的定位是一个简单、易用的批量镜像迁移和镜像同步复制工具。支持几乎所有目前主流的基于docker registry V2搭建的镜像存储服务。对硬件资源几乎没要求（因为 image-syncer 严格控制网络连接数目=并发数，所以只有在单个镜像层过大的情况下，并发数目过大可能会打满内存，内存占用 &lt;= 并发数 x 最大镜像层大小）# 特点1. 支持多对多镜像仓库同步2. 支持基于Docker Registry V2搭建的Docker镜像仓库服务3. 镜像同步复制只经过内存和网络，不依赖磁盘存储，同步速度快。4. 支持增量同步，通过对同步过的镜像blob信息落盘，不会对已同步的镜像进行重复同步。5. 支持并发同步，可以通过配置文件调整并发数。6. 支持自动重试失败的同步任务，解决大部分镜像同步中的网络抖动问题。7. 不依赖Docker以及其他程序。 使用传统脚本方式docker pull/push镜像方式有如下局限性： 1231. 依赖磁盘存储，需要及时进行本地镜像的清理，并且落盘造成多余的时间开销；难以胜任生产场景中大量镜像的迁移。2. 依赖docker程序，docker daemon对pull/push的并发数进行了严格的限制，无法进行高并发同步。3. 一些功能只能通过HTTP api进行操作，仅使用docker cli无法做到，使脚本变得复杂。 安装和使用 安装 12345# 连接https://github.com/AliyunContainerService/image-syncer/releaseswget https://github.com/AliyunContainerService/image-syncer/releases/download/v1.1.0/image-syncer-v1.1.0-linux-amd64.tar.gz# 解压tar zxvf image-syncer-v1.1.0-linux-amd64.tar.gz 设置配置文件 123456789101112131415161718&#123; &quot;auth&quot;: &#123; &quot;registry.cn-shanghai.aliyuncs.com&quot;: &#123; &quot;username&quot;: &quot;xxxx&quot;, &quot;password&quot;: &quot;xxx&quot; &#125;, &quot;ccr.ccs.tencentyun.com&quot;: &#123; &quot;username&quot;: &quot;xxxxxx&quot;, &quot;password&quot;: &quot;xxxxxx&quot; &#125; &#125;, &quot;images&quot;: &#123; &quot;registry.cn-shanghai.aliyuncs.com/key1024/test&quot;: &quot;ccr.ccs.tencentyun.com/key1024/test&quot;, &quot;registry.cn-shanghai.aliyuncs.com/key1024/mysql&quot;: &quot;ccr.ccs.tencentyun.com/key1024/mysql&quot;, &quot;registry.cn-shanghai.aliyuncs.com/key1024/liyk&quot;: &quot;ccr.ccs.tencentyun.com/key1024/liyk&quot;, &quot;registry.cn-shanghai.aliyuncs.com/key1024/demo&quot;: &quot;ccr.ccs.tencentyun.com/key1024/demo1&quot; &#125; &#125; 1234567891011121314151617# harbor同步到阿里云镜像仓库&#123; &quot;auth&quot;: &#123; &quot;harbor.myk8s.paas.com:32080&quot;: &#123; &quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;xxxxxxxxx&quot;, &quot;insecure&quot;: true &#125;, &quot;registry.cn-beijing.aliyuncs.com&quot;: &#123; &quot;username&quot;: &quot;acr_pusher@1938562138124787&quot;, &quot;password&quot;: &quot;xxxxxxxx&quot; &#125; &#125;, &quot;images&quot;: &#123; &quot;harbor.myk8s.paas.com:32080/library/nginx&quot;: &quot;&quot; &#125;&#125; 镜像同步测试123./image-syncer --proc=6 --config=./ali-2-tencent.json# 或者在运行时指定目的端的namespace和仓库地址./image-syncer --proc=6 --config=./ali-2-tencent.json --namespace=key1024 --registry=ccr.ccs.tencentyun.com --retries=2 --log=./ali-2-tencent.log]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker深入]]></title>
    <url>%2F2020%2F04%2F26%2FDocker%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[容器中进程启动的两种模式docker容器内运行的进程全部都是宿主机上的独立进程；该进程是不是docker容器进程本身要依据Dockerfile的写法判定。 12345在ENTRYPOINT和CMD指令中，提供了两种不同的进程执行方式：shell和exec1. 在shell方式中，CMD/ENTRYPOINT指令如下定义：这种方式中的1号进程是以/bin/sh -c &quot;executable param1 param2&quot;方式启动的。CMD &quot;executable param1 param2&quot;2. 而在exec方式中CMD/ENTRYPOINT指令如下定义：此时1号进程会以executable param1 param2方式而不是shell方式启动。CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] 创建dockerfile 启动验证1234567# shell方式# 制作镜像docker build -t myshellredis -f shellredis/dockerfile .# 运行 docker run -d --name myshellredis shellredisFROM ubuntu:14.04RUN apt-get update &amp;&amp; apt-get install redis-server -y &amp;&amp; rm -rf /var/lib/apt/lists/*EXPOSE 6379CMD &quot;/usr/bin/redis-server&quot; 12345# exec方式FROM ubuntu:14.04RUN apt-get update &amp;&amp; apt-get install redis-server -y &amp;&amp; rm -rf /var/lib/apt/lists/*EXPOSE 6379CMD [&quot;/usr/bin/redis-server&quot;] 启动总结 1234由此可看出，execredis镜像中以exec方式启动容器中的redis进程，所以redis进程就是容器进程本身。容器中启动的redis进程就是容器内1号进程。而shellredis镜像中用shell方式启动容器中的redis进程，所以被容器启动的redis进程是容器进程的一个子进程，是独立存在的。 停止容器 123456Docker提供了docker stop和docker kill两个命令向容器中的1号进程发送信号。当执行docker stop时，docker会首先向容器的1号进程发送一个SIGTERM信号，用于容器内程序的退出。如果容器在收到SIGTERM信号后没有结束进程，那么docker Daemon会在等待一段时间（默认10秒）后再向容器发送SIGKILL信号，将容器杀死并变为退出状态。这种方式给docker应用提供了一个优雅的退出机制，允许应用在收到stop命令时清理和释放使用中的资源。而docker kill命令可以向容器内的1号进程发送任何信号，默认是发送SIGKILL信号来强制退出应用。 总结 12345678原因在于，用shell脚本启动的容器，其1号进程是shell进程。shell进程中没有对SIGTERM信号的处理逻辑，所以它忽略了接收到的SIGTERM信号。当docker等待stop命令执行10秒超时之后，Docker Daemon将发送SIGKILL信号强制杀死1号进程，并销毁它的PID命名空间，其子进程redis-server也在收到SIGKILL信号后被强制终止并退出。如果此时应用中还有正在执行的事务或未持久化的数据，强制退出进程可能导致数据丢失或状态不一致。所以容器的1号进程必须能够正确的处理SIGTERM信号来支持优雅退出。如果容器中包含多个进程，则需要1号进程能够正确的传播SIGTERM信号来结束所有的子进程，之后再退出。正确的做法就是，令每个容器中只包含一个进程，同时都采用exec模式启动进程。 容器中的”隔离”12345每个容器内部的PID和进程体系都是相互隔离、互不影响的。docker用户组的权限和root权限差别不大，官方安全文档中警告用户谨慎使用。namespace 内核级别的环境隔离方法 容器的本质简单的给容器下个定论：容器 = CGroups + Namespace + Rootfs Namespace 123456namespace是linux提供的一种内核级别的环境隔离方法，提供了对Mount、UTS、IPC、PID、Network、User等进行隔离的机制。Namespace的实现主要是基于以下三个系统方法：clone(): 实现线程的系统调用，用来创建一个新的进程，并可以通过设置相关参数实现隔离。unshare()：使某个进程脱离某个Namespacesetns()：把某个进程加入某个Namespace中 容器技术原理 chroot 123chroot 是在Unix和Linux系统的一个操作，针对正在运作的软件行程和它的子进程，改变它外显的根目录。chroot虽然实现了当前进程与主机的隔离，但是网络信息、进程信息等并未隔离。还需要Namespace、Cgroups和联合文件系统来实现完整的容器。 Namespace 12Namespace 是 Linux 内核的一项功能，该功能对内核资源进行隔离，使得容器中的进程都可以在单独的命名空间中运行，并且只可以访问当前容器命名空间的资源。Namespace 可以隔离进程 ID、主机名、用户 ID、文件名、网络访问和进程间通信等相关资源。 Cgroups 12Cgroups 是一种 Linux 内核功能，可以限制和隔离进程的资源使用情况（CPU、内存、磁盘 I/O、网络等）。在容器的实现中，Cgroups 通常用来限制容器的 CPU 和内存等资源的使用。 联合文件系统 12联合文件系统，又叫 UnionFS，是一种通过创建文件层进程操作的文件系统，因此，联合文件系统非常轻快。Docker 使用联合文件系统为容器提供构建层，使得容器可以实现写时复制以及镜像的分层构建和存储。 ```]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MQ问答]]></title>
    <url>%2F2020%2F04%2F21%2FMQ%E9%97%AE%E7%AD%94%2F</url>
    <content type="text"><![CDATA[为什么使用MQ比较核心的场景：异步、解耦、削峰填谷 异步 解耦：系统只用把消息扔到MQ，其他系统按需来订阅消息就行。 削峰填谷：秒杀场景 使用MQ的优缺点 系统可用性降低：如果MQ挂了，后面的系统将受影响 系统复杂度提高：加入MQ之后需要考虑消息重复消费、消息丢失、甚至消息顺序性的问题 数据一致性问题：12本来A系统调用BC系统接口，如果BC系统出错了，会抛出异常，返回给A系统让A系统知道，这样的话就可以做回滚操作了。但是使用了MQ之后，A系统发送完消息就完事了，认为成功了。而刚好C系统写数据库的时候失败了，但是A认为C已经成功了？这样一来数据就不一致了。 怎么保证MQ消息不丢失生产者弄丢了数据RabbitMQ生产者将数据发送到rabbitmq的时候,可能数据在网络传输中搞丢了，这个时候RabbitMQ收不到消息，消息就丢了。RabbitMQ提供了两种方式来解决这个问题： 事务方式 123在生产者发送消息之前，通过`channel.txSelect`开启一个事务，接着发送消息。如果消息没有成功被RabbitMQ接收到，生产者会收到异常，此时就可以进行事务回滚`channel.txRollback`然后重新发送。假如RabbitMQ收到了这个消息，就可以提交事务`channel.txCommit`。但是这样一来，生产者的吞吐量和性能都会降低很多，现在一般不这么干。 confirm机制 12345这个confirm模式是在生产者哪里设置的，就是每次写消息的时候会分配一个唯一的id，然后RabbitMQ收到之后会回传一个ack，告诉生产者这个消息ok了。如果rabbitmq没有处理到这个消息，那么就回调一个nack的接口，这个时候生产者就可以重发。事务机制和cnofirm机制最大的不同在于事务机制是同步的，提交一个事务之后会阻塞在那儿但是confirm机制是异步的，发送一个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。所以一般在生产者这块避免数据丢失，都是用confirm机制的。 Rabbitmq弄丢了数据123消息发送到RabbitMQ之后，默认是没有落地磁盘的，万一RabbitMQ宕机了，这个时候消息就丢失了。所以为了解决这个问题，RabbitMQ提供了一个持久化的机制，消息写入之后会持久化到磁盘.这样哪怕是宕机了，恢复之后也会自动恢复之前存储的数据，这样的机制可以确保消息不会丢失。 设置持久化有2个步骤 第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据。 第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。123但是这样一来可能会有人说：万一消息发送到RabbitMQ之后，还没来得及持久化到磁盘就挂掉了，数据也丢失了，怎么办？对于这个问题，其实是配合上面的confirm机制一起来保证的，就是在消息持久化到磁盘之后才会给生产者发送ack消息。万一真的遇到了那种极端的情况，生产者是可以感知到的，此时生产者可以通过重试发送消息给别的RabbitMQ节点。 消费端弄丢了消息1234RabbitMQ消费端弄丢了数据的情况是这样的：在消费消息的时候，刚拿到消息，结果进程挂了，这个时候RabbitMQ就会认为你已经消费成功了，这条数据就丢了。对于这个问题，要先说明一下RabbitMQ消费消息的机制：在消费者收到消息的时候，会发送一个ack给RabbitMQ，告诉RabbitMQ这条消息被消费到了，这样RabbitMQ就会把消息删除。但是默认情况下这个发送ack的操作是自动提交的，也就是说消费者一收到这个消息就会自动返回ack给RabbitMQ，所以会出现丢消息的问题。 12所以针对这个问题的解决方案就是：关闭RabbitMQ消费者的自动提交ack,在消费者处理完这条消息之后再手动提交ack。这样即使遇到了上面的情况，RabbitMQ也不会把这条消息删除，会在你程序重启之后，重新下发这条消息过来。 怎么保证MQ的高可用性123这一块基于RabbitMQ这种经典的MQ来说明一下：RabbitMQ是比较有代表性的，因为是基于主从做高可用性的，我们就以他为例子讲解第一种MQ的高可用性怎么实现。rabbitmq有三种模式：单机模式，普通集群模式，镜像集群模式 单机模式 12单机模式就是demo级别的，就是说只有一台机器部署了一个RabbitMQ程序。这个会存在单点问题，宕机就玩完了，没什么高可用性可言。一般就是你本地启动了玩玩儿的，没人生产用单机模式。 普通集群模式 12是在多台机器上启动多个rabbitmq实例。类似的master-slave模式一样。但是创建的queue，只会放在一个master rabbtimq实例上，其他实例都同步那个接收消息的RabbitMQ元数据。在消费消息的时候，如果你连接到的RabbitMQ实例不是存放Queue数据的实例，这个时候RabbitMQ就会从存放Queue数据的实例上拉去数据，然后返回给客户端。 12总的来说，这种方式有点麻烦，没有做到真正的分布式，每次消费者连接一个实例后拉取数据，如果连接到不是存放queue数据的实例，这个时候会造成额外的性能开销。如果从放Queue的实例拉取，会导致单实例性能瓶颈。如果放queue的实例宕机了，会导致其他实例无法拉取数据，这个集群都无法消费消息了，没有做到真正的高可用。 镜像集群模式123镜像集群模式才是真正的rabbitmq的高可用模式，跟普通集群模式不一样的是：创建的queue无论元数据还是queue里的消息都会存在于多个实例上，每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。这样的话任何一个机器宕机了别的实例都可以用提供服务，这样就做到了真正的高可用了。 1234# 但是也存在着不好之处：性能开销过高:消息需要同步所有机器，会导致网络带宽压力和消耗很重扩展性低：无法解决某个queue数据量特别大的情况，导致queue无法线性拓展。就算加了机器，那个机器也会包含queue的所有数据，queue的数据没有做到分布式存储。]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MQ相关概念]]></title>
    <url>%2F2020%2F04%2F21%2FMQ%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[MQ12消息队列（Message Queue，简称MQ），本质是个队列，FIFO先入先出，只不过队列中存放的内容是message而已。其主要用途：不同进程Process/线程Thread之间通信。 产生消息队列的原因123451.不同进程（process）之间传递消息时，两个进程之间耦合程度过高，改动一个进程，引发必须修改另一个进程.为了隔离这两个进程，在两进程间抽离出一层（一个模块），所有两进程之间传递的消息，都必须通过消息队列来传递，单独修改某一个进程，不会影响另一个。2.同进程（process）之间传递消息时，为了实现标准化，将消息的格式规范化了。并且，某一个进程接受的消息太多，一下子无法处理完，并且也有先后顺序，必须对收到的消息进行排队，因此诞生了事实上的消息队列。 应用场景 服务解耦：系统之间拆分、隔离后的通信，之间的弱依赖可用消息中间件解耦 削峰填谷：秒杀大促，把流量高峰和低谷做一个均衡。把消息缓存然后慢速消费。 异步化缓冲：异步操作，只需做到最终一致性。 应用思考 生产端可靠性投递：消息不丢失，100%投递 消费端幂等：验证消息，防止消息被消费多次 其余点 高可用 低延迟 可靠性 堆积能力 可扩展性 技术选型 各个MQ的性能、优缺点、相应的业务场景 集群架构模式、分布式、可扩展性、高可用、可维护性 综合成本问题、集群规模、人员成本 未来的方向、规划、思考 ActiveMQ古老而又神秘的消息中间件”ActiveMQ” 1采用zookeeper做主备 RabbitMQ集群架构模型与原理解析 RabbitMQ是一个开源的消息代理和队列服务器，用来通过普通协议在完全不同的应用之间共享数据，RabbitMQ是使用Erlang语言来编写的，并且RabbitMQ是基于 AMQP协议的。 四种集群架构 主备模式：warren(兔子窝)，主备方案（主挂了采用HAproxy做切换） 1234567# 主备模式-HAproxy配置listen rabbitmq_clusterbind 0.0.0.0:5672 #配置TCP模式mode tcp #简单的轮询balance roundrobin #主节点server liyk1 192.168.1.1:5672 check inter 5000 rise 2 fall 2server liyk2 192.168.1.2:5672 backup check inter 5000 rise 2 fall 2 #备用节点 远程模式：远距离通信和复制,可以实现双活的一种模式，简称Shovel模式。(很少使用) 镜像模式：经典模式，mirror镜像模式，保障数据不丢失。实际工作中用的最多，并且实现集群非常简单，一般互联网公司都会使用这种镜像集群模式。 Mirror镜像队列：高可靠(往每个节点发数据)、数据同步、奇数节点(3个就行，缺点无法横向扩展因为扩展的节点也只是备份前面的数据) 多活模式：实现异地数据复制的主流模式 Federation：多活模式，federation插件 RocketMQRocketMQ是一款分布式、队列模型的消息中间件，由阿里巴巴自主研发的一款适用于高并发、高可靠性、海量数据场景的消息中间件。 Kafka介绍 kafka是Linkedln开源的分布式消息系统，目前归属于apache顶级项目 特点是基于pull的模式来处理消息消费，追求高吞吐量。最初目的就是用于日志收集和传输。 支持复制，不支持事务，对消息重复、丢失错误没有严格要求，适合大量数据的收集业务。比如日志。 特点 分布式：支持消息分区，topic下的patition 跨平台：java、python、php都支持 实时性：只要存储可以，不影响消息接收、堆积能力强 伸缩性 kafka高性能的原因 顺序写，page cache‘空中接力’（本质上把从磁盘中读取数据变为从内存中读取），高效读写 高性能、高吞吐 后台异步、主动flush IO预读策略 零拷贝 12345678910111213mmap和sendFile这2个零拷贝简单说就是：数据不需要来回的拷贝，大大提升系统的性能。是指CPU不执行拷贝数据从一个存储区域到另外一个存储区域的任务，这通常用于通过网络传输一个文件时以减少CPU周期和内存带宽。# 避免数据拷贝避免操作系统内核缓冲区之间进行数据拷贝操作；避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作；用户应用程序可以避开操作系统直接访问硬件存储。# 零拷贝带来的好处1.减少甚至完全避免不必要的CPU拷贝，从而让CPU解脱出来去执行其他任务。2.减少内存带宽的占用3.减少用户空间和操作系统内核空间之间的上下文切换零拷贝完全依赖于操作系统，操作系统级别的支持 缓冲区1缓冲区是所有IO的基础，IO讲的无非就是把数据移进或移出缓冲区；进程执行IO操作，就是向操作系统发出请求。让它要么把缓冲区的数据排干(写),要么填充缓冲区(读)。 虚拟内存123操作系统使用虚拟内存，虚拟地址取代物理地址好处如下：1、一个以上的虚拟地址可以指向同一个物理内存地址2、虚拟内存空间可大于实际可用的物理地址 零拷贝的2种方式 mmap+write方式123使用mmap+write方式代替原来的read+write方式，mmap是一种内存映射文件的方法。即将一个文件或其他对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一映射关系。这样可以省掉原来内核read缓冲区copy数据到用户缓冲区，但是还是需要内核read缓冲区将数据copy到内核socket缓冲区。 sendfile方式123简化通过网络在2个通道之间进行的数据传输过程，sendfile系统调用的引入，不仅减少了数据复制，还减少了上下文切换的次数。数据传输只发生在内核空间，所以减少了一次上下文切换。上下文切换（Context Switch），性质为环境切换。上下文切换，有时也称做进程切换或任务切换，是指CPU 从一个进程或线程切换到另一个进程或线程。 kafka集群模式123大部分做的内存的replicate；当kafka的生产者和消费者速率相当时都不需要用到磁盘，直接从内存page cache取数据。内存级别的副本。 小结]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Terraform-云平台操作神器(一)]]></title>
    <url>%2F2020%2F04%2F21%2FTerraform-%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%93%8D%E4%BD%9C%E7%A5%9E%E5%99%A8-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[功能简介12Terraform是IT 基础架构自动化编排工具，它的口号是 &quot;Write,Plan, and create Infrastructure as Code&quot;, 基础架构即代码。通过同一套规则和命令来操作不同的云平台，包括私有云。 123456# 四种操作云平台的方法方式一：直接登录到云平台的管理页面，人工点击。这种对于单个或者几个云资源还可以维护，但是当云服务器规模达到成本上千后人工操作变得不再现实、而且容易误操作。方式二：云平台提供的SDK、AIP。这样是大批量操作成为可能，而且代码能相对于减少误操作。但需要相应的开发能力，而且对复杂需求需要编写大量的代码。方式三：云平台提供的命令行工具，例如aws cli、阿里云cli等，这样就可以通过命令操作云资源，就如sql一样使用增删改查等操作元素来管理云。方式四：Terraform闪亮登场，如果方式三中CLI是命令式操作，需要明确告知云服务器本次操作是查询、新增，那么Terraform就是目的式操作。在本地维护一份云服务状态的目标，模板编排成什么样子云服务器就是什么样子。相对于上面三种，Teeraform优势是我们只需要专注于编排结果即可，不需要关心用什么命令去操作。 Terraform知识核心文件有个2个，一个是编排文件、另一个是状态文件 main.tf文件：是业务编排的主文件，定制了一系列的编排规则。 terraform.tfstate：本地状态文件，相当于本地云服务状态的备份，会影响terraform的执行计划。 12问：如果本地状态与云服务状态不一样时会怎样？答：这个不需要担心，前面介绍过Terraform是目的式的编排，会按照预设结果完成编排并最终同步更新本地文件。 安装配置 下载解压 1234# 官网下载，找寻对应平台https://www.terraform.io/downloads.html# 下载解压wget https://releases.hashicorp.com/terraform/0.12.24/terraform_0.12.24_linux_amd64.zip 配置环境变量 123vim /etc/profileexport PATH=$PATH:/data/softappssource /etc/profile 基本操作命令 123456789# 前期初始化操作1.创建一个工作目录，目录就像git的仓库，或者像软件开发中的workspace。2.创建一个xx.ft文件，指定provider等信息。3.执行terraform init命令，就像git init一样对当前目录做初始化，下载tf中的provider，并对后续的操作准备必要环境条件。# 资源相关操作1.terraform plan: 预览执行计划，不是必须的但是强烈建议。好明白这次要把云服务器弄成什么样子。后期版本会与apply合并成一个，所以根据自己的版本使用plan命令。2.terraform apply：真正执行编排计划3.terraform show：展示现在状态4.terraform destroy：销毁云服务，将tf中的云服务清理干净 腾讯云Terraform操作 Terraform工作流程 创建provider.tf文件 123456789101112131415vim provider.tf//provider.tfprovider &quot;tencentcloud&quot; &#123; secret_id = &quot;RGID*************SG*&quot; secret_key = &quot;CBS************GS&quot; region = &quot;ap-shanghai&quot;&#125;# 建议将api密钥放在环境变量中配置,不建议直接把密钥写到源代码里避免泄漏。vim /etc/profileexport TENCENTCLOUD_SECRET_ID=&quot;your_accessid&quot;export TENCENTCLOUD_SECRET_KEY=&quot;your_accesskey&quot;export TENCENTCLOUD_REGION=&quot;ap-shanghai&quot;# 密钥写环境变量里provider.tf文件可省略相关信息vim provider.tfprovider &quot;tencentcloud&quot; &#123;&#125; 初始化 123terraform init：初始化Terraform，erraform会自动检测 provider.tf 文件中的 provider 字段。发送请求到Terraform官方GitHub下载最新版本腾讯云资源的模块和插件，初始化成功时当前脚本的版本信息也会显示出来。当腾讯云脚本有新的版本发布时，可以通过 terraform init -upgrade 指令更新脚本，获取最新的应用。 部署腾讯云资源1234567# 这里展示创建一个vpc$ vim vpc.tf// Create a vpcresource &quot;tencentcloud_vpc&quot; &quot;vpc_test&quot; &#123; name = &quot;vpc-test-liyk&quot; cidr_block = &quot;10.0.0.0/16&quot;&#125; 开通云服务器123456789101112131415161718vim cvm.tf// Create a cvmresource &quot;tencentcloud_instance&quot; &quot;cvm_test&quot; &#123; instance_name = &quot;cvm-test&quot; availability_zone = &quot;ap-hongkong-1&quot; image_id = &quot;img-pi0ii46r&quot; instance_type = &quot;S2.SMALL1&quot; system_disk_type = &quot;CLOUD_PREMIUM&quot; security_groups = [ &quot;$&#123;tencentcloud_security_group.sg_test.id&#125;&quot; ] vpc_id = &quot;$&#123;tencentcloud_vpc.vpc_test.id&quot; subnet_id = &quot;$&#123;tencentcloud_subnet.subnet_test.id&#125;&quot; internet_max_bandwidth_out = 10 count = 1&#125; 123456$ vim vpc.tf// Create a vpcresource &quot;tencentcloud_vpc&quot; &quot;vpc_test&quot; &#123; name = &quot;vpc-test-liyk&quot; cidr_block = &quot;10.0.0.0/16&quot;&#125; 123456789$ vim subnet.tf// Create a subnetresource &quot;tencentcloud_subnet&quot; &quot;subnet_test&quot; &#123; name = &quot;subnet-test&quot; cidr_block = &quot;10.0.1.0/24&quot; availability_zone = &quot;ap-hongkong-1&quot; vpc_id = &quot;$&#123;tencentcloud_vpc.vpc_test.id&#125;&quot; route_table_id = &quot;$&#123;tencentcloud_route_table.rtb_test.id&#125;&quot;&#125; 123456$ vim route_table.tf// Create a route tableresource &quot;tencentcloud_route_table&quot; &quot;rtb_test&quot; &#123; name = &quot;rtb-test&quot; vpc_id = &quot;$&#123;tencentcloud_vpc.vpc_test.id&#125;&quot;&#125; 12345678910111213$ vim security_group.tf// Create a security group and ruleresource &quot;tencentcloud_security_group&quot; &quot;sg_test&quot; &#123; name = &quot;sg-test&quot;&#125;resource &quot;tencentcloud_security_group_rule&quot; &quot;sg_rule_test&quot; &#123; security_group_id = &quot;$&#123;tencentcloud_security_group.sg_test.id&#125;&quot; type = &quot;ingress&quot; cidr_ip = &quot;0.0.0.0/0&quot; ip_protocol = &quot;tcp&quot; port_range = &quot;22,80&quot; policy = &quot;accept&quot;&#125; 总结及案例使用Terraform的Scripting、Plan、Apply和Destroy四个步骤即可轻松实现基础架构资源的全生命周期管理。下面我们以腾讯云的CVM、MySQl、VPC和Security Group服务为例，搭建一个最简单的基础架构。 Scripting 12使用HashiCorp自己的声明型语言HCL编写资源编排脚本。由于是声明型语言，我们熟悉的过程型语言的一些高级特性，比如“for”循环，HCL是不支持的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#1、Provider infoprovider &quot;tencentcloud&quot; &#123; secret_id = &quot;AsVv2va1CE5ipdx4&quot; secret_key = &quot;KQdafafrtJ&quot; region = &quot;ap-shanghai&quot;&#125;#2、Create a VPC resourceresource &quot;tencentcloud_vpc&quot; &quot;main&quot; &#123; name = &quot;demo-VPC&quot; cidr_block = &quot;10.0.0.0/16&quot;&#125;#3、Create route tables for web and DBresource &quot;tencentcloud_route_table&quot; &quot;web&quot; &#123; name = &quot;demo-rt_web&quot; vpc_id = &quot;$&#123;tencentcloud_vpc.main.id&#125;&quot;&#125;resource &quot;tencentcloud_route_table&quot; &quot;db&quot; &#123; name = &quot;demo-rt_db&quot; vpc_id = &quot;$&#123;tencentcloud_vpc.main.id&#125;&quot;&#125;#4、CVM instancesresource &quot;tencentcloud_instance&quot; &quot;nginx&quot; &#123; instance_name = &quot;demo-nginx&quot; availability_zone = &quot;ap-shanghai-2&quot; image_id = &quot;img-pi0ii46r&quot; instance_type = &quot;S4.SMALL2&quot; security_groups = [ &quot;$&#123;tencentcloud_security_group.web.id&#125;&quot; ] vpc_id = &quot;$&#123;tencentcloud_vpc.main.id&#125;&quot; subnet_id = &quot;$&#123;tencentcloud_subnet.web.id&#125;&quot; internet_max_bandwidth_out = 10 count = 10&#125;#5、Mysql instanceresource &quot;tencentcloud_mysql_instance&quot; &quot;demo-mysql&quot; &#123; instance_name = &quot;demo-mysql&quot; mem_size = 1000 root_password = &quot;My_demo_mysql0001&quot; volume_size = 50 availability_zone = &quot;ap-shanghai-2&quot; engine_version = &quot;5.7&quot; internet_service = 0 intranet_port = 3306 parameters = &#123; max_connections = &quot;1000&quot; &#125; security_groups = [ &quot;$&#123;tencentcloud_security_group.db.id&#125;&quot; ] vpc_id = &quot;$&#123;tencentcloud_vpc.main.id&#125;&quot; subnet_id = &quot;$&#123;tencentcloud_subnet.db.id&#125;&quot; tags = &#123; name =&quot;demo-project&quot; &#125;&#125;#6、Create subnets within the VPCresource &quot;tencentcloud_subnet&quot; &quot;web&quot; &#123; name = &quot;demo-SN_web&quot; cidr_block = &quot;10.0.1.0/24&quot; availability_zone = &quot;ap-shanghai-2&quot; vpc_id = &quot;$&#123;tencentcloud_vpc.main.id&#125;&quot; route_table_id = &quot;$&#123;tencentcloud_route_table.web.id&#125;&quot;&#125;resource &quot;tencentcloud_subnet&quot; &quot;db&quot; &#123; name = &quot;demo-SN_db&quot; cidr_block = &quot;10.0.2.0/24&quot; availability_zone = &quot;ap-shanghai-2&quot; vpc_id = &quot;$&#123;tencentcloud_vpc.main.id&#125;&quot; route_table_id = &quot;$&#123;tencentcloud_route_table.db.id&#125;&quot;&#125;#7、Create security groups and rulesresource &quot;tencentcloud_security_group&quot; &quot;web&quot; &#123; name = &quot;demo-sg_web&quot; description = &quot;Accessible for both HTTP and SSH&quot;&#125;resource &quot;tencentcloud_security_group&quot; &quot;db&quot; &#123; name = &quot;demo-sg_db&quot; description = &quot;Accessible for both mysql and SSH from web&quot;&#125;resource &quot;tencentcloud_security_group_rule&quot; &quot;web-from-public&quot; &#123; security_group_id = &quot;$&#123;tencentcloud_security_group.web.id&#125;&quot; type = &quot;ingress&quot; cidr_ip = &quot;0.0.0.0/0&quot; ip_protocol = &quot;tcp&quot; port_range = &quot;80,22&quot; policy = &quot;accept&quot;&#125;resource &quot;tencentcloud_security_group_rule&quot; &quot;web-to-public&quot; &#123; security_group_id = &quot;$&#123;tencentcloud_security_group.web.id&#125;&quot; type = &quot;egress&quot; ip_protocol = &quot;tcp&quot; cidr_ip = &quot;0.0.0.0/0&quot; port_range = &quot;80,22&quot; policy = &quot;accept&quot;&#125;resource &quot;tencentcloud_security_group_rule&quot; &quot;mysql-from-webtier&quot; &#123; security_group_id = &quot;$&#123;tencentcloud_security_group.db.id&#125;&quot; type = &quot;ingress&quot; cidr_ip = &quot;10.0.1.0/24&quot; ip_protocol = &quot;tcp&quot; port_range = &quot;22,3306&quot; policy = &quot;accept&quot;&#125;resource &quot;tencentcloud_security_group_rule&quot; &quot;mysql-to-webtier&quot; &#123; security_group_id = &quot;$&#123;tencentcloud_security_group.db.id&#125;&quot; type = &quot;egress&quot; cidr_ip = &quot;0.0.0.0/0&quot; ip_protocol = &quot;tcp&quot; port_range = &quot;22,3306&quot; policy = &quot;accept&quot;&#125; Plan 12Terraform Plan功能可以很好的支持Terraform脚本执行前的检查确认工作。Terraform基于脚本、本地状态文件（terraform.tfstate）和云平台三者的一致性来保证执行结果的准确性。 Apply 12Terraform apply功能实现基础架构的一键部署。注意，apply前Terraform还是会强制进行资源的确认工作，即Terraform Plan工作。Terraform的执行结果会保存在本地状态文件（terraform.tfstate）中。 Destroy 1通过以上简单地三个步骤即可实现复杂的资源部署工作，同样的，仅需要一个简单地命令即可实现资源的快速高效释放。 优势1231.更高的部署效率。缩短了资源从开发需求到部署实施的流程，同时在批量部署以及多云部署场景下，IaC可以大幅提升资源部署的效率；2.增加了基础资源配置的一致性。由于采用声明型语言，资源配置更加易读，降低了人工犯错的几率；3.降低企业成本。将传统的云迁移工作大大简化，提高资源的利用率，从而有效降低企业云上的OPEX；]]></content>
      <categories>
        <category>Case</category>
      </categories>
      <tags>
        <tag>terraform</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL注入]]></title>
    <url>%2F2020%2F04%2F21%2FSQL%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[何谓SQL注入SQL注入是一种常见的数据库攻击手段，恶意用户通过在表单中填写包含SQL关键字的数据来使数据库执行非常规代码的过程。简单说，就是数据遇阻代庖做了代码才能干的事情。 例如：表单中插入DROP TABLE students;会执行删除表操作。 如何防止SQL注入 避免使用常见数据库名和表名 12尽量使用较为复杂的结构和命名方式.例如,使用项目名_库名_表名 使用正则表达式过滤 12使用正则表达式等字符串过滤手段限制数据项的格式、字符数目等。理论上避免数据项中存在引号、分号等特殊字符就能很大程度上避免SQL注入的发生。 数据库操作方式来执行数据项的查询与写入操作 1使用execute()方法来保证每次执行仅能执行一条语句，然后将数据项以参数的方式与SQL执行语句分离开来，就可以避免SQL注入问题。 做好备份 敏感数据进行加密]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引相关]]></title>
    <url>%2F2020%2F04%2F21%2FMysql%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[Mysql架构 12345678910不同的存储引擎，数据文件和索引文件存放的位置是不同的，因此有了分类：聚簇索引：数据和文件在一起（InnoDB） .frm：存放的是表结构 .ibd：存放数据文件和索引文件 注意：mysql的InnoDB存储引擎默认情况下会把所有的数据文件放到表空间中，不会每一个单独的表保存一份数据文件。如果需要将每一个表单独使用文件保存，设置如下属性：set global innodb_file_per_table=on;非聚簇索引：数据和索引单独一个文件（MyISAM） .frm：存放表结构 .MYI：存放索引数据 .MYD：存放实际数据 InnoDB与MyISAM存储引擎区别 两者主要特点 123InnoDB：行级锁、事务安全（ACID）、支持外键。InnoDB存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全存储引擎。InnoDB是为处理巨大量时拥有最大性能而设计的。它的CPU效率可能是任何其他基于磁盘的关系数据库引擎所不能匹敌的。InnoDB支持事务处理与外键和行级锁，而MyISAM不支持。 12MyISAM：表级锁、不支持事务和全文索引。适合一些CMS内容管理系统作为后台数据库使用，但是使用大并发、重负荷生产系统上，表锁结构的特性就显得力不从心。 两者性能厕所 12随着CPU核数的增加，InnoDB的吞吐量反而越好，而MyISAM，其吞吐量几乎没有什么变化.显然，MyISAM的表锁定机制降低了读和写的吞吐量。 事务支持与否 12InnoDB是事务安全的;事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而MyISAM就不可以了。 12MyISAM是一种非事务性的引擎，使得MyISAM引擎的MySQL可以提供高速存储和检索，以及全文搜索能力。适合数据仓库等查询频繁的应用。 两者构成上的区别1234数据和文件在一起（InnoDB） .frm：存放的是表结构 .ibd：存放数据文件和索引文件基于磁盘的资源是InnoDB表空间数据文件和它的日志文件，InnoDB 表的大小只受限于操作系统文件的大小，一般为2GB。 1234每个MyISAM在磁盘上存储成三个文件：第一个文件的名字以表的名字开始，扩展名指出文件类型，.frm文件存储表定义。第二个文件是数据文件，其扩展名为.MYD (MYData)。第三个文件是索引文件，其扩展名是.MYI (MYIndex) 哈希表：哈希冲突 1234567哈希表可以完成索引的存储，每次在添加索引的时候需要计算指定列的hash值，取模运算后计算出下标，将元素插入下标位置即可。使用场景： 等值查询 表中的数据是无序数据（范围查找的时候比较浪费时间，需要挨个进行遍历操作）在企业中多数的查询时范围查询还是等值查询？范围查询，所以此时hash表不是特别合适hash表在使用的时候，需要将全部的数据加载到内存，比较耗费内存的空间也不是很合适。 1二叉树及其N多的变种都不能支撑索引，原因是数的深度无法控制或者插入数据的性能比较低。 Mysql索引系统mysql索引数据结构 B+ Tree1234B+ Tree是在B Tree的基础上做的一种优化，变化如下：1. B+树每个节点可以包含更多的节点，原因有2个：降低树的高；将数据范围变为多个区间，区间越多数据检索越快。2. 非叶子节点存储key，叶子节点存储key和数据3. 叶子节点两两指针相互连接（符合磁盘的预读特性），顺序查询性能更高。 12注意：在B+树上有两头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点(数据节点)之间是一种链式环结构。因此可以对B+树进行两种查找运算：对于主键的范围查找和分页查找；另一种是从根节点开始，进行随机查找。 12341. InnoDB是通过B+树结构对主键创建索引，然后叶子节点中存储记录。如果没有主键，那么会选择唯一键；如果没有唯一键那么会生成一个6字节的row_id来作为主键。2. 如果创建索引的键是其他字段，那么叶子节点存储的是该记录的主键，然后再通过主键索引找到对应的记录，叫做回表。 mysql数据结构选择 hash表的索引格式 123缺点：1. 利用hash存储的话需要将所有的数据文件添加到内存，比较耗费内存空间。2. 如果所有的查询都是等值查询，那么hash确实很快。但是在企业或者实际工作环境中范围查找的数据更多，而不是等值查询，因此hash就不太适合了。 二叉树和红黑树索引格式 123二叉树：左子树上值小于它的根节点；右子树大于它的根节点；它的左右子树也分别为二叉排序树。红黑树：一种平衡二叉树，红黑树的每个节点都有存储位表示节点的颜色，可以是红或者黑。每个节点都是红或者黑；根节点是黑；如果一个节点是红色的，则它的子节点必须是黑色的。无论是二叉树还是红黑树，都会因为树的深度过深而造成IO次数变多，影响数据库读取的效率。 B树的索引格式 1234567B树特点：1. 所有键值分布在整颗树中2. 搜索有可能在非叶子节点结束，在关键字全集内做一次查找，性能逼近二分查找。3. 每个节点最多拥有m个子树4. 根节点至少有2个子树5. 分支节点至少拥有m/2颗子树（除根节点和叶子节点外都是分支节点）6. 所有叶子节点都在同一层，每个节点最多可以有m-1个key，并且以升序排列。 B+树 1234561. 根节点只有1个，分支数量范围[2,m]2. 除根以外的非叶子节点，每个节点包含分支数范围[[m/2],m]，其中[m/2]表示取大于m/2的最小整数。3. 所有非叶子节点的关键字数目等于它的分支数量。4. 所有叶子节点都在同一层，且关键字数目范围是[[m/2],m],其中[m/2]表示取大于m/2的最小整数。5. 所有非叶子节点的关键字可以看成是索引的部门，这些索引等于其子树(根节点)中的最大（或最小）关键字。6. 叶子节点包含全部关键字的信息（非叶子节点只包含索引），且叶子节点中的所有关键字依照大小顺序链接（所以一个B+树通常有两个头指针，一个是指向根节点的root，另一个是指向最小关键字的sqt）。]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ingress初体验]]></title>
    <url>%2F2020%2F04%2F11%2FIngress%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[前言123456789service暴露的三种方式ClusterIP、NodePort与LoadBalance，这三种都是在service的维度提供的。service的作用体现在2个方面:1. 对集群内部，它不断跟踪pod的变化更新endpoint中对应pod的对象，提供了ip不断变化的pod的服务发现机制。2. 对集群外部，它类似负载均衡器，可以在集群内外部对pod进行访问。但单独用service暴露服务的方式在实际生产环境中不太适用：1. ClusterIP的方式只能在集群内部访问2. NodePort方式在测试环境使用还行，但是有成百上千服务在集群中运行时，NodePort的端口管理很是麻烦。3. LoadBalance方式受限于云平台，而且需要额外费用。最大缺点是每个service用一个LB既浪费又麻烦。 ingress1234ingress则只需要一个NodePort或者一个LB就可满足所有service对外服务的需求。ingress相当于一个7层负载均衡器，是k8s对反向代理的一个抽象。Ingress Contronler 通过与 Kubernetes API 交互，能够动态的获取cluster中Ingress rules的变化，生成一段 Nginx 配置，再写到 Nginx-ingress-control的 Pod 里，reload pod 使规则生效。从而实现注册的service及其对应域名/IP/Port的动态添加和解析。 ingress组成1包括：ingress controller和ingress resources 12ingress controller：核心是一个deployment，实现方式有很多，比如nginx、contour、haproxy、Istio，需要编写的yaml有deploy、service、serviceaccount。其中service类型可以是NodePort或者LoadBalancer.ingress-controller的形式都是一个pod，里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的编号，根据ingress对象生成配置并应用新配置到反向代理，比如nginx-ingress就是动态生成nginx配置，动态更新upstream，并在需要的时候reload程序应用新配置。 12ingressingress resources: 就是一个类型为ingress的k8s api对象，通过yaml文件来配置。ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方式代理。ingress要依靠ingress-controller来具体实现以上功能。 部署ingress-controller123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238# yaml部署，mandatory.yamlapiVersion: v1kind: Namespacemetadata: name: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - &quot;extensions&quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - &quot;extensions&quot; resources: - ingresses/status verbs: - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps resourceNames: # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot; # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot; # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - &quot;ingress-controller-leader-nginx&quot; verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - endpoints verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 123456789101112131415161718192021222324# service-ingress.yamlapiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP #nodePort: 80 - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx 配置ingress12345678910111213141516171819202122232425262728# 方式一apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-1 namespace: default annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: # 配置七层域名 - host: liyk.key1024.cn http: paths: # 配置Context Path - path: /nginx1 backend: serviceName: myapp-svc servicePort: 80 - path: /nginx2 backend: serviceName: nginx servicePort: 80# 说明接着在hosts文件中添加一条解析规则：$&#123;ingress_IP&#125; foo.bar.com，这时通过在浏览器中访问：foo.bar.com/coffee或者foo.bar.com/tea即可访问对应的后端service了使用curl时的操作：curl -H &quot;Host: foo.bar.com&quot; http://$&#123;ingress_IP&#125;/coffee 1234567891011121314151617181920# 方式二apiVersion: extensions/v1beta1kind: Ingressmetadata: name: cafe-ingressspec: rules: # 配置七层域名 - host: tea.foo.bar.com http: paths: - backend: serviceName: tea-svc servicePort: 80 - host: coffee.foo.bar.com http: paths: - backend: serviceName: coffee-svc servicePort: 80 HTTPS TLS类型123# 生成证书openssl genrsa -out tls.key 2048openssl req -new -x509-key tls.key -out tls.crt -subj /C=CN/ST/Beijing/L=Beijing/O=DevOps/CN=tomcat.key1024.cn -days 3650 12# 创建secretkubectl create secret tls tomcat-ingress-secret --cert=tls.crt --key=tls.key 1234567891011121314151617181920# tomcat-ingress-tls.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: tomcat-ingress-tls annotations: kubernetes.io/ingress.class: &quot;nginx&quot;spec: tls: - hosts: - tomcat.key1024.cn secretName: tomcat-ingress-secret rules: - host: tomcat.key1024.cn http: paths: - path: / backend: serviceName: tomcat-svc servicePort: 80]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dockerocker中不适合部署数据库的原因]]></title>
    <url>%2F2020%2F03%2F20%2FDockerocker%E4%B8%AD%E4%B8%8D%E9%80%82%E5%90%88%E9%83%A8%E7%BD%B2%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[数据安全问题123容器的不持久性容器volumes设计是围绕unionFS镜像层提供持久存储，数据安全缺乏保证。如容器突然崩溃、数据库未正常关闭,可能会损坏数据。 性能问题1数据库对IO要求较高，当跑多个容器IO就会累加，导致IO瓶颈，大大降低数据库的读写性能。 网络问题12容器是虚拟机管理程序和主机虚拟机背后的一个隔离层，网络对于数据库复制是至关重要的，需要主从数据库间的稳定连接。容器化使数据库很难管理，花太多时间解决docker网络问题。 状态12docker中允许无状态服务很适合，但是数据库这类不是很适宜docker中水平伸缩只适用于无状态计算服务，而不是数据库 资源隔离12资源隔离方面，docker不如kvm，docker是利用cgroup实现资源限制的，只能限制资源消耗的最大值，而不能隔绝其他程序占用自己的资源。如果其他应用过度占用宿主机资源将会影响容器里mysql的读写效率。 运行数据库的环境需求1数据库(特别是关系型数据库)对IO要求较高，一般数据库引擎为了避免并发资源竞争而使用专用环境。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[K8s一些建议]]></title>
    <url>%2F2020%2F03%2F09%2FK8s%E4%B8%80%E4%BA%9B%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[Kubernetes提供了一种编排容器化服务的方法，因此如果您没有按顺序实践你的容器，那么集群一开始就不会处于良好状态。 容器最佳实践 使用精简基础镜像 12345what：容器是内置在系统镜像中的应用程序堆栈。从业务逻辑到内核的所有内容都打包在一起。最小的镜像会占用尽可能多的OS，并迫使您显式添加所需的任何组件。why：仅在您的容器中包括要使用的软件，同时具有性能和安全性方面的好处。磁盘上的字节数更少，复制镜像的网络流量更少，并且潜在的攻击者无法访问的工具也更少。how：Alpine Linux是一个流行的选择，并具有广泛的支持。 使用提供最佳正常运行时间的注册表 1234567what：注册表是镜像的存储库，使这些镜像可供下载和启动。在指定部署配置时，您需要指定从何处获取路径为&lt;registry&gt; / &lt;remote name&gt;：&lt;tag&gt;的镜像：why：您的集群需要镜像去运行。how：大多数云提供商都提供私有镜像注册表服务：Google提供Google容器注册表，AWS提供Amazon ECR，Microsoft提供Azure容器注册表。仔细调研，并选择提供最佳正常运行时间的私人注册表。由于您的群集将依靠您的注册表来启动软件的较新版本，因此任何停机时间都将阻止对正在运行的服务进行更新。 使用ImagePullSecrets对您的注册表进行身份验证 12345what：ImagePullSecrets是Kubernetes对象，可让您的群集通过注册表进行身份验证，因此注册表可以选择谁可以下载镜像。why：如果您的注册表足够公开，可以让集群从中提取镜像，则表明注册表足够公开，需要身份验证。how：Kubernetes网站在配置ImagePullSecrets方面有很好的演练，该示例使用Docker作为示例注册表。 管理集群 使用命名空间隔离环境 12345what：命名空间是Kubernetes中最基本，最强大的分组机制。它们几乎像虚拟集群一样工作。默认情况下，Kubernetes中的大多数对象仅限于一次影响单个名称空间。why：大多数对象都是在命名空间范围内定义的，因此您必须使用命名空间。鉴于它们提供了强大的隔离性，因此它们非常适合隔离具有不同目的的环境，例如用户服务的生产环境和严格用于测试的环境，或者分离支持单个应用程序的不同服务堆栈，例如保持安全解决方案的工作负载与您自己的应用程序分开。一个好的经验法则是按资源分配划分名称空间：如果两组微服务将需要不同的资源池，请将它们放在单独的名称空间中。how：它是大多数对象类型的元数据的一部分： 通过Labels 管理您的集群 12345what：Labels是组织集群的最基本且可扩展的方法。它们允许您创建用于分隔Kubernetes对象的任意key：value对。例如，您可以创建一个标签密钥，将处理敏感信息的服务与不处理敏感信息的服务区分开。why：如前所述，Kubernetes使用标签进行组织，但更具体地说，它们用于选择。这意味着，当您想给Kubernetes对象引用某个命名空间中的一组对象时（例如告诉网络策略允许哪些服务相互通信），请使用它们的标签。由于它们代表了这种开放式组织类型，因此请尽最大努力使事情简单化，并且仅在需要选择权的地方创建标签。how：标签是一个简单的规范字段，您可以将其添加到YAML文件中： 使用注释来跟踪重要的系统更改等 12345what：注释是可以附加到pod的任意键值元数据，就像标签一样。但是，Kubernetes不会读取或处理批注，因此围绕您可以和不能使用批注进行注释的规则相当宽松，并且不能用于选择。why：它们可帮助您跟踪容器化应用程序的某些重要功能，例如版本号或首次启动的日期和时间。仅在Kubernetes的上下文中，注释是一种无能为力的构造，但是当用于跟踪重要的系统更改时，注释可以成为开发人员和运营团队的资产。how：注释是类似于标签的规格字段。 让集群更加安全 使用RBAC实施访问控制 1234567what：RBAC（基于角色的访问控制）使您可以控制谁可以查看或修改群集的不同方面。why：如果要遵循最小特权原则，则需要设置RBAC来限制群集用户和部署能够执行的操作。how：如果要设置自己的集群（即不使用托管的Kube服务），请确保使用&apos;&apos;--authorization-mode = Node，RBAC“启动您的kube apiserver。如果使用托管的Kubernetes例如，您可以通过查询用于启动kube apiserver的命令来检查它是否设置为使用RBAC。唯一通用的检查方法是在kubectl cluster-info dump的输出中查找“ --authorization-mode ...”。RBAC打开后，您需要更改默认权限以适合您的需求。Kubernetes项目站点在此处提供了有关设置角色和RoleBindings的演练。托管的Kubernetes服务需要启用RBAC的自定义步骤-请参阅Google的GKE指南或Amazon的AKS指南。 使用Pod安全策略防止危险行为 12345what：Pod安全策略是一种资源，非常类似于Deployment或Role，可以通过kubectl以相同的方式创建和更新。每个都有一个标志集合，可用来防止集群中特定的不安全行为。why：如果创建Kubernetes的人认为限制这些行为足够重要，可以创建一个特殊的对象来处理它，那么它们很重要。how：让他们工作可能会令人沮丧。我建议启动并运行RBAC，然后在此处查看Kubernetes项目的指南。在我看来，最重要的使用是防止特权容器和对主机文件系统的写访问，因为它们代表了容器抽象中一些较泄漏的部分。 使用网络策略实施网络控制/防火墙 12345what：网络策略是允许您明确声明允许哪些流量的对象，而Kubernetes将阻止所有其他不符合标准的流量。why：限制群集中的网络流量是一项基本且重要的安全措施。默认情况下，Kubernetes启用所有服务之间的开放式通信。保留此“默认开放”配置意味着与Internet连接的服务与存储敏感信息的数据库仅一步之遥。how：有一篇文章写的很好，具体详情查看这里。 使用Secrets来存储和管理必要的敏感信息 12345what：Secrets是您如何在Kubernetes中存储敏感数据，包括密码，证书和令牌。why：无论您是实施TLS还是限制访问，您的服务都可能需要相互认证，与其他第三方服务或您的用户进行认证。how：Kubernetes项目在此处提供了指南。一个关键建议：避免将机密作为环境变量加载，因为在您的环境中拥有机密数据通常是不安全的。相反，将机密装入容器中的只读卷中-您可以在本 Use Secrets中找到一个示例。 使用镜像扫描识别和修复镜像漏洞 12345what：扫描仪检查镜像中安装的组件。从操作系统到应用程序堆栈的所有内容。扫描程序对于找出镜像所包含的软件版本中存在哪些漏洞非常有用。why：漏洞一直在流行的开源软件包中发现。一些著名的例子是Heartbleed和Shellshock。您将想知道这些漏洞在系统中的什么位置，以便您知道哪些镜像可能需要更新。how：扫描仪是基础设施中相当常见的部分-大多数云提供商都提供了产品。如果您想自己托管一些东西，那么开源Clair项目是一个受欢迎的选择。 保持集群稳定Kubernetes代表很高的技术栈。您拥有在嵌入式内核上运行的应用程序，在VM中运行的应用程序（在某些情况下甚至在裸机上），以及Kubernetes自己的服务共享硬件。考虑到所有这些因素，在物理和虚拟领域中很多事情都会出错，因此尽可能降低开发周期的风险非常重要。Kubernetes周围的生态系统已经开发了一系列最佳实践，以使事情尽可能保持一致。 遵循CI / CD方法 12345what：持续集成/持续部署是一种过程哲学。相信对代码库进行的每次修改都应增加增量值，并准备投入生产。因此，如果代码库中的某些内容发生了更改，则可能要启动服务的新版本，以运行测试。why：遵循CI / CD可以帮助您的工程团队在日常工作中牢记质量。如果出现问题，修复问题将成为整个团队的当务之急，因为此后依赖于已分解的提交的所有更改也将被分解。how：由于云部署软件的兴起，CI / CD越来越流行。因此，您可以从托管或自托管的众多出色产品中进行选择。如果您的团队比较小，我建议您采用托管路线，因为节省的时间和精力绝对值得您付出额外的费用。 使用Canary方法进行更新 12345what：Canary是一种将服务更改从代码库中的提交带给用户的方法。您启动了一个运行最新版本的新实例，然后将用户缓慢迁移到新实例，从而逐渐获得了对更新的信心，而不是一次全部交换。why：无论您的单元测试和集成测试有多广泛，它们都无法完全模拟生产中的运行-总是有可能某些功能无法按预期运行。使用金丝雀可以限制用户接触这些问题。how：Kubernetes的可扩展性提供了许多途径来逐步推出服务更新。最直接的方法是创建一个单独的部署，与当前正在运行的实例共享一个负载平衡器。这个想法是您扩展新的部署，同时缩减旧的部署，直到所有正在运行的实例都是新版本。 实施监控并将其与SIEM集成 12345what：监视意味着跟踪和记录您的服务正在做什么。why：让我们面对现实吧-不管您的开发人员多么出色，无论您的安全专家如何努力地发挥他们的聪明才智，事情都会出错。当他们这样做时，您将想知道发生了什么，以确保您不会两次犯相同的错误。how：成功监视服务有两个步骤-需要对代码进行检测，并且需要将该检测的输出馈送到某个地方以进行存储，检索和分析。执行检测的方式在很大程度上取决于您的工具链，但是快速的网络搜索应该可以让您有所作为。就存储输出而言，除非您有专门知识或需求，否则我建议使用托管SIEM（例如Splunk或Sumo Logic）-根据我的经验，DIY始终是与任何存储相关的期望时间和精力的10倍。 使用服务网格管理服务间通信 1234567what：服务网格是管理服务间通信的一种方法，可以有效地创建在实施服务时使用的虚拟网络。why：使用服务网格可以减轻管理群集的一些较繁琐的方面，例如确保对通信进行正确的加密。how：根据您对服务网格的选择，启动和运行的复杂性可能千差万别。作为最常用的服务网格，Istio似乎正在蓬勃发展，并且您的配置过程将在很大程度上取决于您的工作负载。一个警告：如果您需要采用一个服务网格，请尽早采用它而不是稍后采用它-逐渐改变集群中的通信样式可能会非常痛苦。 使用准入控制器解锁Kubernetes中的高级功能 12345what：准入控制器是一种很好的万能工具，可用于管理集群中发生的一切。它们允许您设置Kubernetes在启动时将参考的Webhook。它们有两种形式：变异和验证。突变准入控制器会在部署启动之前更改其配置。验证准入控制器会与您的webhook一致，以允许启动给定的部署。why：它们的用例广泛且数量众多–它们提供了一种通过自行开发的逻辑和限制来迭代地提高集群稳定性的好方法。how：查看有关如何开始使用Admission Controllers的指南。 [转载] 公众号:DevOps技术栈]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis问答及一些操作]]></title>
    <url>%2F2020%2F03%2F09%2FRedis%E9%97%AE%E7%AD%94%E5%8F%8A%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[分布式缓存Redis1. 什么是Redis NoSQL 分布式缓存中间件 key-value存储 提供海量的数据存储访问 所有数据放在内存中，读取速度非常快 非关系型，分布式、开源，支持水平扩展 2. 为什么使用内存缓存数据库因为在我们的基础架构，我们的数据库一般都是第一节点（主从、MMM、MHA） 一遍导致数据库缓慢或宕机的都是查询导致的，update、delete，80%的SQL都是查询，如果能够将这80%SQL抽离到缓存中 Redis VS Memcache VS Ehcache Ehcache 优点 基于Java开发的，被apache认证 基于JVM缓存的 简单、轻巧、方便（广泛的应用于hibernate，Mybatis） 缺点 不支持集群，单点 不支持分布式，存储容量不支持扩展 Memcache 优点 简单的key-value存储 内存使用率比较高 支持多核多线程 缺点 无法容灾 无法持久化 Redis 优点 丰富的数据结构 持久化：RDB、AOF 主从同步、故障转移（MySQL；主从） 内存数据库 缺点 单线程（不建议进行大数据量的存储） 单核（无法充分利用CPU多核性能，建议使用多实例） 3、Redis作为单线程模型为什么效率还这么高？1、纯内存访问：数据存放在内存中，内存的响应时间是100纳秒 2、非阻塞式的I/O操作：Redis采用epoll作为I/O多路复用技术的实现 3、采用单线程避免了不必要的上下文切换和竞争条件 什么是多路复用 如果你是一个老师，有30个学生做一道题，做完需要检查 1、按顺序检查，A，B，C 2、你创建30个分身，来检查30个结果 3、谁做完了谁举手，A，B、C，D 4. Redis服务安装及常用命令解析123456789101112131415# redis下载解压后yum -y install gcc-c++make &amp;&amp; make install# cd /usr/local/bin 将redis相关的命令安装进来# 进入redis根目录对redis.conf进行修改daemonize yes #后台执行dir /usr/local/redis-6379 #工作目录bind 0.0.0.0 #ip访问控制，这里是放开的意思requirepass icoding #设置redis密码port 6379pidfile /var/run/redis_6379.pid #进程id存放文件maxcliens 0 #客户端的最大连接数，默认0就是不限制timeout 0 #客户端连接的超时时长，默认0就是关闭不限制#启动Redisredis-server redis.conf Redis五大数据类型 string 12345678910keys * #查看redis当前数据库所有的key-value，生成上不要用keys abc*set / setnx : 后者如果有数据就不生效，前者有数据就覆盖expire username# redis默认配置有16个DB数据库,可以增减flushdb #删除当前DB中所有数据flushall #删除所有DB中的数据# 如果要关闭这两个命令的rename-command FLUSHDB "" # rename-command FLUSHDB "icodingfloushdb"rename-command FLUSHALL "" hash：对象 12345678910# 比如一个对象,这个一般在购物车中使用比较多user &#123; name: icoding age: 18 sex: male&#125;hset user name icoding age 18 sex maletype userhget user namehgetall user list：栈 1234# 做管道通知lpush userList 1 2 3 4 5lpop userListrpop userList set：集合 1234sadd userSet 1 2 3 4 5 6 5 4 3 2 1smembers userSetsrandmember userSet 2 #随机展示两个spop userSet 2 #随机取两个 可以做抽奖 zset：带有一个数据标签的集合 12zadd za1 10.99 apple 20.11 peach 40.89 banana 30.80 pear 50.79 cherryzrange za1 0 -1 withscores GEO 1234567# 在Redis3.2版本后才支持# 添加地理坐标geoadd china:city 116.408 39.904 beijing 121.445 31.213 shanghai 113.265 23.108 guangzhou 114.109 22.544 shenzhen 108.969 34.285 xian 108.55 34.09 changan# 获得两个节点间的距离geodist china:city shanghai beijing km # m、km、mi、ft# 搜一搜周边，摇一摇georadiusbymember china:city beijing 1300 km withdist withcoord asc count 2 5. 如何实现Redis数据持久化如果我们Redis宕机内存中的数据没了，这个时候会发生什么？就会导致原来所有从Redis读的请求都去到DB了 确保我们重启完Redis还能将绝大部分的数据恢复进内存，怎么办？ 是不是就要把内存 数据保存到磁盘便于恢复 5.1. RDB模式就是每隔一段时间，定时保存，有点像MySQL中进程用到的mysqldump 默认redis就是开启RDB的 优势 每隔一段时间，全量备份 灾备简单，dump.rdb文件拷走就完了 在RDB备份的时候会fork一个新进程来操作，这就不影响提供读写进程的效率了 劣势 当备份后和故障间这段时间的数据无法保存 新fork的子进程会从父进程copy全部的内存数据（这个时候内存会瞬间膨胀两倍），会造成CPU和内存负担 由于是定时的备份，所以时效差 1234567891011# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changedsave 900 1 # 在900秒内1个key被更新，就触发一次RDB备份save 300 10save 60 10000# 那这个rdb文件放在哪呢？dir /usr/local/redis-6379 #可以通过 config get dirdbfilename dump.rdb #rdb的文件名# yes : 如果save过程中出错，则停止redis服务的写操作stop-writes-on-bgsave-error yes 注意的点：执行的备份命令是bgsave / 如果是使用save会阻塞redis的主进程 5.2. AOF模式有点类似于mysql的binlog，他是把我们所有Redis的写操作命令记录下来了 AOF的特点 以日志的形式来记录用于的写操作 文件是以追加的方式而不是修改的方式 redis的aof的恢复其实就是把文件从头到位执行一遍 优势 每秒数据的记录和操作 aof的文件也是一个，所以当文件比较大的时候会触发aof文件重写机制进行文件压缩 劣势 同样的数据，AOF比RDB大的多 aof同步的时候比rdb慢的多 AOF重写的时候也会fork一个进程来操作 1234567891011121314# AOF默认是关闭的，需要手工启用appendonly yes# AOF文件名appendfilename "appendonly.aof"# 这就是AOF的持久化频率# everysec：每秒备份一次，推荐使用# always：每次操作都备份appendfsync everysec# 触发重写的两个条件# 当现有aof文件比上次大了100%，就触发重写auto-aof-rewrite-percentage 100# 当现有文件大于64mb的时候，就触发重写# 这两个条件同时满足才会触发重写auto-aof-rewrite-min-size 64mb #变成100mb，要等到200mb才触发重写 重启redis不要使用kill进程的方法，这样会导致redis当前数据无法写入aof或rdb 使用客户端的shutdown来安全关闭redis 5.3. 持久化化文件是如何恢复的 RDB文件只需要放在dir目录下我们的Redis会在重启后自动加载 AOF文件也是只需要放在dir目录下我们的Redis会在重启后自动加载 RDB和AOF不互相通信的 AOF启用后，Redis优先选择AOF 如果线上没有开启aof，这个时候需要开启，不要进行配置修改后重启来生成aof文件 使用内部命令先开启config set appendonly yes 再去redis.conf里把appendonly 设置成yes 6. Redis内存管理之缓存过期机制 主动删除 默认1秒巡检10次定义了expire的key，如果过期就删除 可以设置redis.conf hz 10 惰性删除 如果你在访问的时候Redis发现这个key过期，就会返回nil并删除 是调用内部的expireIfNeeded()这个方法 如果超时比较久并且不超时的key比较多，redis内存满的怎么办？ 这就Redis内存缓存的管理机制 123456789101112# maxmemory &lt;bytes&gt; 限定主机的可写入最大内存阀值,还要给系统留一点# redis可写内存逻辑上是决定于主从结构中最小主机的内存 master 8g，slave 4g# 内存淘汰的策略# maxmemory-policy noeviction //这是默认，旧缓存用不过期，如果写满，新的缓存则无法写入# volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru //建议设置这个，如果内存满了，清除最旧最少用的缓存# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don't evict anything, just return an error on write operations. 7. Redis高可用模型主从架构搭建为什么要使用主从 HA：高可用 高并发：读写分离 建议的主从结构，最好是1 master，2 slave 123456# 单机安装参考前面# slave的配置# master的配置replicaof 127.0.0.1 6379masterauth icoding127.0.0.1:6379&gt;info replication 但现在如何主机宕机了，redis是不会自动将master切换大其中一台slave上的 8. Redis故障转移哨兵模式分析redis的sentinel哨兵 集群监控 消息通知 故障转移 配置中心 1234567891011121314151617# 建议哨兵也搭建成集群方式，3台redis就搭建3个sentinel# 先完成一个哨兵的配置,将sentinel.conf进行修改# 访问控制关闭protected-mode noport 26379daemonize yespidfile /var/run/redis-sentinel-26379.piddir /usr/local/redis-6379/sentinellogfile /usr/local/redis-6379/sentinel/redis-sentinel.log# 哨兵监控的master节点，后面这个2是指几个哨兵发现master宕机了，才进行故障转移sentinel monitor mymaster 127.0.0.1 6379 2# 设置主机访问密码sentinel auth-pass mymaster icoding# 这是哨兵多久连接不上master就认为master宕机了的时间，单位是毫秒sentinel down-after-milliseconds mymaster 3000# 新的master出现后，其他follow的slave并行同步的个数，并行的越多同时阻塞的就越多sentinel parallel-syncs mymaster 1 其他sentinel配置需要根据实际情况修改端口号即可 123456# 启动哨兵redis-sentinel sentinel.conf# 进入哨兵进行状态查询sentinel master mymastersentinel slaves mymastersentinel sentinels mymaster 123456789101112131415#单机设置spring redis host: 127.0.0.1 port: 6379 password: 123.com #哨兵在springboot里则需要连接哨兵的ip和端口spring: redis: host: 127.0.0.1 password: 123.com sentinel: master: mymaster nodes: 127.0.0.1:26379,127.0.0.1:26381,127.0.0.1:26382 主从 IP访问需要你自己去关联，你要访问哪个IP就走哪个IP 哨兵 做了一个主从的master故障转移操作 slave的读操作还是要你自己指定 8. Redis分布式集群架构实战刚刚我们学习的内容是不所有方式下，无论是主从（HA）还是单机，其实内存大小受限于一台服务器 如果这个master的内存快满了，你怎么扩展，如果扩展不了，redis何以称为分布式 10个key，A，B，C三个节点，A放了2个，B放了4个，C放了4个 集群的特性 HA的：每个集群节点都是一组M/S（主/从），为什么是三组节点？3个节点，6个Redis 分布式的： 三个Master，三个slave 数据是根据crc16（key）mod 16384分摊到三个master节点上 Slot槽点 Redis会在创建的时候生成16384个slot（固定数值，不变，可以理解为文件夹） 16384/3=5461（如果除不尽，会在一个master上多放一个slot） 每一个key在set时会hash个固定文件夹里，三个master的total才是完整数据 每个slot原则上可以放无数个key，依赖于内存大小 如果三台master内存不够需要扩展 只需要在集群中加入新的master 把现有master上的slot移动一部分给他就行]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK结合redis或kafka收集日志]]></title>
    <url>%2F2020%2F03%2F09%2FELK%E7%BB%93%E5%90%88redis%E6%88%96kafka%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[filebeat+redis+logstash filebeat配置 12345678910111213141516171819# filebeat-redis.ymlfilebeat.inputs:- type: log enabled: true backoff: &quot;1s&quot; tail_files: false paths: - /var/log/nginx/access.log fields: filetype: nginx1 fields_under_root: trueoutput.redis: enabled: true hosts: [&quot;10.90.25.20:6379&quot;] key: nginx db: 0 password: xxzx@789 datatype: list logstash配置 1234567891011121314151617181920212223242526272829# logstash-redis.confinput &#123; redis &#123; host =&gt; &quot;43.254.44.156&quot; port =&gt; 6379 password =&gt; &quot;xxzx@789&quot; key =&gt; &quot;nginx&quot; data_type =&gt; &quot;list&quot; db =&gt; 0 &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;HTTPD_COMBINEDLOG&#125;&quot; &#125; &#125; date &#123; match =&gt; [&quot;timestamp&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;] target =&gt; &quot;@timestamp&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;127.0.0.1:9200&quot;] index =&gt; &quot;nginx-redis-%&#123;+YYYY.MM.dd&#125;&quot; &#125;&#125; filebeat+kafka+logstash filebeat配置 123456789101112131415filebeat.inputs:- type: log enabled: true backoff: &quot;1s&quot; tail_files: false paths: - /var/log/messages fields: filetype: kafka fields_under_root: trueoutput.kafka: enabled: true hosts: [&quot;10.57.22.170:9092&quot;] topic: liyk logstash 1234567891011121314input &#123; kafka &#123; bootstrap_servers =&gt; &quot;10.57.22.170:9092&quot; topics =&gt; [&quot;liyk&quot;] group_id =&gt; &quot;liyk_id&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;127.0.0.1:9200&quot;] index =&gt; &quot;kafka-liyk&quot; &#125;&#125;]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK基础配置]]></title>
    <url>%2F2020%2F03%2F09%2FELK%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[二进制安装ELK 相关地址1234下载：https://www.elastic.co/cn/downloads/文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html elsticsearch安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051521.下载解压wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.0-linux-x86_64.tar.gztar zxvf elasticsearch-7.6.0-linux-x86_64.tar.gz2.添加jdk# elasticsearch中含有jdk，只需将其加入环境变量即可vim /etc/profileexport JAVA_HOME=/data/elasticsearch/jdk# 创建普通用户useradd esvim /home/es/.bash_profileexport PATH=$JAVA_HOME/bin:$PATHsource /etc/profilesource /home/es/.bash_profile3.设置系统参数vim /etc/sysctl.confvm.max_map_count=262144sysctl -p# 设置可打开的文件描述符的最大数vim /etc/security/limits.conf* soft nofile 65536* hard nofile 65536ulimit -Hnulimit -Sn4.修改es配置文件vim config\elasticsearch.ymlcluster.namenode.namepath.datapath.logsnetwork.hosthttp.portcluster.initial_master_nodes# 设置jvm内存大小vim config\jvm.options-Xms1g-Xmx1g5.修改相关目录权限chown -R es:es elasticsearch # es文件目录chown -R es:es es # es日志和数据目录6.添加启动脚本vim startup.sh#!/bin/bashnohup /data/elasticsearch/bin/elasticsearch &amp;# 关闭elasticsearchjpskill -9 进程号 kibana安装123456789101112131415161.下载解压wget https://artifacts.elastic.co/downloads/kibana/kibana-7.6.0-linux-x86_64.tar.gztar zxvf kibana-7.6.0-linux-x86_64.tar.gz2.配置文件vim config\bibana.ymlserver.port: 5601server.host: &quot;0.0.0.0&quot;elasticsearch.hosts: [&quot;http://localhost:9200&quot;]elasticsearch.username: &quot;kibana&quot;elasticsearch.password: &quot;123456&quot;3.添加启动脚本vim startup.sh#!/bin/bashnohup /data/kibana/bin/kibana --allow-root &amp; logstash安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Logstash是一个开源的数据收集引擎，它具有备实时数据传输能力。# 它可以统一过滤来自不同源的数据，并按照开发者制定的规范输出到目的地，支持正则表达式。1.下载解压wget https://artifacts.elastic.co/downloads/logstash/logstash-7.6.0.tar.gztar logstash-7.6.0.tar.gz2.修改配置文件vim config/jvm.options-Xms128m-Xmx128m# 加载的配置文件3.编写收集配置文件vim logstash1.confinput &#123; # 从文件读取日志信息 file &#123; path =&gt; &quot;/var/log/messages&quot; type =&gt; &quot;system&quot; start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123;&#125;output &#123; # 标准输出，输出到控制台 stdout &#123;&#125; # 输出到es #elasticsearch &#123; #hosts =&gt; [&quot;127.0.0.1:9200&quot;] #index =&gt; &quot;msg-%&#123;+YYYY.MM.dd&#125;&quot; #&#125;&#125;4.启动# 配置启动脚本vim startup.sh#!/bin/bashnohup /data/logstash/bin/logstash -f /data/logstash/config/logstash2.conf &amp;# 数据处理流程:input-&gt;解码-&gt;filter-&gt;编码-&gt;output# 安装插件./logstash-plugin install xxxx./logstash-plugin list # 查看已安装的插件# 安装beat日志input输入./logstash-plugin install logstash-input-beats filebeat安装1234567891011121314151617181920211.下载wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.6.0-linux-x86_64.tar.gz2.配置文件vim filebeat1.ymlfilebeat.inputs:- type: log enabled: true backoff: &quot;1s&quot; tail_files: false paths: - /var/log/nginx/access.log#output.elasticsearch:# hosts: [&quot;localhost:9200&quot;] output.console: enabled: true 3.启动./filebeat -e -c filebeat1.yml 收集多个日志 123456789101112131415161718192021222324filebeat.inputs:- type: log enabled: true backoff: &quot;1s&quot; tail_files: false paths: - /var/log/nginx/access.log fields: filetype: nginx fields_under_root: true- type: log enabled: true backoff: &quot;1s&quot; tail_files: false paths: - /var/log/messages fields: filetype: linux fields_under_root: true #自定义字段将为文档中的顶级字段output.logstash: enabled: true hosts: [&quot;x.x.x.x:5044&quot;]]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2F2020%2F02%2F10%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言123缓存：数据可以丢，追求的是急速数据库：数据绝对不能丢，速度+持久性所以redis+mysql都做数据库就不太合适 12345存储层：1、快照/副本RDB：时点性2、日志AOF 123456管道：1、衔接：前一个命令的输出作为后一个命令的输入2、管道会触发创建[子进程]# $$优先级高于管道,所以普通$取值前先执行管道开辟新进程。# 子进程看不到父进程数据，但是父进程数据可以让子进程看到（export）；export环境变量后的子进程数据修改数据不会影响父进程，父进程修改数据也不会影响子进程。# redis创建子进程速度快，内存空间小。fork() redis的写时复制 redis RDB 简介 1RDB是redis进行持久化的一种方式，是把当前内存中的数据集快照写入磁盘，也就是snapshot快照(数据库中所有键值对数据)，恢复时是将快照文件直接读到内存里。 持久化触发方式 自动触发 12# redis.conf 配置文件中的SNAPSHOTTING下save m n :表示m秒内数据集存在n次修改时，自动触发bgsave。 手动触发 121、save：该命令会阻塞当前Redis服务器，执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止。所以出现了bgsave。2、bgsave：该命令执行后Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求。具体操作是Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。 恢复数据 123将备份文件（dump.rdb）移动到redis安装目录并启动服务即可，redis会自动加载文件数据至内存。注意：Redis 服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止。获取redis的安装目录可以使用 config get dir 命令 优缺点 12341、不支持拉链，只有一个dump.rdb。且备份时占用内存（因为Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍哦），最后再将临时文件替换之前的备份文件。）。2、想对容易丢失数据，时间点直接的窗口数据容易丢失。3、优点类似java中的序列化，恢复的速度相对快。4、适合大规模数据恢复 redis AOF 简介 12Redis 默认不开启。它的出现是为了弥补RDB的不足（数据的不一致性），所以它采用日志的形式来记录每个写操作，并追加到文件中。Redis重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 配置 123456789101112131415161、修改redis.conf,将appendonly 的no改为yesappendonly yes2、指定本地数据库文件名，默认值为 appendonly.aofappendfilename &quot;appendonly.aof&quot;3、指定更新日志条件# appendfsync alwaysappendfsync everysec# appendfsync no# 解说：always：同步持久化，每次发生数据变化会立刻写入到磁盘中。性能较差当数据完整性比较好（慢，安全）everysec：出厂默认推荐，每秒异步记录一次（默认值）no：不同步4、配置重写触发机制auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 解说：当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发。一般都设置为2-3G，64M太小了。 数据恢复 12345正常情况下，将appendonly.aof文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。但在实际开发中，可能因为某些原因导致appendonly.aof文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof --fix appendonly.aof 进行修复。# 注意：redis中，RDB和AOF可同时开启，但是如果开启了AOF后只会用AOF恢复。4.0版本后，AOF包含RDB全量，增加记录新的写操作。 优缺点 12优点：数据的完整性和一致性更高缺点：因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢。]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python脚本备份数据到OSS]]></title>
    <url>%2F2020%2F01%2F20%2Fpython%E8%84%9A%E6%9C%AC%E5%A4%87%E4%BB%BD%E6%95%B0%E6%8D%AE%E5%88%B0OSS%2F</url>
    <content type="text"><![CDATA[需求背景123客户需要将数据备份到对象存储（windows环境）最初使用的是osscmd工具(此工具目前已下线)将压缩的数据上传到oss，此方法是先利用bat脚本对目标路径进行压缩打包，然后使用osscmd上传，再进行上传检查告警通知。目前改良后使用python压缩以及调用api上传数据，根据时间创建对应oss目录。 一些步骤 安装环境（windows） 123456781. 安装python3https://www.python.org/ftp/python/3.8.1/python-3.8.1-amd64.exe配置系统环境变量2. 安装相关模块pip install oss2pip install pyyaml3. 安装压缩软件此处使用的winara，在配置文件config.yaml中设置winara路径。 修改配置文件1根据自身环境情况设置 配置任务计划1根据自身需求配置任务计划 测试 相关代码1234567891011121314151617181920# config.yaml# 配置文件# 客户名称name: liyk# 压缩命令路径rar_path: D:\Program Files (x86)\WinRAR\WinRAR.exe# 压缩目标目录compressdir: D:\\liyk# 压缩目的路径compress_dst: D:\\# 压缩名#compressname:# oss公网urlossurl: http://oss-cn-xxx.aliyuncs.com# bucket名ossbucket: xxzx-test# 阿里云api密钥keyid: LTAIhf******6tWkeysecret: GwfAMvR********gRfAso 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# oss_oper.py#! /usr/bin/env python# -*- coding:utf-8 -*-# Auther: liyk time:2018/12/11# File : oss_oper.pyimport oss2import yamlimport osimport datetime,timeimport logging# 日志配置LOG_FORMAT = &quot;%(asctime)s - %(levelname)s - %(message)s&quot;DATE_FORMAT = &quot;%m/%d/%Y %H:%M:%S %p&quot;logging.basicConfig(filename=&apos;oss_upload.log&apos;, level=logging.DEBUG, format=LOG_FORMAT, datefmt=DATE_FORMAT)# 获取配置文件curpath = os.path.dirname(os.path.realpath(__file__))yamlpath = os.path.join(curpath, &quot;config.yaml&quot;)f = open(yamlpath, &apos;r&apos;, encoding=&apos;utf-8&apos;)cfg = f.read()d = yaml.load(cfg, Loader=yaml.FullLoader)f.close()# 定义上传对象名和上传文件名i = datetime.datetime.now()now_date = &quot;&#123;&#125;/&#123;&#125;&quot;.format(i.year,i.month)name = d.get(&apos;name&apos;)ObjName = f&apos;&#123;name&#125;/&#123;now_date&#125;/&#123;name&#125;-&#123;i.month&#125;-&#123;i.day&#125;.rar&apos;LocalFile = f&apos;&#123;name&#125;.rar&apos;print(LocalFile)# 压缩目的目录和压缩名target_dir = d.get(&apos;compress_dst&apos;)target = target_dir + d.get(&apos;name&apos;) + time.strftime(&apos;%m-%d&apos;) + &apos;.rar&apos;def Auth(id, key): # 认证 Auth = oss2.Auth(id, key) bucket = oss2.Bucket(Auth, d.get(&apos;ossurl&apos;), d.get(&apos;ossbucket&apos;)) return bucketdef upload(bucket, ObjName, target): # 端口续传 上传文件 logging.info(&quot;Start uploading...&quot;) oss2.resumable_upload(bucket, &apos;%s&apos; %ObjName, &quot;%s&quot; %target, store=oss2.ResumableStore(root=&apos;D:\\&apos;), multipart_threshold=20000*1024, part_size=10000*1024, num_threads=4) logging.info(&quot;Backup upload completed&quot;)def list_object(bucket): # 列出文件 obj_list = [] for obj in oss2.ObjectIterator(bucket, delimiter=&apos;/&apos;): if obj.is_prefix(): print(&apos;dir:&apos; + obj.key) else: obj_list.append(obj.key) return obj_listdef delete_obj(bucket,obj_list): # 删除文件 for i in obj_list: bucket.delete_object(&apos;%s&apos; %i)def compress_data(): # 压缩 source = d.get(&apos;compressdir&apos;) rar_path = d.get(&apos;rar_path&apos;) rar_command = f&apos;&quot;&#123;rar_path&#125;&quot; a &#123;target&#125; &#123;source&#125;&apos; # 判断压缩文件是否存在 if os.path.exists(target): logging.info(&quot;file exists!&quot;) pass else: os.system(rar_command) logging.info(&quot;Data compression successful&quot;)if __name__ == &quot;__main__&quot;: id = d.get(&apos;keyid&apos;) key = d.get(&apos;keysecret&apos;) compress_data() Bucket = Auth(id, key) upload(Bucket, ObjName, target) # obj_list = list_object(Bucket) # delete_obj(Bucket, obj_list)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等保整改(linux主机)]]></title>
    <url>%2F2020%2F01%2F03%2F%E7%AD%89%E4%BF%9D%E6%95%B4%E6%94%B9-linux%E4%B8%BB%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Linux终端系统安全检查与整改 口令周期设置 123456vim /etc/login.defs# 添加如下PASS_MAX_DAYS 90PASS_MIN_DAYS 5PASS_MIN_LEN 5PASS_WARN_AGE 5 密码复杂度 123vim /etc/pam.d/system-auth# 添加如下password requisite pam_cracklib.so retry=3 difok=3 minlen=10 ucredit=-1 lcredit=-2 dcredit=-1 ocredit=-1 登录锁定 123vim /etc/pam.d/sshd# 添加如下，在第2行位置auth required pam_tally2.so deny=3 unlock_time=600 even_deny_root=5 root_unlock_time=1200 root权限 1vim /etc/passwd,修改权限为0的用户 wheel组用户1vim /etc/group SSH策略 12345678910# 修改端口vim /etc/ssh/sshd_configPort 22122# 修改root登录PermitRootLogin noMaxAuthTries 3# 禁止密码认证登录PasswordAuthentication no# 只允许root和xxx用户登录AllowUsers root xxx]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s组件简介]]></title>
    <url>%2F2019%2F12%2F15%2Fk8s%E7%BB%84%E4%BB%B6%E7%89%88%E6%9C%AC%E5%8F%8A%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[组件说明 etcd 123456789版本:v3.3高可用:部署奇数台服务器，以便leader选主功能:1.使用raft一致性算法来实现的一款分布式kv存储2.用于存储kubernetes集群中的配置、服务发现、各对对象的状态以及元数据信息等3.用于存储网络插件flannel、calico等网络配置信息注意事项:1.网络插件flannel操作etcd时，只能使用v2版本的API2.kubernetes集群需要使用v3版本的API docker引擎 1234567版本:19.03下载:yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo &amp;&amp; yum makecache &amp;&amp; yum -y install docker-ce功能:1.提供容器操作的底层引擎2.为kubernets中Pod提供容器注意事项:1.需要配合flannel提供的网络插件一起使用 kube-apiserver 12345678910111213版本:1.17下载:https://dl.k8s.io/v1.17.0/kubernetes-server-linux-amd64.tar.gz高可用:水平扩展（横向扩展）实现功能:1. kube-apiserver负责与etcd交互，其它组件不与etcd交互2.对集群进行操作和管理都需要通过kube-apiserver组件提供的API来完成3.集群中各组件不会相互调用，都是通过kube-apiserver组件来完成4.kube-apiserver开启bootstrap token认证，支持kubelet TLS bootstrapping，确保集群安全5.为避免其它组件频繁查询apiserver，apiserver为其它组件提供了watch接口，用于监视资源的增删改操作注意事项:1.关闭非安全端口8080和匿名访问2.使用安全端口64433.使用nginx 4层代理与keepalive实现高可用 kube-controller-manager 123456789101112131415版本:1.17下载:https://dl.k8s.io/v1.17.0/kubernetes-server-linux-amd64.tar.gz高可用:leader选举实现，只有leader处于运行状态，其它副本去竞争leade功能:1.它是控制器管理器，逻辑上每个控制器都有一个单独的协程2.每个控制器，负责监视（watch）apiserver暴露的集群状态，并不断地尝试把当前状态向所期待的状态迁移3.默认非安全端口10252，安全端口102574.使用kubeconfig访问kube-apiserver安全端口5.使用approve kubelet证书签名请求（CSR），证书过期后自动轮转6.kube-controller-manager各节点使用自己的ServiceAccount访问kube-apiserver7.kube-controller-manager 3节点高可用，去竞争leader锁，成为leader注意事项:1.需要--leader-elect=true启动参数2.安全端口和非安全端口都需要打开3.kubectl get cs使用的是http| kube-scheduler 123456789101112131415版本:1.17下载:https://dl.k8s.io/v1.17.0/kubernetes-server-linux-amd64.tar.gz高可用:leader选举实现，只有leader处于运行状态，其它副本去竞争leader功能:1.使用kubeconfig访问kube-apiserver安全端口2.kube-scheduler 3节点高可用，自选举；3.它监视kube-apiserver提供的watch接口，找一个最佳适配的节点，然后创建资源，如Pod资源;4.需要根据预选（Predicates）与优选（Priorities）两个环节找最佳适配节点；5.预选策略是过滤掉那些不满足Polices的节点，预选的输出作为优选的输入；6.优选策略是根据预选后的节点进行打分排名，得分最高的节点作为合适的节点，要创建的资源即被调到到此节点；7.默认非安全端口10251，安全端口10259|注意事项:1.需要--leader-elect=true启动参数；2.安全端口和非安全端口都需要打开；3.kubectl get cs使用的是http；| kubelet 12345678910版本:1.17下载:https://dl.k8s.io/v1.17.0/kubernetes-node-linux-amd64.tar.gz高可用:每台机器部署，不需要高可用，节点故障会自动迁移到其它节点功能:1.每个Node节点上面都可以接受master节点下发的任务，这些任务由kubelet来管理，主要用来管理Pod和其中的容器；2.kubelet会在API Server上注册节点信息，它内置了cAdvisor，通过它监控容器和节点资源，并定期向Master节点汇报资源使用情况；&lt;br&gt;3.使用kubeadm动态创建bootstrap token；4.使用TLS bootstrap机制自动生成Client和Server证书，过期后自动轮转；5.默认非安全端口10248，安全端口10250；6.安全端口接受https请求，对请求进行认证和授权；7.使用kubeconfig访问kube-apiserver的安全端口| kube-proxy 123456789高可用:每台机器部署，不需要高可用，节点故障会自动迁移到其它节点功能:1.service是一组Pod的抽象，相当于一组pod的负载均衡器，负责将请求分发到对应的pod；2.kube-proxy就是负责service实现的，内部请求到达service，它通过label关联并转发到后端某个pod;3.外部的node port向service的访问，通过label关联并转发到后端的某个pod；4.kube-proxy提供了三种负载均衡模式（代理模式），用户态（userspace）、iptables、ipvs模式；5.service的服务暴露方式由NodePort、hostNetwork、LoadBalancer、Ingress；6.使用kubeconfig访问kube-apiserver的安全端口；7.默认非安全端口10249，安全端口10256； flanneld 12345678版本:0.11下载:https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz高可用:每台机器部署，不需要高可用，节点故障会自动迁移到其它节点|功能:1.为每台Node节点分配Pod网段2.为集群Service网络分配IP（如有需要）3.flanneld只是做为网络插件的一种，还有很多；注意事项:flanneld使用的是v2版本etcd的API 插件coredns 1234567版本:1.4下载:coredns/coredns:1.4.0高可用:以Pod的方式部署在集群当中，部署2个副本功能:1.通过监听service与endpoints的变更事件，将域名和IP对应信息同步到CoreDNS配置中；2.CoreDNS中ClusterIP，这个IP是虚拟的，具有TCP/IP协议栈，早期版本无法ping通些IP（这个是iptables规则没有允许icmp通过），新版本中可以ping通；注意事项:Pod副本数，根据需要自行调整]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s+jenkins+gitlab+harbor]]></title>
    <url>%2F2019%2F11%2F03%2Fk8s-jenkins-gitlab-harbor%2F</url>
    <content type="text"><![CDATA[k8s中安装jenkins传统jenkins集群存在的问题 主master发生单点故障时，整个流程都不可用 每个Slave的环境配置不一样，来完成不同语言的编译打包，但是这些差异化的配置导致管理起来不方便，维护麻烦 资源分配不均衡，有的slave要运行的job出现排队等待，而有的salve处于空闲状态 资源有浪费，每台slave可能是物理机或者虚拟机，当slave处于空闲状态时，也不能完全释放掉资源 正因为上面的问题，我们需要采用一种更高效可靠的方式来完成这个CI/CD流程。而Docker虚拟化容器技能很好的解决这个痛点，又特别是在Kubernetes集群环境下面能够更好来解决上面的问题。 k8s部署jenkins点评 如上图我们可以看到Jenkins master和Jenkins slave以Pod形式运行在Kubernetes集群的Node。Master运行在其中一个节点，并且将其配置数据存储到一个volume上去，slave运行在各个节点上。但是它的状态并不是一直处于运行状态，它会按照需求动态的创建并自动删除。 运行流程 121、当Jenkins Master接受到Build请求后，会根据配置的Label动态创建一个运行在Pod中的Jenkins Slave并注册到Master上。2、当运行完Job后，这个Slave会被注销并且这个Pod也会自动删除，恢复到最初的状态(这个策略可以设置) 优点： 服务高可用 121、当Jenkins Master出现故障时，Kubernetes会自动创建一个新的Jenkins Master容器。2、然后将Volume分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用的作用。 动态伸缩 1231、合理使用资源，每次运行Job时，会自动创建一个Jenkins Slave。2、Job完成后，Slave自动注销并删除容器，资源自动释放。3、Kubernetes会根据每个资源的使用情况，动态分配slave到空闲的节点上创建，降低出现因某节点资源利用率高，降低出现因某节点利用率高出现排队的情况。 扩展性好 1当Kubernetes集群的资源严重不足导致Job排队等待时，可以很容易的添加一个k8s节点到集群,从而实现扩展。 Kubernetes安装配置Jenkins搭建nfs服务器1234567891011121、服务端配置yum install nfs-utils rpcbind -ymkdir -p /data/k8s-nfsecho &quot;/data/k8s-nfs *(rw,no_root_squash,sync)&quot; &gt; /etc/exportssystemctl start rpcbind &amp;&amp; systemctl enable rpcbindsystemctl start nfs &amp;&amp; systemctl enable nfs2、客户端配置yum install nfs-utils -ymkdir -p /datamount -t nfs 192.168.100.2:/data/k8s-nfs /data# 建议加入开机挂载echo &quot;192.168.100.2:/data/k8s-nfs /data nfs defaults 0 0&quot; &gt;&gt;/etc/fstab 编写yaml文件 创建命名空间 12kubectl create namespace jenkins-cicd# k8s不支持&quot;_&quot;下划线 创建jenkins持久化存储 12345678910111213141516171819202122232425[root@master k8s-jenkins]# cat jenkins_pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: jenkins-pvspec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: path: /data/k8s-nfs/jenkins_home server: 192.168.100.2---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: jenkins-pvc namespace: jenkins-cicdspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi 创建jenkins的deployment 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@master k8s-jenkins]# cat jenkins_deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: jenkins namespace: jenkins-cicdspec: template: metadata: name: jenkins labels: app: jenkins spec: terminationGracePeriodSeconds: 5 serviceAccount: jenkins containers: - name: jenkins image: jenkins/jenkins:lts imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP resources: limits: cpu: 1 memory: 1Gi requests: cpu: 0.5 memory: 500Mi livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 securityContext: fsGroup: 1000 volumes: - name: jenkins-home persistentVolumeClaim: claimName: jenkins-pvc #hostPath: #path: /data/k8s-data/jenkins_home 创建serviceAccount的Jenkins用户 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@master k8s-jenkins]# cat jenkins_rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: jenkins namespace: jenkins-cicd---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkinsrules:- apiGroups: [&quot;extensions&quot;,&quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;pods/exec&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;pods/log&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: jenkins namespace: jenkins-cicdroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: jenkinssubjects:- kind: ServiceAccount name: jenkins namespace: jenkins-cicd 创建jenkins svc 1234567891011121314151617181920[root@master k8s-jenkins]# cat jenkins_svc.yaml apiVersion: v1kind: Servicemetadata: name: jenkins namespace: jenkins-cicd labels: app: jenkinsspec: type: NodePort selector: app: jenkins ports: - name: web port: 8080 targetPort: web nodePort: 30003 - name: agent port: 50000 targetPort: agent 创建harbor仓库secret 12345kubectl create secret -n jenkins docker-registry regcred --docker-server=192.168.100.2 --docker-username=admin --docker-password=xxzx@789 --docker-email=admin@qq.com# yaml文件中加入secret用于访问镜像仓库imagePullSecrets:- name: regcred# 和containers: ，同一位置 先创建pv然后在执行rbac,再创建deployment和svc 登录jenkins 1234# 可查看pod日志kubectl logs -n jenkins-cicd jenkins-5458f468bf-mw6n4# 也可在持久化目录下查看cat /data/k8s-data/jenkins_home/secrets/initialAdminPassword 安装插件 12# 可直接安装推荐插件也可自定义安装需安装的插件：Kubernetes 配置动态生成slave的pod 添加kubernetes 1进入系统管理 &gt;系统配置 下拉，找到cloud，添加kubernetes 123这里name就是配置的名称，url这里写的是k8s内部域名。jenkins的url就是svcname+namespace+cluster.local:port。其他配置默认即可 添加jenkins slave模版 123Name：Pod名称Namespave：Pod命名空间Labels：Pod标签 123456789101112131415161718# slave镜像Dockerfile# 需提前下载好jenkins-slaveFROM centos:7ARG AGENT_WORKDIR=/root/agentRUN curl --create-dirs -sSLo /usr/share/jenkins/slave.jar https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/3.16/remoting-3.16.jar \ &amp;&amp; chmod 755 /usr/share/jenkinsCOPY epel-apache-maven.repo /etc/yum.repos.d/epel-apache-maven.repoRUN yum -y install git libtool-ltdl java-1.8.0-openjdk apache-mavenENV AGENT_WORKDIR=$&#123;AGENT_WORKDIR&#125;COPY jenkins-slave /usr/local/bin/jenkins-slaveRUN mkdir /root/.jenkins &amp;&amp; mkdir -p $&#123;AGENT_WORKDIR&#125;ENV JAVA_HOME=/usr/lib/jvm/java-1.8.0ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/binWORKDIR /rootENTRYPOINT [&quot;jenkins-slave&quot;] 12# 也可使用创建好的slave镜像，下载下来上传到自己的镜像仓库registry.cn-beijing.aliyuncs.com/abcdocker/jenkins:v1.2 121、容器模版的name必须为jnlp、工作目录需为/home/jenkins/agent，否则可能无法连接上jenkins slave反复重启。2、docker镜像这里采用的别人做好的，registry.cn-beijing.aliyuncs.com/abcdocker/jenkins:v1.4。我上传到自己的harbor镜像仓库上。 添加volume卷12# 镜像主要是用于kubectl，kubectl是需要kubeconfig文件，所以我们还需要创建一个volume的配置选择Host path volume 配置kubectl权限1如果jenkins提示权限错误，请在配置中添加jenkins rbac创建的serviceaccounts 应用并保存 添加jenkins job并测试 创建一个freestyle project 限制运行节点1通过上面pod模版的label限制 添加构建 执行shell测试 1234567添加一个shell命令，测试docker和kubectlecho &quot;*******测试jenkins-slave******&quot;sleep 3echo &quot;*******测试docker******&quot;docker infoecho &quot;*******测试kubectl******&quot;kubectl get pod -n jenkins-cicd 立即构建 1234返回项目，点击立即构建，观察。1、先拉取指定镜像，然后创建一个slave pod2、slave连接上后开始执行相关任务命令3、执行完后自动释放该pod(这里设置了执行完保留3分钟,便于观察) 使用pipeline发布准备代码 创建测试gitlab项目 客户端配置gitlab免密(本例暂未做免密)1将需免密登录的公钥配置到gitlab 上传测试代码12345678910111、拉取代码git clone http://192.168.100.3/root/my-demo4.git2、下载测试代码wget http://down.i4t.com/hello-world-war-master.zipunzip hello-world-war-master.zipmv hello-world-war-master/* .rm -rf hello-world-war-master*3、上传代码git add .git commit -m &quot;xxx&quot;git push origin master 编写tomcat Dockerfile1234567891011121314151617181920FROM centosWORKDIR /tmp# install JDKCOPY jdk1.8.0_66.tar.gz /tmpRUN tar zxf /tmp/jdk1.8.0_66.tar.gz -C /usr/local/&amp;&amp; rm -rf /tmp/jdk1.8.0_66.tar.gz &amp;&amp; \ln -s /usr/local/jdk1.8.0_66 /usr/local/jdk#/etc/profileENV JAVA_HOME /usr/local/jdkENV CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH $PATH:$JAVA_HOME/bin# add apacheCOPY apache-tomcat-8.5.39.tar.gz /tmpRUN tar zxf apache-tomcat-8.5.39.tar.gz -C /usr/local &amp;&amp; \ rm -rf /tmp/apache-tomcat-8.5.39.tar.gz &amp;&amp; \ mv /usr/local/apache-tomcat-8.5.39/usr/local/tomcat &amp;&amp; \ rm -rf /usr/local/tomcat/webapps/docs examples host-manager managerEXPOSE 8080ENTRYPOINT /usr/local/tomcat/bin/startup.sh &amp;&amp; tail -f /usr/local/tomcat/logs/catalina.out# 完成tomcat镜像的创建，接下来配置Jenkins，每次打包上线自动替换ROOT目录 配置jenkins pipeline 添加gitlab凭证 创建并配置pipeline项目任务 创建任务 添加参数化构建(需安装插件:Git Parameter Plug-In) 配置触发器(可选,生产环境不建议配置) 配置gitlab1点击push events如出现200则说明配置成功 配置流水线(也可直接写pipeline script) 应用保存 代码提交上添加Jenkinsfile脚本 123456789101112131415161718192021222324252627# 代码库上创建deploy目录及Jenkinsfile文件node(&apos;jenkins-slave&apos;) &#123; stage(&apos;Git Clone&apos;)&#123; checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;$&#123;branch&#125;&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: &apos;gitlab_root_id&apos;, url:&apos;http://172.20.2.195:81/root/my-demo1.git&apos;]]]) //git credentialsId: &apos;92ae8184-9661-425d-8609-3c994ddecb21&apos;, url: &apos;http://192.168.100.3/root/my-demo1.git&apos; &#125; stage(&apos;Maven Build&apos;)&#123; echo &quot;Maven Build&quot; sh &apos;&apos;&apos; /usr/local/maven/bin/mvn -B clean package -Dmaven.test.skip=true -Dautoconfig.skip -s settings.xml &apos;&apos;&apos; &#125; stage(&quot;Docker Build Stage&quot;)&#123; echo &quot;Docker Build Stage&quot; sh &apos;&apos;&apos; docker build -t 172.20.5.117/ops/tomcat:v$&#123;BUILD_NUMBER&#125; . docker login --username=admin 172.20.5.117 --password &quot;xxzx@789&quot; docker push 172.20.5.117/ops/tomcat:v$&#123;BUILD_NUMBER&#125; &apos;&apos;&apos; &#125; stage(&quot;Deploy&quot;)&#123; echo &quot;K8s Deploy&quot; sh &apos;&apos;&apos; kubectl set image -n jenkins deploy/my-tomcat my-tomcat=172.20.5.117/ops/tomcat:v$&#123;BUILD_NUMBER&#125; &apos;&apos;&apos; &#125;&#125; 创建tomcat的deploy和svc12345678910111213141516171819202122232425262728293031[root@master k8s-jenkins]# cat jenkins_tomcat.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: my-tomcat namespace: jenkins-cicdspec: replicas: 1 selector: matchLabels: app: my-tomcat minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: name: my-tomcat labels: app: my-tomcat spec: containers: - name: my-tomcat image: 192.168.100.2/ops/tomcat:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 123456789101112131415[root@master k8s-jenkins]# cat jenkins_tomcat_svc.yaml apiVersion: v1kind: Servicemetadata: name: my-tomcat namespace: jenkins-cicd labels: app: my-tomcatspec: type: NodePort ports: - port: 8080 nodePort: 30004 selector: app: my-tomcat 提交发布测试 jenkins上开始构建 查看构建生成的镜像版本 harbor上查看镜像推送情况 访问测试]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s+jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Helm-k8s包管理工具]]></title>
    <url>%2F2019%2F10%2F20%2FHelm-k8s%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[概述Helm是kubernetes包管理工具，可以方便快捷的安装、管理、卸载kubernetes应用，类似于Linux操作系统中yum或apt-get软件的作用。其主要的设计目的： 创建新的chart包 将charts包文件打包压缩 同chart仓库进行集成，获取charts文件 安装及卸载charts到kubernetes集群 管理通过helm安装的charts应用 概念介绍 chart: 一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义。 release：在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次，每次安装都会创建一个新的 release。 repository：用于发布和存储 Chart 的仓库，Helm客户端通过HTTP协议来访问仓库中Chart的索引文件和压缩包。 组件 helm: 提供给用户的客户端程序，可以以命令行的形式同服务端-tiller进行通信。 tiller：服务端软件，用来同helm客户端进行交互，并同kubernetes api server组件进行交互。 安装部署 1.helm的安装部署 版本下载，版本列表 https://github.com/helm/helm/releases 解压缩, tar -zxvf helm-v2.0.0-linux-amd64.tgz 将解压缩后的二进制文件放在可执行目录下 mv linux-amd64/helm /usr/local/bin/helm，然后执行 helm –help查看帮助文档 2.tiller的安装部署 1234控制台执行 &gt; helm init命令，该命令会将从charts仓库中下载charts包，并根据其中的配置部署至kubernetes集群。默认的charts仓库为 https://kubernetes-charts.storage.googleapis.com/index.yaml默认使用的tiller镜像为 gcr.io/kubernetes-helm/tiller:v2.13.1 国内由于墙的原因无法直接访问，需要我们自行处理可替代的仓库和镜像版本，通过如下命令进行helm服务端的安装部署： 1&gt; helm init --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 稍等一会然后执行如下命令，看到如下输出说明安装成功： 123&gt; helm versionClient: &amp;version.Version&#123;SemVer:&quot;v2.13.1&quot;, GitCommit:&quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&quot;, GitTreeState:&quot;clean&quot;&#125;Server: &amp;version.Version&#123;SemVer:&quot;v2.13.1&quot;, GitCommit:&quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&quot;, GitTreeState:&quot;clean&quot;&#125; 3.常用命令(helm –help) 12345678910search 在helm仓库进行查找应用fetch 从仓库中下载chart包到本地list 在该k8s集群的部署的release列表status 显示release的具体信息install 安装chartsinspect 描述charts信息delete 删除部署的releasecreate 创建一个chartspackage 将某一charts进行打包压缩repo 显示、添加、移除charts仓库 访问授权在上面的步骤中我们将tiller所需的资源部署到了kubernetes集群中，但是由于Deployment tiller-deploy没有定义授权的ServiceAccount导致访问apiserver拒绝，执行如下命令为tiller-deploy进行授权： 123&gt; kubectl create serviceaccount --namespace kube-system tiller&gt; kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller&gt; kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos; 通过helm部署wordpress输入如下命令，我们可以通过helm创建一个WordPress博客网站 1&gt; helm install --name wordpress-test --set &quot;persistence.enabled=false,mariadb.persistence.enabled=false&quot; stable/wordpress 通过如下命令获取登录信息： 12&gt; kubectl get svc -o wide&gt; kubectl get secret --namespace default wordpress-test-wordpress -o jsonpath=&quot;&#123;.data.wordpress-password&#125;&quot; | base64 --decode 在浏览器中打开页面，并输入用户名和密码就可以看到搭建好的WordPress博客网站了。 升级当有新的chart包发布时或者想改变已有release的配置时，可以通过 helm upgrade命令实现，比如： 123&gt; helm upgrade wordpress-test \&gt; --set &quot;persistence.enabled=true,mariadb.persistence.enabled=true&quot; \&gt; stable/wordpress]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oarcle 日常维护]]></title>
    <url>%2F2019%2F09%2F30%2FOarcle-%E6%97%A5%E5%B8%B8%E7%BB%B4%E6%8A%A4%2F</url>
    <content type="text"><![CDATA[背景 由于归档日志不停增加并且得不到清理，导致硬盘空间不足。因此需要定时清理归档日志，腾出可用空间。 操作步骤123456789101112# 使用rman清理$ rmanRMAN&gt; connect target /RMAN&gt; crosscheck archivelog all;# 列出日志RMAN&gt; list archivelog all;# 清理所有日志RMAN&gt; delete expired archivelog all;# 期间要输入yesRMAN&gt; exit# 删除3天前的归档日志RMAN&gt; delete archivelog all completed before &apos;sysdate-3&apos;; 12# 强制删除delete noprompt force archivelog all completed before &apos;sysdate-2&apos;; 启动顺序：先启动备库，后启动主库 关闭顺序：先关闭主库，后关闭备库 1、正确打开备库和主库备库: 1234567SQL&gt; conn / as sysdbaSQL&gt; startup nomount; SQL&gt; alter database mount standby database;SQL&gt; recover managed standby database disconnect from session;查看备库状态和模式：SQL&gt;select name,open_mode,protection_mode,database_role from v$database; 主库: 12345678[oracle@localhost~]$ lsnrctl start[oracle@localhost~]$ sqlplus /nologSQL&gt;conn / as sysdbaSQL&gt; STARTUP;查看主库状态和模式：SQL&gt; select name,open_mode,protection_mode,database_role from v$database;在主库归档当前日志：SQL&gt; alter system archive log current; 2、正确关闭顺序主库 12SQL&gt; alter system archive log current;SQL&gt; shutdown immediate; 备库: 12SQL&gt; ALTER DATABASE RECOVER MANAGED STANDBY DATABASE CANCEL;SQL&gt; SHUTDOWN IMMEDIATE; 遇到的问题 重启后无法通过cmd连接进入SQL&gt;模式，连接报错提示 1234检查是否oracle相关服务没启动启动以下服务：OracleOraDb11g_home1ClrAgent、OracleOraDb11g_home1TNSListener、OracleServiceORCL就可以了 授权用户权限1234567891011# 运行sql*plus进入SQL模式sqlplus /nolog# 以sysdba模式连接数据库SQL&gt; conn / as sysdba# 创建用户(user 用户，identified 密码)create user whw identified by xxx;# 对创建的用户进行授权(connect连接权限，创建操作视图权限，只读权限)grant connect,create view,select any table to whw;# 修改密码alter user xxx identified by xxxx; 1234567# 操作表# 授予all on 权限set line 999set pagesize 0spool c:\liyunkai.txt;select &apos;grant all on &apos;||owner||&apos;.&apos;||object_name||&apos; to WUHW;&apos; from dba_objects where owner =&apos;WMS_USER&apos; and object_type=&apos;TABLE&apos;;spool off; 123456# 创建SYNONYMset line 999set pagesize 0spool c:\liyunkai_syn.txt;select &apos;create synonym WUHW.&apos;||object_name||&apos; for WMS_USER&apos;||&apos;.&apos;||object_name||&apos;;&apos; from dba_objects where owner =&apos;WMS_USER&apos; and object_type=&apos;TABLE&apos;;spool off; 123456# 授予操作视图的权限set line 999set pagesize 0spool c:\liyunkai1.txt;select &apos;grant all on &apos;||owner||&apos;.&apos;||object_name||&apos; to wuhw;&apos; from dba_objects where owner =&apos;WMS_USER&apos; and object_type=&apos;VIEW&apos;; spool off; 123456# 创建视图synonymset line 999set pagesize 0spool c:\liyunkai1_syn.txt;select &apos;create synonym wuhw.&apos;||object_name||&apos; for WMS_USER&apos;||&apos;.&apos;||object_name||&apos;;&apos; from dba_objects where owner =&apos;WMS_USER&apos; and object_type=&apos;VIEW&apos;; spool off; 123456# 创建存储过程同义词synonymset line 999set pagesize 0spool c:\liyunkai2_syn.txt;select &apos;create synonym wuhw.&apos;||object_name||&apos; for WMS_USER&apos;||&apos;.&apos;||object_name||&apos;;&apos; from dba_objects where owner =&apos;WMS_USER&apos; and object_type=&apos;PROCEDURE&apos;; spool off; 查看oracle用户锁定时间12alter session set nls_date_format=&apos;yyyy-mm-dd hh24:mi:ss&apos;;select username,lock_date from dba_users where username=&apos;TEST&apos;; ORACLE清理监听日志12/opt/oracle/app/diag/tnslsnr/机器名/listener/trace/listener.logD:\app\Administrator\diag\tnslsnr\主机名\listenner\trace\listener.log 12345678# 清理步骤# 先用lsnrctl进入监听模式 G:\docu&gt;lsnrctlLSNRCTL&gt;set log_status off (此时便可删除监听日志文件listener.log，然后再新建一个listener.log，日志文件位置：D:\app\Administrator\diag\tnslsnr\主机名\listenner\trace\下面) LSNRCTL&gt;set log_status on意思就是先停止监听日志输出。删除后再开启日志 win普通用户无法使用sqlplus / as sysdba登录123其实在这种情况下，Oracle采用的是操作系统认证方式.当属于操作系统DBA组用户登陆到数据库服务器，那么Oracle认为这样的用户就可以授权以SYSDBA身份登录数据库。这时的用户名和密码随便输什么，都能连上库，使用show user命令查看登陆用户，其实都是SYS用户。 123Linux下类似，使用id命令，查看用户所在组，如果在dba组，那么也可以以操作系统的方式直接连库。[oracle@mydb ~]$ id oracleuid=54321(oracle) gid=54321(oinstall) groups=54321(oinstall),54322(dba)]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm安装Kubernetes]]></title>
    <url>%2F2019%2F09%2F30%2Fkubeadm%E5%AE%89%E8%A3%85Kubernetes%2F</url>
    <content type="text"><![CDATA[Kubernetes简介123Kubernetes（简称K8S）是开源的容器集群管理系统，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。它既是一款容器编排工具，也是全新的基于容器技术的分布式架构领先方案。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等功能，提高了大规模容器集群管理的便捷性。 两种节点类型 管理节点：主要负责K8S集群管理，集群中各节点间的信息交互、任务调度，还负责容器、Pod、NameSpaces、PV等生命周期的管理。 工作节点：主要为容器和Pod提供计算资源，Pod及容器全部运行在工作节点上，工作节点通过kubelet服务与管理节点通信以管理容器的生命周期，并与集群其他节点进行通信。 环境准备配置信息如下： IP地址 节点角色 hostname 192.168.100.10 master master 192.168.100.11 worker node1 192.168.100.12 worker node2 1.设置主机名，所有主机分别进行设置(contos7)hostnamectl set-hostname master 2.编辑/etc/hosts文件，添加域名解析 12345cat &lt;&lt; EOF &gt;&gt; /etc/hosts192.168.100.10 master192.168.100.11 node1192.168.100.12 node2EOF 3.关闭防火墙、SELinux和swap 123456789# 关闭防火墙systemctl stop firewalldsystemctl disable firewalld# 关闭SELinuxsetenforce 0sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config# 关闭swap，取消开机加载(如果有设置)swapoff -ased -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstab 4.配置内核参数，将桥接的IPv4流量传递到iptables的链 12345cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 5.配置国内相关安装源 123456# 配置国内yum源yum install -y wget # 安装wgetmkdir /etc/yum.repos.d/bak &amp;&amp; mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repowget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repoyum clean all &amp;&amp; yum makecache 12345678910# 配置国内Kubernetes源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 12# 配置docker源wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 软件安装(如下所有节点均安装) 1.安装docker 1234# docker服务为容器运行提供计算资源，是所有容器运行的基本平台。yum install -y docker-cesystemctl enable docker &amp;&amp; systemctl start dockerdocker --version 2.安装kubeadm、kubelet、kubectl 12345# Kubelet负责与其他节点集群通信，并进行本节点Pod和容器生命周期的管理。# Kubeadm是Kubernetes的自动化部署工具，降低了部署难度，提高效率。# Kubectl是Kubernetes集群管理工具。yum install -y kubelet kubeadm kubectlsystemctl enable kubelet 部署master节点 1.在master进行kubernetes集群初始化 123456789101112# 定义POD的网段为: 10.244.0.0/16，api server地址就是master本机IP地址。# 这一步很关键，由于kubeadm默认从官网k8s.grc.io下载所需镜像，国内无法访问，因此需要通过–image-repository指定阿里云镜像仓库地址，很多新手初次部署都卡在此环节无法进行后续配置。kubeadm init --kubernetes-version=1.15.3 \--apiserver-advertise-address=192.168.100.10 \--image-repository registry.aliyuncs.com/google_containers \--service-cidr=10.1.0.0/16 \--pod-network-cidr=10.244.0.0/16# 此过程需要点时间# 如果报错如下(warning可忽略)，就是初始化kubernetes的版本低了[ERROR KubeletVersion]: the kubelet version is higher than the control plane version. This is not a supported version skew and may lead to a malfunctional cluster. Kubelet version: &quot;1.15.3&quot; Control plane version: &quot;1.14.2&quot;[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` 1234# 集群初始化成功后返回如下信息：# 记录生成的最后部分内容，此内容需要在其它节点加入Kubernetes集群时执行。kubeadm join 192.168.100.10:6443 --token ny71b9.yh4680y0vaw0uhso \ --discovery-token-ca-cert-hash sha256:fd8b1e92b8f4006e9af6db653241cd622a32b9cd9e8a02baba60bd66d159a860 2.配置kubectl工具 1234567891011121314mkdir -p /root/.kubecp /etc/kubernetes/admin.conf /root/.kube/configkubectl get nodes[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 17m v1.15.3node1 Ready &lt;none&gt; 110s v1.15.3node2 Ready &lt;none&gt; 62s v1.15.3kubectl get cs[root@master ~]# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 3.部署flannel网络 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 部署node节点123# node节点上加入到kubernetes集群kubeadm join 192.168.100.10:6443 --token ny71b9.yh4680y0vaw0uhso \ --discovery-token-ca-cert-hash sha256:fd8b1e92b8f4006e9af6db653241cd622a32b9cd9e8a02baba60bd66d159a860 集群状态检测 1.在master节点检查集群状态 1234567kubectl get nodes# 重点查看STATUS内容为Ready时，则说明集群状态正常。[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 18m v1.15.3node1 Ready &lt;none&gt; 2m45s v1.15.3node2 Ready &lt;none&gt; 117s v1.15.3 2.创建pod以验证集群是否正常 123kubectl create deployment nginx --image=nginxkubectl expose deployment nginx --port=80 --type=NodePortkubectl get pod,svc 部署Dashboard 1.创建Dashboard的yaml文件1wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 123# 使用如下命令或直接手动编辑kubernetes-dashboard.yaml文件sed -i &apos;s/k8s.gcr.io/loveone/g&apos; kubernetes-dashboard.yamlsed -i &apos;/targetPort:/a\ \ \ \ \ \ nodePort: 30001\n\ \ type: NodePort&apos; kubernetes-dashboard.yaml 手动编辑kubernetes-dashboard.yaml文件时，需要修改两处内容，首先在Dashboard Deployment部分修改Dashboard镜像下载链接，由于默认从官方社区下载，而不“科学上网”是无法下载的，因此修改为：image: loveone/kubernetes-dashboard-amd64:v1.10.1 修改后内容如图 此外，需要在Dashboard Service内容加入nodePort： 30001和type: NodePort两项内容，将Dashboard访问端口映射为节点端口，以供外部访问，编辑完成后，状态如图 2.部署Dashboard 1kubectl create -f kubernetes-dashboard.yaml 3.检查相关服务运行状态 1234kubectl get deployment kubernetes-dashboard -n kube-systemkubectl get pods -n kube-system -o widekubectl get services -n kube-systemnetstat -ntlp|grep 30001 4.浏览器访问测试Dashboard访问地址：https://192.168.100.10:30001 5.查看访问Dashboard的认证令牌 123kubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk &apos;/dashboard-admin/&#123;print $1&#125;&apos;) 6.使用输出的token登录Dashboard]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tornado构建RESTful应用]]></title>
    <url>%2F2019%2F09%2F27%2FTornado%E6%9E%84%E5%BB%BARESTful%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[RESTful Representational State Transfer：表中状态转移 资源(Resources),表现层(Representation),状态转化(State Transfer) RESTful风格 Resources(资源)：使用URI指向的一个实体 Representation(表现层)：资源的表现形式，比如图片、HTML文本等 State Transfer(转态转化)：GET、POST、PUT、DELETE HTTP动词来操作资源 常用HTTP动词 RESTful解释 GET/POST/PUT/DELETE分别用来 获取/新建/更新/删除 资源 幂等性：GET/PUT/DELETE是幂等操作（无论操作多少次都是获得同样的数据，幂等的可以放心的多次请求，非幂等就要注意） 幂等指的是无论一次还是多次操作具有一样的副作用 Tornado RESTful Api示例 HTTP方法 URL 动作 GET http://[hostname]/api/users 检索用户列表 GET http://[hostname]/api/users/[user_id] 检索单个用户 POST http://[hostname]/api/users 创建新用户 PUT http://[hostname]/api/users/[user_id] 更新用户信息 DELETE http://[hostname]/api/users/[user_id] 删除用户 简单实现 使用Tornado实现Restful Api Tornado适合构建RESTful微服务 实现UserModel 实现UserListRequesthandler和UserRequestHandler 相关代码12345678910111213141516171819202122232425262728293031# models/user.pyclass UserModel(object): users = &#123; 1: &#123;&apos;name&apos;: &apos;zhang&apos;, &apos;age&apos;: 10&#125;, 2: &#123;&apos;name&apos;: &apos;wang&apos;, &apos;age&apos;: 12&#125;, 3: &#123;&apos;name&apos;: &apos;li&apos;, &apos;age&apos;: 14&#125;, 4: &#123;&apos;name&apos;: &apos;zhao&apos;, &apos;age&apos;: 11&#125;, &#125; @classmethod def get(cls, user_id): return cls.users[user_id] @classmethod def get_all(cls): return list(cls.users.values()) @classmethod def create(cls, name ,age): user_dict = &#123;&apos;name&apos;: name, &apos;age&apos;: age&#125; max_id = max(cls.users.keys()) + 1 cls.users[max_id] = user_dict @classmethod def update(cls, user_id, age): cls.users[user_id][&apos;age&apos;] = age @classmethod def delete(cls, user_id): if user_id in cls.users: return cls.users.pop(user_id) 1234567891011121314151617181920212223242526272829303132333435# handlers/user.pyimport tornado.webfrom tornado.escape import json_encodefrom models.user import UserModelclass UserListHandler(tornado.web.RequestHandler): def get(self): users = UserModel.get_all() self.write(json_encode(users)) def post(self): name = self.get_argument(&apos;name&apos;) age = self.get_argument(&apos;age&apos;) UserModel.create(name, age) resp = &#123;&apos;status&apos;: True, &apos;msg&apos;: &apos;create success&apos;&#125; self.write(json_encode(resp))class UserHandler(tornado.web.RequestHandler): def get(self, user_id): try: user = UserModel.get(int(user_id)) except KeyError: return self.set_status(404) self.write(json_encode(user)) def put(self, user_id): age = self.get_argument(&apos;age&apos;) UserModel.update(int(user_id), age) resp = &#123;&apos;status&apos;: True, &apos;msg&apos;: &apos;update success&apos;&#125; self.write(json_encode(resp)) def delete(self, user_id): UserModel.delete(int(user_id)) resp = &#123;&apos;status&apos;: True, &apos;msg&apos;: &apos;delete success&apos;&#125; self.write(json_encode(resp)) 12345678910111213141516171819202122# app.pyimport tornado.webfrom handlers import user as user_handlersHANDLERS = [ (r&quot;/api/users&quot;, user_handlers.UserListHandler), (r&quot;/api/users/(\d+)&quot;, user_handlers.UserHandler)]def run(): app = tornado.web.Application( HANDLERS, debug=True, ) http_server = tornado.httpserver.HTTPServer(app) port = 8888 http_server.listen(port) print(&apos;Server start on port &#123;&#125;&apos;.format(port)) tornado.ioloop.IOLoop.instance().start()if __name__ == &apos;__main__&apos;: run()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tornado简介和安装]]></title>
    <url>%2F2019%2F09%2F10%2FTornado%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Tornado框架 http://www.tornadoweb.org/en/stable/ 使用python编写的网络框架和高性能的异步网络库 框架特性和使用场景 优势 微框架、高性能、抗负载能力、搞并发 异步支持: 异步非阻塞IO的处理方式 缺点 轮子少，不像Django，Flask等框架有大量插件支持 缺少最佳实践，使用的公司不多，学习资料少 构建微服务 不适合复杂的CMS(内容管理系统)应用 适合构建网站或者App后端微服务 学习资料 文档和书籍 官方文档：http://www.tornadoweb.org/en/stable/ https://github.com/tornadoweb/tornado 12345678910111213141516171819同步：含义：指两个或两个以上随时间变化的量在变化过程中保持一定的相对关系现象：有一个共同的时钟，按来的顺序一个一个处理直观感受：就是需要等候，效率低下异步：含义：双方不需要共同的时钟，也就是接收方不知道发送方什么时候发送，所以在发送的信息中就要有提示接收方开始接收的信息，如开始位，同时在结束时有停止位现象：没有共同的时钟，不考虑顺序来了就处理直观感受：就是不用等了，效率高阻塞调用：含义：阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，CPU不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回现象：读套接字时没有数据等数据来，写套接字时写不下了也一直等，等能写下了往里写（套接字被写满的原因不在本地，在于网络另一头的套接字被写满了来不及读出去，导致本地的套接字内容来发不出去堵住了）直观感受：执着非阻塞调用：含义：非阻塞调用是指没有调用结果立即返回，当前线程不被挂起，可以继续做其它工作现象：读套接字时没有数据，不等直接返回干别的事去，写套接字写不下了也不写了，直接返回干别的事去直观感受：勤奋 安装123pip install tornado # 运行，浏览器访问127.0.0.1:8888 或者使用curl命令测试python testapp.py 12345678910111213141516171819202122232425262728# testapp.py# tornado的核心IO循环模块,封装了linux的epoll和BSD的kqueue，是tornado高效的基础import tornado.ioloop# tornado的基础web框架模块import tornado.webclass MainHandler(tornado.web.RequestHandler): def get(self): # 对应http请求的方法，给浏览器响应信息 self.write(&quot;Hello, Tornado&quot;)def make_app(): # 实例化一个make_app对象 # Application是tornado web框架的核心应用类，是与服务器对应的接口，里面保存了路由映射表 return tornado.web.Application([ (r&quot;/&quot;, MainHandler), ])# 执行时name名为__main__，其他调用时为文件名if __name__ == &quot;__main__&quot;: app = make_app() # 绑定监听端口，此时服务并没有开启监听 # http_server.bind(8888)、http_server.start()相当于http_server.listen(8888); http_server = tornado.httpserver.HTTPServer(app)加.listen(8888)相当于app.listen(8888) app.listen(8888) # app.listen() 只能在单进程中使用，因为没指定要创建几个进程。 # IOLoop.current():返回当前线程的IOLoop实例 # IOLoop.start():启动IOLoop实例的I/O循环，并且开启了监听 tornado.ioloop.IOLoop.current().start() 12345tornado启动多进程建议是手动运行启动多个，不建议在start()方法中定义多个进程。原因：1、每个子进程都会从父进程中复制一份IOLoop的实例，如果在创建子进程前修改了IOLoop会影响所有的子进程。2、所有的进程都是一个命令启动的，无法做到在不停止服务的情况下修改代码。3、所有进程共享一个端口，想要分别监控很困难。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django支持网站多语言切换]]></title>
    <url>%2F2019%2F09%2F02%2FDjango%E6%94%AF%E6%8C%81%E7%BD%91%E7%AB%99%E5%A4%9A%E8%AF%AD%E8%A8%80%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[分为models、views和模版(templates)三处转换 修改settings 1.增加中间件，顺序靠前 12345MIDDLEWARE = [ ... &apos;django.middleware.locale.LocaleMiddleware&apos;, ...] 2.增加LANGUAGES和LOCALE_PATHS，并且手动在项目根目录创建locale文件夹 12345678910# I18N translationLANGUAGE_CODE = &apos;zh-Hans&apos;LANGUAGES = ( (&apos;zh-hans&apos;, ugettext_lazy(&apos;Simplified Chinese&apos;)), (&apos;en&apos;,(&apos;English&apos;)))LOCALE_PATHS = [ os.path.join(BASE_DIR, &apos;locale&apos;),] 生成翻译文件1234567891011121314151.执行命令生成翻译文件，请先安装gettexthttp://gnuwin32.sourceforge.net/packages/gettext.htm安装完成后执行如下命令python manage.py makemessages -l zh_Hanspython manage.py makemessages -d djangojs -l zh_Hans执行完成后就可在locale文件下生成了zh_Hans/LC_MESSAGES/2.修改django.po文件，对每一个msgid进行翻译，翻译内容填在msgstr中，如下格式：msgid &quot;Request is busy&quot;msgstr &quot;请求繁忙&quot;msgid &quot;Set up MFA&quot;msgstr &quot;设置MFA&quot;3.使用命令进行翻译python manage.py compilemessages 修改templates目录下的html文件12345# 在base.html页面中加载i18n，其他页面extand base.html&#123;% load i18n %&#125;&#123;% extends &apos;base.html&apos; %&#125;修改需要进行翻译的内容，与djano.po文件对应&#123;% trans &apos;MFA State&apos; %&#125; 修改url,添加切换语言功能12from django.conf.urls.i18n import i18n_patternsurlpatterns += i18n_patterns(&apos;&apos;,) 12345urlpatterns = [ ... path(&apos;i18n/&lt;str:lang&gt;/&apos;, I18NView.as_view(), name=&apos;i18n-switch&apos;), ...]]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署prometheus+grafana]]></title>
    <url>%2F2019%2F08%2F06%2F%E9%83%A8%E7%BD%B2prometheus-grafana%2F</url>
    <content type="text"><![CDATA[简介 prometheus 是由SoundCloud开发的开源监控报警系统和时序列数据库(TSDB)，prometheus是一个监控采集与数据存储框架（监控server端），具体采集什么数据依赖于具体的exporter（监控client端）grafana是一个高颜值的监控绘图程序，也是一个可视化面板（Dashboard），grafana的厉害之处除了高颜值，还支持多种数据源（支持Graphite、zabbix、InfluxDB、Prometheus和OpenTSDB作为数据源）、支持灵活丰富的dashboard配置选项 安装prometheus https://prometheus.io/download/ prometheus程序包 node_exporter：监控主机磁盘、内存、CPU等硬件性能指标的采集程序包 mysql_exporter：监控mysql各种性能指标的采集程序包 alertmanager: 监控告警 安装prometheus1234wget https://github.com/prometheus/prometheus/releases/download/v2.5.0/prometheus-2.5.0.linux-amd64.tar.gztar -zxvf prometheus-2.5.0.linux-amd64.tar.gz -C /data/mv /data/prometheus-2.5.0.linux-amd64 /data/prometheus#依次下载并解压node_exporter、mysql_exporter 配置prometheus 进入prometheus目录,配置prometheus.yml配置文件（要注意yml缩进语法规则） 12345678910111213141516171819202122232425262728293031323334353637383940# 规则global:alerting: alertmanagers: - static_configs: - targets:rule_files:scrape_configs:- file_sd_configs: - files: - host.yml job_name: Host metrics_path: /metrics relabel_configs: - source_labels: [__address__] regex: (.*) target_label: instance replacement: $1 - source_labels: [__address__] regex: (.*) target_label: __address__ replacement: $1:9100- file_sd_configs: - files: - mysql.yml job_name: MySQL metrics_path: /metrics relabel_configs: - source_labels: [__address__] regex: (.*) target_label: instance replacement: $1 - source_labels: [__address__] regex: (.*) target_label: __address__ replacement: $1:9104- job_name: prometheus static_configs: - targets: - localhost:9090 启动prometheus进程 12/data/prometheus/prometheus --storage.tsdb.retention=30d &amp;# 30d表示prometheus只保留30天以内的数据 控制台查看 123456789101112 http://172.20.2.214:9090 #访问，点击Status-&gt;Targets,查看刚prometheus.yml配置文件创建的job_name``` ![mark](http://img.key1024.cn/md/20190806/aRqOzs3qbvec.png)- 添加系统服务启动脚本###### 安装grafana- 下载安装包 https://grafana.com/grafana/download grafana程序包https://github.com/percona/grafana-dashboards/releases grafana-dashboards包wget https://github.com/percona/grafana-dashboards/archive/v1.16.0.tar.gz 12- 解压安装包 wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.3.4-1.x86_64.rpmyum localinstall grafana-5.3.4-1.x86_64.rpmsystemctl start grafana-server 12345678910111213141516171819202122- 配置grafana **a**、打开grafana页面(默认账户密码：admin/admin，默认端口3000),配置数据来源。 ![mark](http://img.key1024.cn/md/20190806/VHB1NBaqNNiN.png) **b**、点击+Add data source，HTTP的URL填写prometheus服务所在的服务器ip及服务端口,然后点击保存并测试。 ![mark](http://img.key1024.cn/md/20190806/7anaEe7mQEDR.png)- grafana中导入grafana-dashboards 解压grafana-dashboards包，该包提供了大量的json格式文件的grafana dashboards,根据需要自行选择。![mark](http://img.key1024.cn/md/20190806/j66xNY89q6S9.png)- grafana页面中，导入需要的json文件![mark](http://img.key1024.cn/md/20190806/hqWPLuoi2UkE.png)![mark](http://img.key1024.cn/md/20190806/8OFgPoo9cRJU.png)###### 3、监控节点部署- 添加主机监控 **a**、添加prometheus主机为例，解压exporter压缩包 tar zxvf node_exporter-0.15.2.linux-amd64.tar mv node_exporter-0.15.2.linux-amd64 node_exporter 1**b**、启动node_exporter程序 cd node_exporter nohup ./node_exporter &amp; 123456**c**、配置prometheus.yml文件，如果其中已定义了监控主机的配置文件host.yml，则只需把主机IP信息填入即可动态生效 ![mark](http://img.key1024.cn/md/20190806/mfXPqqUCrFRQ.png) 如果需要在添加新的实例，可直接把实例IP放在同一个targets下即可。 **d**、添加mysql监控 tar -zxvf mysqld_exporter-0.11.0.linux-amd64.tar.gz 123- 配置 监控数据库需要的主机IP、数据库端口、数据库帐号和密码的环境变量（注意：该账号需要单独创建，需要对所有库所有表至少具有PROCESS, REPLICATION CLIENT, SELECT权限）- 启动exporter nohup ./mysqld_exporter &amp;``` 注意事项：如果grafana中没获取到数据，可检查配置的Data Sources中的name是否与监控项的一致]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署django+uwsgi+nginx]]></title>
    <url>%2F2019%2F08%2F03%2F%E9%83%A8%E7%BD%B2django-uwsgi-nginx%2F</url>
    <content type="text"><![CDATA[本地准备工作 从开发机上导出requirements.txt文件，方便在部署的时候安装。使用pip freeze &gt; requirements.txt将当前环境的包导出到requirements.txt文件中。 将本地项目上传到目标服务器上（推荐使用git，可进行版本控制）123456git init # 初始化。进入本地项目目录，右键&apos;Git Bash Here&apos;后执行git remote add origin xxx.git # 添加本地与远程仓库的关联git add . # 添加本地文件到缓冲区git commit -m &apos;first commit&apos; # 提交代码待仓库git pull origin master --allow-unrelated-histories # 拉取代码，第一次拉取加上参数&apos;--allow-unrelated-histories&apos;git push origin master # 推送本地代码 服务器准备工作 服务器上拉取相关代码文件 服务器上安装好python、pip、创建虚拟环境virtualenv以及virutalenvwrapper，使用pip安装。临时更改安装源，以豆瓣源为例：pip install &lt;包名&gt; -i https://pypi.douban.com/simple 12345678# 编辑当前用户家目录vim ~/.bashrc 进入文件中，填入以下两行代码：export WORKON_HOME=$HOME/.virtualenvssource /usr/local/bin/virtualenvwrapper.shsource ~/.bashrc # 重新加载配置文件，使之生效。mkvirtualenv django-env # 创建虚拟环境 安装mysql、缓存数据库等（创建相关数据库create database deploy_demo charsete utf8;） 安装requirements.txt相应包 执行python manage.py migrate命令，将迁移文件，映射到数据库中，创建相应的表。 setting中设置ALLOW_HOST为你的域名，以及ip地址。ALLOWED_HOSTS = [&#39;key1024.cn&#39;,&#39;x.x.x.x&#39;] 设置DEBUG=False，避免如果你的网站产生错误，而将错误信息暴漏给用户。 执行python manage.py runserver 0.0.0.0:8000，在浏览器中输入http://x.x.x.x:8000/，访问下网站所有页面，确保所有页面都没有错误。 收集静态文件：python manage.py collectstatic。 123456# 在setting.py中设置相关路径,static、static_dist目录提前创建好。 STATIC_URL = &apos;/static/&apos;STATIC_ROOT = os.path.join(BASE_DIR,&apos;static_dist&apos;)STATICFILES_DIRS = [ os.path.join(BASE_DIR,&apos;static&apos;)] 安装uwsgi uwsgi是一个应用服务器，非静态文件的网络请求就必须通过他完成，他也可以充当静态文件服务器，但不是他的强项。uwsgi是使用python编写的，因此通过pip install uwsgi就可以了。(uwsgi必须安装在==系统级别==的python环境中，不要安装到虚拟环境中)。 启动测试 1进入项目文件下,使用命令`uwsgi --http :8000 --module xfz.wsgi --vritualenv /root/.virtualenvs/django-env`。用`uwsgi`启动项目，如果能够在浏览器中访问到这个页面，说明`uwsgi`可以加载项目了。 配置uwsgi1234567891011121314151617181920212223# 在项目的路径下面，创建一个文件叫做`demo_uwsgi.ini`的文件，然后填写以下代码：[uwsgi]# Django相关的配置# 必须全部为绝对路径# 项目的路径chdir = /srv/your_demo# Django的wsgi文件module = demo.wsgi# Python虚拟环境的路径home = /root/.virtualenvs/django-env# 监听端口http :8000# 进程相关的设置# 主进程master = true# 最大数量的工作进程processes = 10# socket文件路径，绝对路径socket = /srv/your_demo/xxx.sock# 设置socket的权限chmod-socket = 666# 退出的时候是否清理环境vacuum = true 然后使用命令uwsgi --ini demo_uwsgi.ini，看下是否还能启动这个项目。 nginx配置 在/etc/nginx/conf.d目录下,新建配置文件xfz.conf，然后复制如下配置文件内容。 12345678910111213141516171819202122upstream xfz &#123; server unix:///srv/your_demo/xxx.sock&#125;# 配置nginx服务server &#123; listen 80; server_name x.x.x.x; charset utf-8; client_max_body_size 20M; # 静态文件访问url地址 location /static &#123; alias /data/xfz/static; &#125; # 发送非静态文件请求到django服务器 location / &#123; uwsgi_pass xfz; # uwsgi_params文件地址 include /etc/nginx/uwsgi_params; &#125;&#125; 配置supervisor 让supervisor管理uwsgi，可以在uwsgi发生异常情况下会自动重启恢复 1.’supervisor’的安装：在系统级别的python环境下pip install supervisor,启动supervisord -c xfz_supervisor.conf 2.在项目的根目录下创建一个文件叫做xfz_supervisor.conf，内容如下： 1234567891011121314151617# supervisor的程序名字[program:mysite]# supervisor执行的命令command = uwsgi --ini xfz_uwsgi.ini# 项目的目录directory = /srv/xfz# 开始的时候等待多少秒startsecs = 0# 停止的时候等待多少秒stopwaitsecs = 0 # 自动开始autostart = true# 程序挂了后自动重启autorestart = true# 输出的log文件stdout_logfile = /srv/xfz/log/supervisord.logstderr_logfile = /srv/xfz/log/supervisord.err 1234567891011121314151617181920212223242526272829[supervisord]# log的级别，一般设置为info或debugloglevel = info[inet_http_server]port = 9001username = adminpassword = xxzx@789# 使用supervisorctl的配置[supervisorctl]# 使用supervisorctl登录的地址和端口号serverurl = http://127.0.0.1:9001# 登录supervisorctl的用户名和密码username = adminpassword = xxzx@789# [rpcinterface:supervisor]supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface配置完成后，使用supervisord -c xfz_supervisor.conf运行就可以。如果想启动uwsgi就可通过supervisorctl -c xfz_supervisor.conf进入到命令管理控制台，然后可执行相关的命令进行管理：supervisor&gt; help * status # 查看状态 * start program_name #启动程序 * restart program_name #重新启动程序 * stop program_name # 关闭程序 * reload # 重新加载配置文件 * quit # 退出控制台]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django的MFA验证码接口]]></title>
    <url>%2F2019%2F07%2F25%2FDjango%E7%9A%84MFA%E9%AA%8C%E8%AF%81%E7%A0%81%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[背景 增加双因子认证，提高网站登录的安全性。利用谷歌身份验证器绑定密钥，从而进行动态MFA验证。 原理核心内容 Google Authenticator采用的算法是TOTP（Time-Based One-Time Password基于时间的一次性密码），其核心内容包括以下三点： 一个共享密钥 当前时间输入 一个签名函数 加密原理和步骤 Step1：base32 secret Key：共享密钥，在Google Authenticator中是通过将一段字符串进行base32解码成bytes得到的。但由于此密钥在不够32位或超过32位时会用’=’表示，故用的pyotp随机生成的密钥。 12Secret = pyotp.random_base32()Key = base64.b32decode(Secret, True) Step2：get current timestamp Count：计数器，通过当前时间戳除以30然后将得到的整数转换成一个大端序的字节。123# int(time.time()) // 30 到当前经历了多少个30秒# 将间隔时间转为big-endian(大端序)并且为长整型的字节Count = struct.pack(&quot;&gt;Q&quot;, int(time.time()) // 30) Step3：start hmac-sha1 Hmac：将K和C做HMAC-SHA-1加密然后以字节方式保存，因为后期需要进行与运算，而str是不能和int进行与运算的。12345# hmac = SHA1(secret + SHA1(secret + input))# 为了方便演示，将字节转换成了字符串显示Hmac = hmac.new(K, C, hashlib.sha1).digest()# 取出最后一位和数字15做与运算O = H[19] &amp; 15 Step4：get DynamicPasswd 通过计算出来的O在H中取出4个16进制的字节，然后将字节转换成正整数，因转换后的正整数是放在数组里面的，所以需要使用[0]取出。最后与一个全为1的二进制与运算然后与10^6做取余运算，最终会得到一个6位数的TOTP12345DynamicPasswd = str((struct.unpack(&quot;&gt;I&quot;, H[O:O + 4])[0] &amp; 0x7fffffff) % 1000000) # struct.unpack(&apos;&gt;I&apos;,h[o:o+4])[0] :转为big-endian(大端序)并且不为负数的数字(整数),因为转换完是一个数组,类似&quot;(2828101188,)&quot;,所以需要[0]取出# h[o:o+4] :取其中4个字节 o=10 则取索引分别为 10,11,12,13的字节# &amp; 0x7fffffff = 11111111 :与字节转换的数字做与运算# % 1000000 :得出的数字与1000000相除然后取余 Step5：get MFA 最后计算出的6位数字最左边的一位可能为0，所以需要判断如果DynamicPasswd得到的是一个5位数的数字，那就在最左边加上一个0。1TOTP = str(0) + str(DynamicPasswd) if len(DynamicPasswd) &lt; 6 else DynamicPasswd 实现步骤 1.使用python的pyotp模块生成谷歌认证需要的密钥 2.根据密钥生成二维码图片以及计算出6位动态验证码 3.使用谷歌的身份验证器app，扫描二维码或者手动输入密钥 4.平台二次认证通过对输入的动态验证码进行校验 相关代码12345678910111213141516171819202122232425262728293031323334353637383940414243import base64, time, struct, hmac, hashlibimport pyotpfrom qrcode import QRCodefrom qrcode import constantsname = &apos;user01&apos;+&apos;:SmartMS&apos;# 利用参数secretKey,计算Google Authenticator 6位动态码。def getMFACode(Secret): print(&apos;MFA密钥:&#123;&#125;&apos;.format(Secret)) K = base64.b32decode(Secret, True) C = struct.pack(&quot;&gt;Q&quot;, int(time.time()) // 30) H = hmac.new(K, C, hashlib.sha1).digest() O = H[19] &amp; 15 # bin(15)=00001111=0b1111 DynamicPasswd = str((struct.unpack(&quot;&gt;I&quot;, H[O:O + 4])[0] &amp; 0x7fffffff) % 1000000) TOTP = str(0) + str(DynamicPasswd) if len(DynamicPasswd) &lt; 6 else DynamicPasswd print(&apos;动态MFA:&#123;&#125;&apos;.format(TOTP)) return TOTPdef getMFAImg(name,Secret): # otpauth://totp/ 固定格式 # name：标识符信息，issuer：发行信息 url = &quot;otpauth://totp/&quot; + name + &quot;?secret=%s&quot; % Secret + &quot;&amp;issuer=Anchnet&quot; qr = QRCode(version=1, error_correction=constants.ERROR_CORRECT_L,box_size=6,border=4) qr.add_data(url) qr.make(fit=True) img = qr.make_image() # img.show()def checkCode(Secret): code = int(input(&apos;输入验证码:&apos;)) t = pyotp.TOTP(Secret) result = t.verify(code) msg = result if result is True else False print(&apos;验证码验证&#123;&#125;&apos;.format(msg)) return msgif __name__ == &apos;__main__&apos;: Secret = pyotp.random_base32() # Secret = &apos;UFB6R5QKLPV7FGIU&apos; getMFACode(Secret) Secret = getMFAImg(name,Secret) # checkCode(Secret) 封装为接口 resful.py 12345678910111213141516171819202122232425262728293031# 新建一个包，包下创建resful.py文件from django.http import JsonResponseclass HttpCode(object): ok = 200 paramserror = 400 unauth = 401 methoderror = 405 servererror = 500def result(code=HttpCode.ok,message=&quot;&quot;,data=None,kwargs=None): json_dict = &#123;&quot;code&quot;:code,&quot;message&quot;:message,&quot;data&quot;:data&#125; if kwargs and isinstance(kwargs,dict) and kwargs.keys(): json_dict.update(kwargs) return JsonResponse(json_dict)def ok(): return result()def params_error(message=&quot;&quot;,data=None): return result(code=HttpCode.paramserror,message=message,data=data)def unauth(message=&quot;&quot;,data=None): return result(code=HttpCode.unauth,message=message,data=data)def method_error(message=&quot;&quot;,data=None): return result(code=HttpCode.methoderror,message=message,data=data)def server_error(message=&quot;&quot;,data=None): return result(code=HttpCode.servererror,message=message,data=data) views.py 123456789101112131415161718192021222324from utils import restful # 自定义的restfulimport base64, time, struct, hmac, hashlibimport pyotpfrom qrcode import QRCodefrom qrcode import constantsdef getMFAinfo(request): name = request.GET[&apos;name&apos;] + &apos;:SmartMS&apos; # Secret = request.GET[&apos;secret&apos;] Secret = pyotp.random_base32() K = base64.b32decode(Secret, True) C = struct.pack(&quot;&gt;Q&quot;, int(time.time()) // 30) H = hmac.new(K, C, hashlib.sha1).digest() O = H[19] &amp; 15 # bin(15)=00001111=0b1111 DynamicPasswd = str((struct.unpack(&quot;&gt;I&quot;, H[O:O + 4])[0] &amp; 0x7fffffff) % 1000000) TOTP = str(0) + str(DynamicPasswd) if len(DynamicPasswd) &lt; 6 else DynamicPasswd url = &quot;otpauth://totp/&quot; + name + &quot;?secret=%s&quot; % Secret + &quot;&amp;issuer=Anchnet&quot; qr = QRCode(version=1, error_correction=constants.ERROR_CORRECT_L, box_size=6, border=4) qr.add_data(url) qr.make(fit=True) img = qr.make_image() codeinfo = &#123;&quot;name&quot;:name, &quot;MFAcode&quot;:TOTP, &quot;Secret&quot;:Secret, &quot;QRurl&quot;:url&#125; return restful.result(message=&quot;获取成功&quot;,data=codeinfo) 主urls.py 12345678from django.urls import path,includefrom apps.news import viewsurlpatterns = [ ... path(&apos;account/&apos;,include(&quot;apps.xfzauth.urls&quot;)), ...] apps的urls.py 12345678910from django.urls import pathfrom . import viewsapp_name = &apos;xfzauth&apos;urlpatterns = [ ... path(&apos;code/&apos;,views.getMFAinfo,name=&apos;code&apos;), ...] 测试 验证码生成测试 接口测试]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>MFA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群搭建]]></title>
    <url>%2F2019%2F07%2F19%2FHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基础配置 三台机器上编辑/etc/hosts配置文件，修改主机名。 三台机器在集群中担任的角色： master:NameNode、DataNode、ResourceManager、NodeManager node1:DataNode、NodeManager node2:DataNode、NodeManager 配置ssh免密登录 1234# 三台机器上分别执行，互相免密登录ssh-keygen -t rsassh-copy-id -i ~/.ssh/id_rsa.pub node1ssh-copy-id -i ~/.ssh/id_rsa.pub node2 安装JDK 1234# 官网下载https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.htmlhttp://anchnet-script.oss-cn-shanghai.aliyuncs.com/oracle/jdk-8u171-linux-x64.rpmyum localinstall jdk-8u171-linux-x64.rpm -y 配置java环境变量 12345678# 三台均操作vim /etc/profileJAVA_HOME=/usr/java/jdk1.8.0_171-amd64JAVA_BIN=/usr/java/jdk1.8.0_171-amd64/binJRE_HOME=/usr/java/jdk1.8.0_171-amd64/jreexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH# source命令加载配置文件，让其生效source /etc/profile Hadpop配置分发 下载hadoop 12https://www.apache.org/dist/hadoop/common/wget https://www.apache.org/dist/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz 解压 1tar -zxvf hadoop-2.6.5.tar.gz -C /data/modules 查看hadoop目录结构 1cd /data/modules/hadoop-2.6.5/ bin目录存放可执行文件 etc目录存放配置文件 sbin目录下存放服务的启动命令 share目录下存放jar包与文档 配置hadoop环境变量 1234vim ~/.bash_profileexport HADOOP_HOME=/data/modules/hadoop-2.6.5/export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHsource !$ 编辑hadoop配置文件 编辑etc/core-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://10.234.2.169:8020&lt;/value&gt; # 指定默认的访问地址以及端口号 &lt;/property&gt; 编辑etc/hdfs-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/app/tmp/dfs/name&lt;/value&gt; # namenode临时文件所存放的目录 &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/app/tmp/dfs/data&lt;/value&gt; # datanode临时文件所存放的目录 &lt;/property&gt; 创建数据目录 12mkdir -p /data/hadoop/app/tmp/dfs/namemkdir -p /data/hadoop/app/tmp/dfs/data 编辑etc/yarn-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 编辑etc/mapred-site.xml 123456789cp mapred-site.xml.template mapred-site.xmlvim !$&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; # 指定mapreduce运行在yarn框架上 &lt;/property&gt;&lt;/configuration&gt; 配置从节点的主机名etc/slaves 12node1node2 2台node节点也需创建对应安装目录，环境变量配置等 123456scp ~/.bash_profile node1:~/.bash_profilescp * node1:/data/modules/hadoop-2.6.5/etc/hadoop/# 2台节点上如下操作source ~/.bash_profilemkdir -p /data/hadoop/app/tmp/dfs/namemkdir -p /data/hadoop/app/tmp/dfs/data Hadoop格式化及启停 对NameNode做格式化，只需要在master上执行即可 12[root@master hadoop]# hdfs namenode -format# 格式化是对HDFS这个分布式文件系统中的DataNode进行分块，统计所有分块后的初始元数据的存储在NameNode中 格式化之后就可以启动Hadoop集群 12# sbin目录下,执行脚本，执行后要输入master服务器的密码[root@master hadoop]# ./start-all.sh 查看机器进程 master 123456789101112131415161718[root@master sbin]# jps19237 SecondaryNameNode22231 NodeManager21978 ResourceManager19019 NameNode30941 Jps21327 DataNode# 如果master上的某些服务进程未启动，需要再次手动执行脚本启动# 启动NameNodesbin/hadoop-daemon.sh start namenode# 启动DataNodesbin/hadoop-daemon.sh start datanode# 启动SecondaryNameNodesbin/hadoop-daemon.sh start secondarynamenode# 启动Resourcemanagersbin/yarn-daemon.sh start resourcemanager# 启动nodemanagersbin/yarn-daemon.sh start nodemanager node1234[root@node1 ~]# jps25827 Jps18742 DataNode20583 NodeManager 访问测试 访问主节点 50070 访问主节点 YARN的web页面 点击“Active Nodes”查看存活的节点 执行命令 12345hdfs dfs -ls /hdfs dfs -mkdir /test123hdfs dfs -put ./test.sh /test123# 然后去其他节点查看，不同的节点，访问的数据也是一样的hdfs dfs -ls / 运行MapReduce Job 在Hadoop的share目录里，自带了一些jar包，里面带有一些mapreduce实例小例子。位置在/data/modules/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar 创建输入目录 1234567bin/hdfs dfs -mkdir -p /test1/input# 原始文件[root@master hadoop-2.6.5]# cat /data/hadoop/test1.input hadoop hadoop hivemapreduce hive stormsqoop hadoop hivespark hadoop hbase 上传到HDFS的/test1/input目录中 运行WordCount MapReduce Job1[root@master hadoop-2.6.5]# bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount /test1/input /test1/output YARN web控制台查看 查看输出结果 123456789101112131415[root@master hadoop-2.6.5]# hdfs dfs -ls /test1/outputFound 2 items-rw-r--r-- 3 root supergroup 0 2018-12-13 15:22 /test1/output/_SUCCESS-rw-r--r-- 3 root supergroup 60 2018-12-13 15:22 /test1/output/part-r-00000# output目录中有两个文件，_SUCCESS文件是空文件，有这个文件说明Job执行成功。part-r-00000文件是结果文件，其中-r-说明这个文件是Reduce阶段产生的结果。一个reduce会产生一个part-r-开头的文件。# 查看执行结果[root@master hadoop-2.6.5]# hdfs dfs -cat /test1/output/part-r-00000hadoop 4hbase 1hive 3mapreduce 1spark 1sqoop 1storm 1# 结果是按照键值排序好的 停止Hadoop 123456789# 主节点上执行 stop-all.sh，如果进程未全部终止，就执行如下脚本对应停止。[root@master hadoop-2.6.5]# sbin/hadoop-daemon.sh stop namenodestopping namenode[root@master hadoop-2.6.5]# sbin/hadoop-daemon.sh stop datanodestopping datanode[root@master hadoop-2.6.5]# sbin/yarn-daemon.sh stop resourcemanagerstopping resourcemanager[root@master hadoop-2.6.5]# sbin/yarn-daemon.sh stop nodemanagerstopping nodemanager 开启历史服务 Hadoop开启历史服务可以在web页面上查看Yarn上执行job情况的详细信息。可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。 开启日志聚集1234567891011# Hadoop默认是不启用日志聚集的。在yarn-site.xml文件里配置启用日志聚集。&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;106800&lt;/value&gt;&lt;/property&gt;# yarn.log-aggregation-enable:是否启用日志聚集功能。# yarn.log-aggregation.retain-seconds：设置日志保留时间，单位是秒。 将配置文件发布到其他节点： 12scp etc/hadoop/yarn-site.xml node1:/data/modules/hadoop-2.6.5/etc/hadoop/scp etc/hadoop/yarn-site.xml node2:/data/modules/hadoop-2.6.5/etc/hadoop/ 重启Yarn进程和HistoryServer进程 1234sbin/stop-yarn.shsbin/start-yarn.shsbin/mr-jobhistory-daemon.sh stop historyserversbin/mr-jobhistory-daemon.sh start historyserver 测试日志聚集运行一个demo MapReduce，使之产生日志：12[root@master hadoop-2.6.5]# bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount2 /test1/input /test1/output2# 访问主节点的19888端口，运行job后就可以在web页面查看各个Map和Reduce的日志了。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django-debug-toolbar]]></title>
    <url>%2F2019%2F07%2F19%2FDjango-debug-toolbar%2F</url>
    <content type="text"><![CDATA[django开发中的调试工具组件Django-Debug-Toolbar 介绍django-debug-toolbar 是一组可配置的面板，可显示有关当前请求/响应的各种调试信息，并在单击时显示有关面板内容的更多详细信息。 在用django开发时，最喜欢的就是ORM功能，对于不熟悉sql语句的朋友是一大福利！但是，问题也在于此，在遍历的同时也会带来一系列问题。如：一不小心，语句写的有问题就会造成全表数据加载。 安装1234# 文档地址：https://django-debug-toolbar.readthedocs.io/en/latest/installation.html # 使用pip安装pip3 install django-debug-toolbar 配置 添加debug_toolbar到INSTALL_APPS中 1234567# 先决条件，确保&apos;django.contrib.staticfiles&apos;被正确设置，&apos;debug_toolbar&apos;要在&apos;django.contrib.staticfiles&apos;下面。INSTALLED_APPS = [ # ... &apos;django.contrib.staticfiles&apos;, # ... &apos;debug_toolbar&apos;,] 设置URL 1234567891011121314# 将调试工具栏的URL添加到项目的URLconf中，在主urls.py里from django.urls import path,includefrom apps.news import viewsfrom django.conf.urls.static import staticfrom django.conf import settingsurlpatterns = [ path(&apos;&apos;,views.index,name=&apos;index&apos;), # ...]if settings.DEBUG: import debug_toolbar urlpatterns.append(path(&quot;__debug__/&quot;,include(debug_toolbar.urls))) 启用中间件 1234567# 调试工具栏主要在中间件中实现，在设置模块中启用它。MIDDLEWARE = [ # ... &apos;debug_toolbar.middleware.DebugToolbarMiddleware&apos;, # ...]# 顺序MIDDLEWARE很重要，您应该尽早在列表中包含Debug Toolbar中间件。但是，它必须在编码响应内容的任何其他中间件之后，例如 GZipMiddleware。如果没用到GZip那就放到首位。 配置内部IP 123# 仅当你的IP地址在INTERNAL_IPS设置中列出时，才会显示调试工具栏 。这意味着，对于当地的发展，你必须添加&apos;127.0.0.1&apos;到INTERNAL_IPS; 如果您的设置模块中尚不存在，则需要创建此设置：INTERNAL_IPS = [&apos;127.0.0.1&apos;] 自定义配置调试工具栏提供了两个设置，您可以在项目的设置模块中添加这些设置以自定义其行为。 DEBUG_TOOLBAR_PANELS1234567891011121314151617181920212223242526272829# 此设置指定要包含在工具栏中的每个面板的完整Python路径。它就像Django的MIDDLEWARE设置一样。根据需求开启对应功能。DEBUG_TOOLBAR_PANELS = [ # 代表是哪个django版本 &apos;debug_toolbar.panels.versions.VersionsPanel&apos;, # 用来计时的，判断加载当前页面总共花的时间 &apos;debug_toolbar.panels.timer.TimerPanel&apos;, # 读取django中的配置信息 &apos;debug_toolbar.panels.settings.SettingsPanel&apos;, # 看到当前请求头和响应头信息 &apos;debug_toolbar.panels.headers.HeadersPanel&apos;, # 当前请求的想信息（视图函数，Cookie信息，Session信息等） &apos;debug_toolbar.panels.request.RequestPanel&apos;, # 查看SQL语句 &apos;debug_toolbar.panels.sql.SQLPanel&apos;, # 静态文件 &apos;debug_toolbar.panels.staticfiles.StaticFilesPanel&apos;, # 模板文件 &apos;debug_toolbar.panels.templates.TemplatesPanel&apos;, # 缓存 &apos;debug_toolbar.panels.cache.CachePanel&apos;, # 信号 &apos;debug_toolbar.panels.signals.SignalsPanel&apos;, # 日志 &apos;debug_toolbar.panels.logging.LoggingPanel&apos;, # 重定向 &apos;debug_toolbar.panels.redirects.RedirectsPanel&apos;, # 提供了一个profiling panel，它包含了line_profiler的输出 &apos;debug_toolbar.panels.profiling.ProfilingPanel&apos;,] DEBUG_TOOLBAR_CONFIG1此字典包含所有其他配置选项。一些适用于工具栏本身，另一些适用于某些面板。 配置JQuery的URL12345678910# django-debug-toolbar 默认使用的是Google的地址，默认配置如下：JQUERY_URL = &apos;//ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js&apos;# 如果无法访问国外网站可在setting.py中配置一下DEBUG_TOOLBAR_CONFIG = &#123; &quot;JQUERY_URL&quot;: &apos;//cdn.bootcss.com/jquery/2.2.4/jquery.min.js&apos;,&#125;或者你如果在Django项目中使用了jquery的话就可以直接将这一项置为空，那么django-debug-toolbar 就会使用你项目中用到的jquery:DEBUG_TOOLBAR_CONFIG = &#123; &quot;JQUERY_URL&quot;: &apos;&apos;,&#125; 使用测试访问页面的时候在右侧有各项配置面板，点击即可查看各种调试信息。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>toolbar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派相关配置]]></title>
    <url>%2F2019%2F07%2F19%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[下载镜像12345# 建议下载带桌面服务的https://www.raspberrypi.org/downloads/raspbianWin32 DiskImager，这是一个把镜像写入SD卡的工具：https://sourceforge.net/projects/win32diskimager/# 解压出img镜像文件，点击Write，写入系统。 格式化为F32 WIFI网络设置12345678910111213# 树莓派3B，树莓派官方Raspbian系统久加入了允许在开机前对 WiFi 网络进行配置的机制。# 原文链接：http://shumeipai.nxez.com/2017/09/13/raspberry-pi-network-configuration-before-boot.html#more-3463# 将刷好 Raspbian 系统的 SD 卡用电脑读取。在 boot 分区，也就是树莓派的 /boot 目录下新建 wpa_supplicant.conf 文件，按照下面的参考格式填入内容并保存 wpa_supplicant.conf 文件。country=CNctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1#如果你的 WiFi 使用WPA/WPA2加密network=&#123;ssid=&quot;你的无线网络名称（ssid）&quot;key_mgmt=WPA-PSKpsk=&quot;你的wifi密码&quot;&#125; 开启SSH服务1234同样在 boot 分区新建一个文件，空白的即可，文件命名为 ssh。注意要小写且不要有任何扩展名。树莓派在启动之后会在检测到这个文件之后自动启用 ssh 服务。随后即可通过登录路由器找到树莓派的 IP 地址，通过 ssh 连接到树莓派了。树莓派默认账户：pi；默认密码：raspberry使用pi账户进行登陆命令行，执行命令：【sudo passwd root】设置root用户密码，然后在执行【sudo passwd --unlock root】开启root账户，在使用【su root】测试是否生效！重新锁定root账户可执行命令：sudo passwd --lock root 树莓派更新vim1234567# 系统自带的vi编辑器很坑，需要卸载之前的vi，然后重新安装sudo apt-get remove vim-common；sudo apt-get install vim# 更改vi的配置（个人喜好）# 编辑/etc/vim/vimrc文件，在末尾添加以下内容set nu #显示行号syntax on #语法高亮set tabstop=4#tab 退四格 修改默认源12345# 备份/etc/apt/sources.list 文件，编辑/etc/apt/sources.list，删除之前内容，添加如下源(清华大学源)deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main non-free contribdeb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main non-free contrib# 保存退出sudo apt-get update &amp;&amp; apt-get upgrade -y 开启VNC123raspi-config选择5 Interfacing Options -&gt; P3 VNC调整分辨率，选择7 Advanced Options -&gt; A5 Resolution 安装teamviewer12345678910111213141516171819202122232425262728a、下载wget http://download.teamviewer.com/download/linux/version_11x/teamviewer-host_armhf.debsudo dpkg -i teamviewer-host_armhf.debsudo apt-get -f installb、安装GDebi，解决依赖问题sudo apt-get install gdebic、安装Teamviewersudo gdebi teamviewer-host_armhf.debd、命令行终端环境#查看帮助信息teamviewer help#查看本机IDteamviewer info#设置本机密码sudo teamviewer passwd [你的密码]#启动TeamViewer服务sudo teamviewer --daemon start#开启TeamViewer服务随机启动sudo teamviewer --daemon enable#重启即可连接sudo reboote、常用管理命令teamviewer --daemon start 启动TeamViewer服务teamviewer --daemon stop 停止TeamViewer服务teamviewer --daemon restart 重启TeamViewerteamviewer --daemon disable 关闭TeamViewer服务随机启动teamviewer --daemon enable 开启TeamViewer服务随机启动/usr/bin/teamviewer &amp; 打开teamviewer控制面板]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker上安装mongodb]]></title>
    <url>%2F2019%2F07%2F19%2FDocker%E4%B8%8A%E5%AE%89%E8%A3%85mongodb%2F</url>
    <content type="text"><![CDATA[查看下载镜像12345# 查看镜像docker search mongo# 拉取镜像docker pull mongo# 如果要拉取指定版本，可在docker hub上查看具体所有的mongo版本，然后使用 mongo:x.x 拉取指定版本。 运行启动容器1234567docker run --name mongodb -p 27072:27017 -v /data/mongodb/db:/data/db -v /data/mongodb/configdb:/data/configdb -d mongo --auth# --name ：指定容器名# -p ：指定容器暴露端口，宿主机端口:容器内端口# -v ：指定容器存储卷，宿主机目录:容器内目录# -d ：设置容器为后台运行，后面的mongo为镜像名# --auth ：开启密码授权访问docker ps # 查看刚创建的容器 创建管用户 以admin用户身份进入mongo，创建管理员用户 1234567891011121314151617181920212223242526272829303132docker exec -it mongodb bash# -it : 已交互式的方式# 使用mongo命令，进入mongouse admin;db.createUser(&#123; user: &apos;root&apos;, pwd: &apos;mongo&apos;, roles: [ &#123; role: &quot;root&quot;, db: &quot;admin&quot;&#125;]&#125;);db.auth(&apos;root&apos;,&apos;mongo&apos;) # 返回1成功# 退出，再进入docker exec -it mongodb mongo admindb.auth(&apos;root&apos;,&apos;mongo&apos;)# 创建普通用户db.createUser(&#123; user: &apos;root&apos;, pwd: &apos;mongo&apos;, roles: [ &#123; role: &quot;readWriteAnyDatabase&quot;, db: &quot;admin&quot;&#125;]&#125;);# # role后面的参数参考,可根据时间情况选择：Read：允许用户读取指定数据库readWrite：允许用户读写指定数据库dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profileuserAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。root：只在admin数据库中可用。超级账号，超级权限# 验证db.auth(&apos;root&apos;,&apos;mongo&apos;) 客户端管理工具12# 下载Robo 3Thttps://robomongo.org/download 创建用户 python调用mongo存储数据123456789101112131415# 先安装pymongo包import pymongo# 建立连接client = pymongo.MongoClient(&apos;mongodb://liyk:***@43.x.x.x:27072/&apos;)# 没密码认证可如下连接# client = pymongo.MongoClient(&apos;43.x.x.x&apos;,27072)# 新建名为weather的数据库book_weather = client[&apos;weather&apos;]# 在weather库中新建名为sheet_weather_1的表sheet_weather = book_weather[&apos;sheet_weather_1&apos;]dict_data = strhtml2.json()# 向数据库表中插入数据sheet_weather.insert_one(dict_data)]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle DG]]></title>
    <url>%2F2019%2F07%2F19%2FOracle-DG%2F</url>
    <content type="text"><![CDATA[Data Guard DataGuard是甲骨文推出的一种高可用性数据库方案，在Oracle 8i之前被称为Standby Database。从Oracle 9i开始，正式更名为Data Guard。它是在主节点与备用节点间通过日志同步来保证数据的同步，可以实现数据库快速切换与灾难性恢复。Data Guard只是在软件上对数据库进行设置，并不需要额外购买任何组件。用户能够在对主数据库影响很小的情况下，实现主备数据库的同步。而主备机之间的数据差异只限于在线日志部分，因此被不少企业用作数据容灾解决方案。 写在前面1234# 开库顺序先启备库，再启主库（启动监听，打开告警日志）# 关库顺序先关主库再关备库 配置准备 查看数据库版本123# 主库ip：192.168.100.2；备库ip：192.168.100.3# 查看数据库版本，必须是企业版否则不支持oracle data guardSQL&gt; select * from v$version; 备库只安装软件 确保备库安装路径、实例名与主库完全一致，避免同步出错 主库配置 在/app下创建interlib文件夹(自定义文件夹名),在创建文件夹log 12# 日志文件路径：&apos;logUrl=D:\app\interlib\log&apos;# 控制文件路径：&apos;standbyUrl=D:\app\interlib&apos; 开启归档模式12345678910111213# 在主库上启动数据库到mount模式，开启归档模式与force loggingsqlplus / as sysdbaSQL&gt;shutdown immediate; SQL&gt;startup mount; #修改为归档模式SQL&gt;alter database archivelog; SQL&gt;alter database open; #设置强制归档模式SQL&gt; alter database force logging;#查看命令：select log_mode,force_logging from v$database;#查看是否归档命令：Archive log list ; 为备库创建日志文件 1234# logUrl=D:\app\interlib\log 根据实际的“日志文件路径”改变SQL&gt; alter database add standby logfile group 4 (&apos;D:/app/interlib/log/STAN04.LOG&apos;) size 50m;SQL&gt; alter database add standby logfile group 5 (&apos;D:/app/interlib/log/STAN05.LOG&apos;) size 50m;SQL&gt; alter database add standby logfile group 6 (&apos;D:/app/interlib/log/STAN06.LOG&apos;) size 50m; 创建standby控制文件 1SQL&gt; alter database create standby controlfile as &apos;D:\app\interlib\standby.ctl&apos;; 导出当前数据库参数并修改 123456789101112131415161718192021222324252627282930313233343536373839404142SQL&gt; create pfile=&apos;D:/app/interlib/initora.ora&apos; from spfile;# 修改相关路径，以及增加没有的参数orcl.__db_cache_size=654311424orcl.__java_pool_size=16777216orcl.__large_pool_size=16777216orcl.__oracle_base=&apos;D:\app\Administrator&apos;#ORACLE_BASE set from environmentorcl.__pga_aggregate_target=704643072orcl.__sga_target=1023410176orcl.__shared_io_pool_size=0orcl.__shared_pool_size=318767104orcl.__streams_pool_size=0*.audit_file_dest=&apos;D:\app\Administrator\admin\orcl\adump&apos;*.audit_trail=&apos;db&apos;*.compatible=&apos;11.2.0.0.0&apos;*.control_files=&apos;D:\app\Administrator\oradata\orcl\control01.ctl&apos;,&apos;D:\app\Administrator\flash_recovery_area\orcl\control02.ctl&apos;*.db_block_size=8192*.db_domain=&apos;&apos;*.db_name=&apos;orcl&apos;*.db_recovery_file_dest=&apos;D:\app\Administrator\flash_recovery_area&apos;*.db_recovery_file_dest_size=4102029312*.diagnostic_dest=&apos;D:\app\Administrator&apos;*.dispatchers=&apos;(PROTOCOL=TCP) (SERVICE=orclXDB)&apos;*.local_listener=&apos;LISTENER_ORCL&apos;*.memory_target=1717567488*.open_cursors=300*.processes=150*.remote_login_passwordfile=&apos;EXCLUSIVE&apos;*.undo_tablespace=&apos;UNDOTBS1&apos;# 如下是增加的参数*.db_unique_name=&apos;primary&apos;*.archive_lag_target=1800*.fal_client=&apos;standby&apos;*.fal_server=&apos;primary&apos;*.log_archive_config=&apos;DG_CONFIG=(primary,standby)&apos;*.log_archive_dest_1=&apos;LOCATION=D:\app\interlib\log\ VALID_FOR=(all_logfiles,all_roles) db_unique_name=primary&apos;*.log_archive_dest_2=&apos;service=standby arch async valid_for=(online_logfiles,primary_role) db_unique_name=standby&apos;*.log_archive_dest_state_1=&apos;enable&apos;*.log_archive_dest_state_2=&apos;enable&apos;*.log_archive_format=&apos;%t_%s_%r.dbf&apos;*.DB_FILE_NAME_CONVERT=&apos;D:\app\Administrator\oradata\orcl\&apos;,&apos;D:\app\Administrator\oradata\orcl\&apos;*.LOG_FILE_NAME_CONVERT=&apos;D:\app\interlib\log&apos;,&apos;D:\app\interlib\log&apos;*.standby_file_management=&apos;auto&apos; 重新加载配置启动服务 12345678910#停止服务SQL&gt; shutdown immediate;#使用新参数文件启动数据库SQL&gt; startup pfile=&apos;D:\app\interlib\initora.ora&apos; nomount;#创建新的 spfile 文件SQL&gt; create spfile from pfile=&apos;D:\app\interlib\initora.ora&apos;;#停止服务SQL&gt; shutdown immediate;#启动服务SQL&gt; startup; 创建密码文件 123456# 如果有此步，存在密码文件可不操作# 在DOS窗口执行，不需要登录sqlplus，路径不能加引号，否则会报opw-00001错误 orapwd file=passwordUrl\PWDorcl.ora password=123 entries=10# 密码文件存放路径：密码文件存放路径：passwordUrl=D:\app\Administrator\product\11.2.0\dbhome_1\database\PWDorcl.ora 配置监听和访问服务 1234567891011121314151617181920212223242526272829# 修改 listener.ora# listener.ora Network Configuration File: D:\app\Administrator\product\11.2.0\dbhome_1\network\admin\listener.ora# Generated by Oracle configuration tools.SID_LIST_LISTENER = (SID_LIST = (SID_DESC = (SID_NAME = CLRExtProc) (ORACLE_HOME = D:\app\Administrator\product\11.2.0\dbhome_1) (PROGRAM = extproc) (ENVS = &quot;EXTPROC_DLLS=ONLY:D:\app\Administrator\product\11.2.0\dbhome_1\bin\oraclr11.dll&quot;) ) # 添加SID_DESC (SID_DESC = (GLOBAL_DBNAME = orcl) (ORACLE_HOME = D:\app\Administrator\product\11.2.0\dbhome_1) (SID_NAME = orcl) ) )LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.100.2)(PORT = 1521)) ) )ADR_BASE_LISTENER = D:\app\Administrator 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 修改tnsname.ora文件# tnsnames.ora Network Configuration File: D:\app\Administrator\product\11.2.0\dbhome_1\network\admin\tnsnames.ora# Generated by Oracle configuration tools.LISTENER_ORCL = (ADDRESS = (PROTOCOL = TCP)(HOST = 92.168.100.2)(PORT = 1521))ORACLR_CONNECTION_DATA = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) (CONNECT_DATA = (SID = CLRExtProc) (PRESENTATION = RO) ) )ORCL = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 1922.168.100.2)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = orcl) ) )# 增加主库配置PRIMARY = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP) (HOST = 192.168.100.2) (PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl) )) # 增加从库配置STANDBY= (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP) (HOST = 192.168.100.3) (PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl) )) 从库配置 拷贝数据到备库 主库和备库创建 D:\app\interlib\tmp 文件夹，并把interlib其余目录也拷贝过去 将主库oracle目录下的oradata文件夹下内容复制到从库相同目录 将D:\app\Administrator 目录下的admin,cfgtollogs,diag,flash_recover_area 目录以及密码文件(‘D:\app\Administrator\product\11.2.0\dbhome_1\database\PWDorcl.ora’)拷贝到备用库的相同路径。可直接覆盖 将主库的listener.ora和tnsname.ora拷贝到备库相同路径，并修改linstener.ora的ip为备库ip (‘D:\app\Administrator\product\11.2.0\dbhome_1\NETWORK\ADMIN’)1234567LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.100.3)(PORT = 1521)) )) 备库新建实例 1234# 备库新建实例，如果备库也安装了数据库，实例也是orcl这步可跳过# 在备库上注册oracle实例到服务中，cmd下执行oradim -new -sid orcllsnrctl start 修改备库参数并创建实例12345678910111213141516# 将从主库拷贝的 D:\app\interlib\initora.ora修改#下面是要修改的地方*.db_unique_name=&apos;standby&apos;*.archive_lag_target=1800*.fal_client=&apos;primary&apos;*.fal_server=&apos;standby&apos;*.log_archive_config=&apos;DG_CONFIG=(primary,standby)&apos;*.log_archive_dest_1=&apos;LOCATION=D:\app\interlib\log\ VALID_FOR=(all_logfiles,all_roles) db_unique_name=standby&apos;*.log_archive_dest_2=&apos;service=primary arch async valid_for=(online_logfiles,primary_role) db_unique_name=primary&apos;*.log_archive_dest_state_1=&apos;enable&apos;*.log_archive_dest_state_2=&apos;enable&apos;*.log_archive_format=&apos;%t_%s_%r.dbf&apos;*.DB_FILE_NAME_CONVERT=&apos;D:\app\Administrator\oradata\orcl\&apos;,&apos;D:\app\Administrator\oradata\orcl\&apos;*.LOG_FILE_NAME_CONVERT=&apos;D:\app\interlib\log\&apos;,&apos;D:\app\interlib\log\&apos;*.standby_file_management=&apos;auto&apos; 12345# 使用新参数文件建立从库实例SQL&gt; startup nomount pfile=&apos;D:\app\interlib\initora.ora&apos;;SQL&gt; create spfile from pfile=&apos;D:\app\interlib\initora.ora&apos;;SQL&gt; shutdown immediate; （此步骤可能会报错 01507,暂时忽略）SQL&gt; startup nomount; 主库执行相关语句 建立主库备份12345# 复制主库，使用RMAN建立备份，cmd下执行rman target /RMAN&gt; backup full database format=&apos;D:\app\interlib\tmp\FOR_STANDBY_%u%p%s,RMN&apos; include current controlfile for standby;# 将当前archivelog归档，执行sql语句RMAN&gt; sql &apos;alter system archive log current&apos;; 复制数据库 12345# 复制数据库；将主库D:\app\interlib\tmp\下产生的的备份集拷贝到备库的相同路径下# 拷贝完成后在主库刚才的RMAN中执行RMAN&gt; connect auxiliary sys/123.com@standby # 123.com为备库sys的密码, 可能会提示实例未装载# 如果提示无法连接，请检查防火墙等是否有限制RMAN&gt; duplicate target database for standby nofilenamecheck; 备库启动standby1234# 在备库执行sqlplus / as sysdbaalter database mount standby database; # 可能会报错 01100不管alter database recover managed standby database disconnect from session; 检查测试 状态查看测试12# 主库从库分别执行如果 APPLIED 列的值为 yes,表示重做应用成功SELECT SEQUENCE#,APPLIED FROM V$ARCHIVED_LOG ORDER BY SEQUENCE#; 1234# 或者查看切换归档，归档日志记录会+1select max(sequence#) from v$archived_log;alter system switch logfile;select max(sequence#) from v$archived_log; 日志查看测试 1# 主库上执行alter system switch logfile;，通过select name from v$archived_log; 可以看到主库和备库都增加了一个log文件(.DBF) 查看主备库状态 12# 执行sql语句select open_mode,protection_mode,database_role,switchover_status from v$database; DG切换123456789101112# 主库执行# 先将主库切换成备库，然后将原主库启动到物理库的状态alter database commit to switchover to physical standby with session shutdown;# 关闭主库shutdown immediate;# 开数据库nomountstartup nomount;# 更改主库为备库alter database mount standby database;alter database recover managed standby database disconnect from session;# 如果配置了 standby redo log 并需要启用实时同步则执行以下代码alter database recover managed standby database using current logfile disconnect from session; 1234567# 备库执行,switchover到primary# 更改备库为主库alter database commit to switchover to primary with session shutdown;# 如果备库还有未应用的日志则执行alter database recover managed standby database disconnect from session;shutdown immediate;startup DG切换后再恢复最初 即原主库切换为备库，再从备库切换为主库 123456# 开库顺序先启备库，再启主库（启动监听，打开告警日志）# 关库顺序先关主库再关备库lsnrctl stoplsnrctl start 主库操作 1234# 登录原主库rman target /RMAN&gt; connect auxiliary sys/123.com@standbyRMAN&gt; duplicate target database for standby nofilenamecheck; 备库stnadby 1234# 在备库执行sqlplus / as sysdbaalter database mount standby database; # 可能会报错 01100不管alter database recover managed standby database disconnect from session; 状态监测 1234# 或者查看切换归档，归档日志记录会+1select max(sequence#) from v$archived_log;alter system switch logfile;select max(sequence#) from v$archived_log;]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Memcache基础使用]]></title>
    <url>%2F2019%2F07%2F05%2FMemcache%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[连接语法12telnet HOST PORT telnet 127.0.0.1 11211 memcached set命令 Memcached set 命令用于将 value(数据值) 存储在指定的 key(键) 中。如果set的key已经存在，该命令可以更新该key所对应的原来的数据，也就是实现更新的作用。 参数说明如下： 123456key：键值 key-value 结构中的 key，用于查找缓存值。flags：可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。exptime：在缓存中保存键值对的时间长度（以秒为单位，0 表示永远）bytes：在缓存中存储的字节数noreply（可选）： 该参数告知服务器不需要返回数据value：存储的值（始终位于第二行）（可直接理解为key-value结构中的value） 实例（以下实例中我们设置）： 12345key → runoobflag → 0exptime → 900 (以秒为单位)bytes → 9 (数据存储的字节数)value → memcached 输出信息说明： STORED：保存成功后输出。 ERROR：在保存失败后输出。 memcached add命令 语法： 123add 命令的基本语法格式如下：add key flags exptime bytes [noreply]value 参数说明如下： 123456key：键值 key-value 结构中的 key，用于查找缓存值。flags：可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。exptime：在缓存中保存键值对的时间长度（以秒为单位，0 表示永远）bytes：在缓存中存储的字节数noreply（可选）： 该参数告知服务器不需要返回数据value：存储的值（始终位于第二行）（可直接理解为key-value结构中的value） 实例: 123456以下实例中我们设置：key → new_keyflag → 0exptime → 900 (以秒为单位)bytes → 10 (数据存储的字节数)value → data_value 输出:如果数据添加成功，则输出：STORED输出信息说明：STORED：保存成功后输出。NOT_STORED:在保持失败后输出。]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>Memcache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Memcache安装]]></title>
    <url>%2F2019%2F07%2F05%2FMemcached%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[简介Memcached是一个自由开源的，高性能，分布式内存对象缓存系统。Memcached是以LiveJournal旗下Danga Interactive公司的Brad Fitzpatric为首开发的一款软件。现在已成为mixi、hatena、Facebook、Vox、LiveJournal等众多服务中提高Web应用扩展性的重要因素。Memcached是一种基于内存的key-value存储，用来存储小块的任意数据（字符串、对象）。这些数据可以是数据库调用、API调用或者是页面渲染的结果。本质上，它是一个简洁的key-value存储系统。一般的使用目的是，通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web应用的速度、提高可扩展性。 特征memcached作为高速运行的分布式缓存服务器，具有以下的特点。 协议简单 基于libevent的事件处理 内置内存存储方式 memcached不互相通信的分布式 支持多语言许多语言都实现了连接memcached的客户端，其中以Perl、PHP为主。仅仅memcached网站上列出的有：Perl、PHP、Python、Ruby、C#、C/C++、Lua等等 安装(linux)123456789101.安装libevent、gcc库yum install -y libevent libevent-deve gcc-c++2.下载安装包(在官网下载最新稳定版)wget http://memcached.org/files/memcached-1.5.7.tar.gz3.解压安装tar -zxvf memcached-1.x.x.tar.gzcd memcached-1.x.x./configure &amp;&amp; make &amp;&amp; make test &amp;&amp; make install4.查看命令帮助/usr/local/bin/memcached -h 启动选项123456789/usr/local/bin/memcached -d -m 64M -u root -l 127.0.0.1 -p 11211 -c 256 -P /tmp/memcache.pid/usr/local/memcache/bin/memcached -d -l 127.0.0.1 -p 11211 -u root -m 64 -c 1024 -P /var/run/memcached.pid-d 是启动一个守护进程；-m 是分配给Memcache使用的内存数量，单位是MB；-u 是运行Memcache的用户；-l 是监听的服务器IP地址，可以有多个地址；-p 是设置Memcache监听的端口，，最好是1024以上的端口；-c 是最大运行的并发连接数，默认是1024；-P 是设置保存Memcache的pid文件。 安装(windows)memcached &lt;1.4.5 版本安装123456789101112131415http://static.runoob.com/download/memcached-win64-1.4.4-14.zip1、解压下载的安装包到指定目录。2、在 1.4.5 版本以前 memcached 可以作为一个服务安装，使用管理员权限运行以下命令：c:\memcached\memcached.exe -d install注意：你需要使用真实的路径替代 c:\memcached\memcached.exe。3、然后我们可以使用以下命令来启动和关闭 memcached 服务：c:\memcached\memcached.exe -d startc:\memcached\memcached.exe -d stop4、如果要修改 memcached 的配置项, 可以在命令行中执行 regedit.exe 命令打开注册表并找到 &quot;HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\memcached&quot; 来进行修改。如果要提供 memcached 使用的缓存配置 可以修改 ImagePath 为:&quot;c:\memcached\memcached.exe&quot; -d runservice -m 512-m 512 意思是设置 memcached 最大的缓存配置为512M。此外我们还可以通过使用 &quot;c:\memcached\memcached.exe -h&quot; 命令查看更多的参数配置。5、如果我们需要卸载 memcached ，可以使用以下命令：c:\memcached\memcached.exe -d uninstall memcached &gt;= 1.4.5 版本安装1234567891、解压下载的安装包到指定目录。2、在 memcached1.4.5 版本之后，memcached 不能作为服务来运行，需要使用任务计划中来开启一个普通的进程，在 window 启动时设置 memcached自动执行。我们使用管理员身份执行以下命令将 memcached 添加来任务计划表中：schtasks /create /sc onstart /tn memcached /tr &quot;&apos;c:\memcached\memcached.exe&apos; -m 512&quot;注意：你需要使用真实的路径替代 c:\memcached\memcached.exe。注意：-m 512 意思是设置 memcached 最大的缓存配置为512M。注意：我们可以通过使用 &quot;c:\memcached\memcached.exe -h&quot; 命令查看更多的参数配置。3、如果需要删除 memcached 的任务计划可以执行以下命令：schtasks /delete /tn memcached]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>Memcache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLserver维护计划]]></title>
    <url>%2F2019%2F07%2F05%2FSQLserver%E7%BB%B4%E6%8A%A4%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[备份新建维护计划管理-&gt; 维护计划 -&gt; 新建维护计划 a.新建维护计划，设置名称b.工具箱将“备份数据库”任务拖动到设计器界面 编辑备份数据库任务右键任务，点击编辑 可自行按需设置，选择备份类型、备份数据库、选择备份到的目录、勾选验证备份完整性等。 设置作业任务属性 设置计划类型、频率等 定期清理历史备份 前几个过程和备份相同，从工具箱里拖动“清除维护”任务,编辑 这里需要注意2点 a.搜索文件夹这里，不支持递归查找，如果有多个目录需要创建多个任务，单独制定到目录b.文件扩展名为“bak”,不要写为“.bak”,这样是无法识别到的 最后设置计划类型、频率等]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>sqlserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx版本平滑升级-编译安装]]></title>
    <url>%2F2019%2F07%2F05%2Fnginx%E7%89%88%E6%9C%AC%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、nginx编译安装 源码下载地址：http://nginx.org/en/download.html 安装依赖环境 1yum install gcc gcc-c++ automake pcre pcre-devel zlip zlib-devel openssl openssl-devel -y 下载源码包 1wget http://nginx.org/download/nginx-1.14.0.tar.gz 解压并编译 123tar -zxvf xxxx./configure --prefix=usr/local/nginxmake &amp;&amp; make install 软链接到/usr/bin下 1ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx 查看nginx版本 12nginx -v 查看nginx版本nginx -V 大写V，查看nginx版本及编译路径等设置 二、升级nginx1.下载需要升级的版本2.解压、编译但不安装 解压要升级的nginx，进入目录，编译和老版本一样的配置 1./configure --prefix=usr/local/nginx 编译生成objs目录，进入该目录替换nginx 1make 升级nginx不需要make install备份之前的nginx执行文件 12cd /usr/local/nginx/sbin mv nginx nginx.old 3.升级 把新编译的nginx执行脚本拷贝到对应的目录 12cp objs/nginx /usr/local/nginx/sbin/ make upgrade 4.查看版本 1nginx -v]]></content>
      <categories>
        <category>WEB</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片上传到七牛云及图片上传进度条]]></title>
    <url>%2F2019%2F07%2F02%2F%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E5%88%B0%E4%B8%83%E7%89%9B%E4%BA%91%E5%8F%8A%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E8%BF%9B%E5%BA%A6%E6%9D%A1%2F</url>
    <content type="text"><![CDATA[七牛云存储是一个集图片、视频对象存储为一体的网站。并且他上面集成了cdn加速服务，图片处理（加水印，图片裁剪）等功能，对于一些想要快速开发产品，不想花大量时间来构建自己资源服务器的中小型公司而言，无疑是最好的选择。更主要是有免费额度。 申请七牛云账号 申请链接：https://portal.qiniu.com ,申请步骤不做过多阐述。 查看密钥：个人中心-&gt; 密钥管理 （AK、SK） 创建空间：进入对象存储-&gt; 新建存储空间 （自定义空间名称、选择存储区域、设置为公开空间） 绑定域名：七牛默认提供的域名只有30天试用时间，建议更换为自己的域名(需备案),设置后将此域名解析到生成的cname，后续访问即可通过此域名访问到图片。 图片上传到七牛云代码 在模板中引入最新版(2.5.2)的JavaScript SDK 123456789101112131415# html# 引入最新版的JavaScript SDK&lt;script src=&quot;https://unpkg.com/qiniu-js@2.5.2/dist/qiniu.min.js&quot;&gt;&lt;/script&gt;# 简单上传界面&lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;thumbnail-form&quot;&gt;缩略图&lt;/label&gt; &lt;div class=&quot;input-group&quot;&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; id=&quot;thumbnail-form&quot; name=&quot;thumbnail&quot;&gt; &lt;span class=&quot;input-group-btn&quot;&gt; &lt;label class=&quot;btn btn-default btn-file&quot;&gt; 上传图片&lt;input hidden type=&quot;file&quot; class=&quot;btn btn-default&quot; id=&quot;thumbnail-btn&quot;&gt; &lt;/label&gt; &lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 然后监听一个type=file类型的按钮的change事件，一旦选择了文件，那么就会执行change事件，在change事件的处理函数中，我们就可以获取到当前选中的文件。然后通过七牛的SDK发送给服务器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# jsfunction News() &#123; var self = this; self.progressGroup = $(&quot;#progress-group&quot;); self.progressBar = $(&quot;.progress-bar&quot;);&#125;News.prototype.listenQiniuUploadFileEvent = function () &#123; var self = this; var thumbnailBtn = $(&apos;#thumbnail-btn&apos;); thumbnailBtn.change(function (event) &#123; var file = this.files[0]; xfzajax.get(&#123; &apos;url&apos;: &apos;/cms/qntoken/&apos;, &apos;success&apos;: function (result) &#123; var token = result[&apos;uptoken&apos;]; var key = (new Date()).getTime() + &apos;.&apos; + file.name.split(&apos;.&apos;)[1]; var putExtra = &#123; fname: key, params: &#123;&#125;, mimeType: [&apos;image/png&apos;,&apos;video/x-ms-wmv&apos;,&apos;image/jpeg&apos;] &#125;; var config = &#123; useCdnDomain: true, retryCount: 6, region: qiniu.region.z2 &#125;; var observable = qiniu.upload(file, key, token, putExtra,config); observable.subscribe(&#123; &quot;next&quot;:self.updateUploadProgress, &quot;error&quot;:self.uploadErrorEvent, &quot;complete&quot;: self.complateUploadEvent &#125;); self.progressGroup.show(); &#125; &#125;); &#125;);&#125;;News.prototype.updateUploadProgress = function (response) &#123; var self = this; var total = response.total; var percent = total.percent; var percentText = percent.toFixed(0) + &apos;%&apos;; var progressBar = $(&quot;.progress-bar&quot;); progressBar.css(&#123;&quot;width&quot;:percentText&#125;); progressBar.text(percentText);&#125;;News.prototype.uploadErrorEvent = function (error) &#123; window.messageBox.showError(error.message);&#125;;News.prototype.complateUploadEvent = function (response) &#123; var self = this; var filename = response[&apos;key&apos;]; var domain = &quot;http://img.key***.cn/&quot;; var thumbnailUrl = domain + filename; var thumbnailInput = $(&quot;#thumbnail-form&quot;); thumbnailInput.val(thumbnailUrl); var progressGroup = $(&quot;#progress-group&quot;); progressGroup.hide();&#125;; 上传进度条Demo通过进度条，灵活的为当前工作流程或动作提供实时反馈。 https://v3.bootcss.com/components/#progress 效果图 代码 前端部分 12345678910111213141516171819# html&lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;thumbnail-form&quot;&gt;缩略图&lt;/label&gt; &lt;div class=&quot;input-group&quot;&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; id=&quot;thumbnail-form&quot; name=&quot;thumbnail&quot;&gt; &lt;span class=&quot;input-group-btn&quot;&gt; &lt;label class=&quot;btn btn-default btn-file&quot;&gt; 上传图片&lt;input hidden type=&quot;file&quot; class=&quot;btn btn-default&quot; id=&quot;thumbnail-btn&quot;&gt; &lt;/label&gt; &lt;/span&gt; &lt;/div&gt;&lt;/div&gt;&lt;div id=&quot;progress-group&quot; class=&quot;form-group&quot; style=&quot;display: none;&quot;&gt; &lt;div class=&quot;progress&quot;&gt; &lt;div class=&quot;progress-bar progress-bar-success progress-bar-striped&quot; role=&quot;progressbar&quot; aria-valuenow=&quot;40&quot; aria-valuemin=&quot;0&quot; aria-valuemax=&quot;100&quot; style=&quot;width: 0%&quot;&gt; 0% &lt;/div&gt; #内容起始设置为0%，这样初始上传时就只从0开始 &lt;/div&gt;&lt;/div&gt; 1234567891011121314# js// 上传函数、进度条显示News.prototype.handleFileUploadProgress = function (response) &#123; var total = response.total; var percent = total.percent; var percentText = percent.toFixed(0)+&apos;%&apos;; // 24.0909，89.000.... var progressGroup = News.progressGroup; // undefined/None progressGroup.show(); var progressBar = $(&quot;.progress-bar&quot;); progressBar.css(&#123;&quot;width&quot;:percentText&#125;); progressBar.text(percentText);&#125;;]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell脚本一键mvn发布war包]]></title>
    <url>%2F2019%2F07%2F02%2FShell%E8%84%9A%E6%9C%AC%E8%87%AA%E5%8A%A8mvn%E5%8F%91%E5%B8%83war%E5%8C%85%2F</url>
    <content type="text"><![CDATA[脚本初衷 客户想减少手动发布过程，减少部分工作量。但是客户不会使用jenkins，想直接脚本执行然后发布。 脚本发布步骤 登录github，拉取指定项目到本地 构建war包 停止tomcat服务 替换war包 启动tomcat服务 详细脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195#!/bin/bash# author:key.liyk# version: v1.0# func: auto build war package[ -f /etc/init.d/functions ] &amp;&amp; . /etc/init.d/functions[ $(id -u) != &quot;0&quot; ] &amp;&amp; echo &quot;You need use root to run this script&quot; &amp;&amp; exit 1clearcat &lt;&lt;EOF$(printf %.s# &#123;1..44&#125;)# _ _ ## __ _ _ __ ___| |__ _ __ ___| |_ ## / _\ | &apos;_ \ / __| &apos;_ \| &apos;_ \ / _ \ __| ## | (_| | | | | (__| | | | | | | __/ |_ ## \__,_|_| |_|\___|_| |_|_| |_|\___|\__| ## ## The script for maven deploy! #$(printf %.s+ &#123;1..44&#125;)EOFcat &lt;&lt;EOF 1: Maven deploy 2: EXIT (or ctrl+c) EOF# 定义相关目录git_dir=/data/softapps #定义git拉取存放目录tomcat_dir=/data/tomcat9 #定义tomcat目录git_url=&quot;https://github.com/liyk1024/wartest.git&quot; #定义自己github项目clone路径git_name=wartest #github项目名remove_dir=/tmp/remove-war #移除后war目录log_file=/tmp/install.log #定义日志目录mvn_path=/data/maven3/bin# 定义日志显示信息do_log() &#123; local color=$1 local level=$2 shift # shift位置参数偏移 shift local msg=$@ local prefix=&quot;&quot; if [[ $color -gt 30 ]]; then prefix=&quot;\033[1;$color;40m[$level]\033[0m&quot; else prefix=&quot;[$level]&quot; fi echo -e &quot;$prefix [`date +&apos;%Y-%m-%d %H:%M:%S&apos;`] $msg&quot;&#125;log_info() &#123; local msg=$@ do_log 0 INFO $msg&#125;log_warn() &#123; local msg=$@ do_log 33 WARN $msg&#125;log_error() &#123; local msg=$@ do_log 31 ERROR $msg&#125;log_success() &#123; local msg=$@ do_log 32 SUCCESS $msg&#125;# 检查目录check_dir() &#123;log_info check directory...for dir in $*;do [ ! -d $dir ] &amp;&amp; mkdir -p $dir &amp;&gt;&gt; $log_filedone[ $? -eq 0 ] &amp;&amp; log_success check over&#125;# 检查基础命令check_command() &#123;log_info check basic commands...for cmd in $*;do hash $cmd &amp;&gt;&gt; $log_file if [ $? -ne 0 ];then action &quot;$&#123;cmd&#125; check&quot; /bin/false log_info $&#123;cmd&#125; not find,installing... yum -y install $cmd &amp;&gt;&gt; $log_file else action &quot;$&#123;cmd&#125; check&quot; /bin/true fidone[ $? -eq 0 ] &amp;&amp; log_success check over&#125;# 检查javacheck_java() &#123;log_info check java...hash java &amp;&gt;&gt; $log_fileif [ $? -eq 0 ];then action &quot;java check&quot; /bin/trueelse action &quot;java check&quot; /bin/false log_error Pls install java! exit 1fi&#125;# 检查mavencheck_mvn() &#123;log_info check maven...hash mvn &amp;&gt;&gt; $log_fileif [ $? -ne 0 ];then action &quot;maven check&quot; /bin/trueelse action &quot;maven check&quot; /bin/false log_error Pls install maven! exit 1fi&#125;# 拉取代码git_pull() &#123;log_info check git project...find $&#123;git_dir&#125; -type d -name $&#123;git_name&#125; |xargs rm -rvf &amp;&gt;&gt; $log_filelog_info git clone...cd $&#123;git_dir&#125;git clone $git_url &amp;&gt;&gt; $log_file[ $? -eq 0 ] &amp;&amp; log_success git pull over! &#125;# 构建war包build() &#123;log_info begin build warPackage...cd $&#123;git_dir&#125;/$&#123;git_name&#125;if [ -f pom.xml ];then $&#123;mvn_path&#125;/mvn clean package -Dmaven.test.skip=true &amp;&gt;&gt; $log_file war=`find $&#123;git_dir&#125;/$&#123;git_name&#125; -name &quot;*.war&quot; |wc -l` if [ $war -eq 1 ];then log_success build war success! else log_error build war fail! exit 1 fielse log_warn Pls check,not find pom.xml! exit 1fi&#125;# 停止tomcatstop_tomcat() &#123;sh $&#123;tomcat_dir&#125;/bin/shutdown.sh &amp;&gt;&gt; $log_file[ $? -eq 0 ] &amp;&amp; log_success stop_tomcat over!&#125;# 替换war包remove_war() &#123;cd $&#123;tomcat_dir&#125;/webappsif [ -d ROOT ];then rm -rf ROOTfiwar_url=`find $&#123;git_dir&#125;/$&#123;git_name&#125; -name &quot;*.war&quot;`cp $&#123;war_url&#125; $&#123;tomcat_dir&#125;/webapps/ROOT.war&#125;# 启动tomcatstart_tomcat() &#123;sh $&#123;tomcat_dir&#125;/bin/startup.sh &amp;&gt;&gt; $log_file[ $? -eq 0 ] &amp;&amp; log_success startup_tomcat over!&#125;main() &#123;check_dir $git_dir $remove_dircheck_command git wgetcheck_javacheck_mvngit_pullbuildstop_tomcatremove_warstart_tomcat&#125;# 根据选择操作read -p &quot;Please input your choice:&quot; choiceif [ $choice = 1 ];then echo &quot;&quot; echo -e &quot;You can view detail use: \033[5;33mtail -f $&#123;log_file&#125;\033[0m&quot; log_info Start Maven deploy... mainelif [[ $choice = 2 ]];then log_info Your choice Exit! exit 0else log_warn pls choice 1|2 exit 1fi]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯云网络灾备方案]]></title>
    <url>%2F2019%2F07%2F01%2F%E8%85%BE%E8%AE%AF%E4%BA%91%E7%BD%91%E7%BB%9C%E7%81%BE%E5%A4%87%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[客户需求上海区云、北京区云、IDC互通。上海区是生产环境、北京区是灾备、IDC是办公室机房 具体要求：1、2条专线分别从IDC-上海、IDC-北京，能做到双活切换2、上海、北京互通，北京灾备保持与上海的数据一致。 实施难点 网段重叠：客户的上海、北京、IDC均是172.18.0.0/16网段 专线切换：IDC-上海、IDC-北京，任意一条专线中断可自动切换到另外一条。 需求分析1、由于网段重叠，无法使用对等连接，只能使用云联网 来解决网段重叠问题。2、客户IDC也是172.18.0.0/16网段，导致在上海、北京分别到IDC的路由冲突，无法同时存在。故只能做到主备，正常开启上海的路由，上海-IDC专线中断后切换到北京-IDC。使用python调用云联网SDK操作开启/关闭路由。3、北京灾备方面，云数据库用DTS实时同步数据、网站文件利用COS迁移工具将上海CVM数据迁移到北京COS，然后北京服务器再从COS取数据。或者直接使用rsync同步数据。北京服务器可由上海已配置环境的服务器做镜像，镜像复制到北京，再利用镜像开出服务器。 操作配置 1、云联网配置 新建云联网(目前公测阶段，需要申请此产品)，并关联对应实例 关联实例后会自动把实例所拥有的路由自动添加到云联网路由表里。 2、专线配置(云平台) 专线网关创建配置(云产品-&gt;私有网络-&gt;专线网关)新建专线网关时，关联网络类型要选择为云联网，云联网实例可现在关联也可之后关联。 专用通道创建配置(云产品-&gt;专线接入-&gt;专用通道)创建专用通道时选择专线类型，本例使用的是共享专线，需要填写专线提供方的账户ID、共享专线ID，接入网络选择云联网。一条专线(专用通道)对应一个专线网关。VLAN ID、IDC侧BGP AS号这些需向专线提供方索要，边界IP和专线提供方协商规划。这些配置完毕后等待专线提供方接受提交的申请，然后再IDC设置上配置BGP宣告IDC内网网段，如果路由方式是静态路由那就在设备上写到云上的路由。 3、专线网关上添加路由 点击需要添加路由的专线网关，进入IDC网段添加需要的IDC网段如果此专线网关未加入云联网，请在云联网下关联此专线网关，在专线网关上添加的路由会自动添加到云联网路由表里。在云联网路由表里可看出路由的详情，下一跳、是否启动该路由等。如果存在路由重叠，优先匹配长掩码(例如同网段24和25掩码,优先匹配25掩码的路由)。如果路由网段相同后添加的则自动禁用此路由。 使用云联网SDK自动切换路由 1、编写相关代码将代码放置到相关服务器上，触发脚本条件可设置为ping隧道边界ip及IDC内网ip，如果同时不通则触发脚本切换专线。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#! /usr/bin/env python# -*- coding:utf-8 -*-# Auther: liyk time:2019/1/3# File : CCN.pyfrom tencentcloud.common import credentialfrom tencentcloud.vpc.v20170312 import vpc_client,modelsimport jsondef Auth_vpc(id,key): # 认证ak cred = credential.Credential(id,key) Vpc_client = vpc_client.VpcClient(cred,&quot;ap-shanghai&quot;) return Vpc_clientdef SH_DescribeCcnRoutesRequest(Vpc_client): # 获取上海专线网关路由ID req = models.DescribeCcnRoutesRequest() req.CcnId = &apos;ccn-a187ua1z&apos; resp = Vpc_client.DescribeCcnRoutes(req) result = json.loads(resp.to_json_string()) RouteSet = result[&apos;RouteSet&apos;] SH_RouteIds = [] for i in RouteSet: if i[&apos;InstanceName&apos;] == &quot;shanghai-IDC&quot;: SH_RouteId = i[&apos;RouteId&apos;] SH_RouteIds.append(SH_RouteId) return SH_RouteIdsdef BJ_DescribeCcnRoutesRequest(Vpc_client): # 获取北京专线网关路由ID req = models.DescribeCcnRoutesRequest() req.CcnId = &apos;ccn-a187ua1z&apos; resp = Vpc_client.DescribeCcnRoutes(req) result = json.loads(resp.to_json_string()) RouteSet = result[&apos;RouteSet&apos;] BJ_RouteIds = [] for i in RouteSet: if i[&apos;InstanceName&apos;] == &quot;beijing-IDC&quot;: BJ_RouteId = i[&apos;RouteId&apos;] BJ_RouteIds.append(BJ_RouteId) return BJ_RouteIdsdef SH_EnableCcnRoutes(Vpc_client,SH_id): # 开启上海路由 req = models.EnableCcnRoutesRequest() req.CcnId = &apos;ccn-a187ua1z&apos; req.RouteIds = [&apos;%s&apos; %SH_id] print(&apos;开启上海路由 %s&apos; % SH_id) resp = Vpc_client.EnableCcnRoutes(req) # print(resp.to_json_string())def BJ_EnableCcnRoutes(Vpc_client,BJ_id): # 开启北京路由 req = models.EnableCcnRoutesRequest() req.CcnId = &apos;ccn-a187ua1z&apos; req.RouteIds = [&apos;%s&apos; %BJ_id] print(&apos;开启北京路由 %s&apos; % BJ_id) resp = Vpc_client.EnableCcnRoutes(req)def SH_DisableCcnRoutes(Vpc_client,SH_id): req = models.DisableCcnRoutesRequest() req.CcnId = &apos;ccn-a187ua1z&apos; req.RouteIds = [&apos;%s&apos; %SH_id] print(&apos;关闭上海路由 %s&apos; % SH_id) resp = Vpc_client.DisableCcnRoutes(req)def BJ_DisableCcnRoutes(Vpc_client,BJ_id): req = models.DisableCcnRoutesRequest() req.CcnId = &apos;ccn-a187ua1z&apos; req.RouteIds = [&quot;%s&quot; %BJ_id] print(&apos;关闭北京路由 %s&apos; % BJ_id) resp = Vpc_client.DisableCcnRoutes(req)if __name__ == &quot;__main__&quot;: id = &apos;AKID*******sTzK&apos; key = &apos;KUWT*******M&apos; Vpc_client = Auth_vpc(id, key) SH_RouteIds = SH_DescribeCcnRoutesRequest(Vpc_client) BJ_RouteIds = BJ_DescribeCcnRoutesRequest(Vpc_client) # 这里可使用ping返回结果 oper = &quot;normal&quot; if oper == &quot;normal&quot;: for BJ_id in BJ_RouteIds: BJ_DisableCcnRoutes(Vpc_client, BJ_id) for SH_id in SH_RouteIds: SH_EnableCcnRoutes(Vpc_client, SH_id) # SH_DisableCcnRoutes(Vpc_client, SH_id) else: for SH_id in SH_RouteIds: SH_DisableCcnRoutes(Vpc_client, SH_id) for BJ_id in BJ_RouteIds: BJ_EnableCcnRoutes(Vpc_client, BJ_id) 2、测试效果模拟异常时切换切换完毕时中间大概中断30秒左右。 最佳方案 如果网络不重叠，2个VPC之间使用对等连接、使用2个云联网然后分别加入对应VPC和IDC的专线，这样从IDC到云上可实现双活。例如IDC-上海中断，可通过IDC-北京再通过对等连接到上海。]]></content>
      <categories>
        <category>Case</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>腾讯云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯云api调用]]></title>
    <url>%2F2019%2F05%2F06%2F%E8%85%BE%E8%AE%AF%E4%BA%91api%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[背景随着业务上云，云资源也逐步增多。需要批量操作的如果还是通过控制台人肉运维操作不仅效率低下而且容易出错，并且不能做到定时操作。 腾讯云api介绍目前腾讯云产品很多api已有封装好的SDK(API 3.0)，这为调用云产品的api提供了很大的方便和简洁，大大提高了开发和运维效率。但是有的云产品还没有配套的SDK，对于这种云产品只能通过计算签名然后拼接字符串去调用。 1234# 腾讯SDK explorer，封装好的SDK以及控制台在线调用。https://console.cloud.tencent.com/api/explorer# 安装腾讯云SDK，pythonpip install tencentcloud-sdk-python API概览 123这里提供了调用api的Action，定义操作不同的Action有不同的必要输入参数https://cloud.tencent.com/document/api/302/4031 请求结构包含：服务地址、通信协议、请求方法、请求参数和字符编码组成。 服务地址 12API支持就近地域接入，例如上海地域的cvm接口域名地址：cvm.ap-shanghai.tencentcloudapi.com,如果对调用无时延要求，也可不加地址：cvm.tencentcloudapi.comhttps://cloud.tencent.com/document/api/213/15691 通信协议 1腾讯云 API 的大部分接口都通过 HTTPS 进行通信，为您提供高安全性的通信通道。 请求方法 1腾讯云 API 同时支持POST和GET请求 请求参数 1腾讯云 API 的每个请求都需要指定两类参数：公共请求参数以及接口请求参数。 字符编码 1腾讯云 API 的请求及返回结果均使用 UTF-8 字符集进行编码 公共请求参数 12345678910# 必选参数Action: 具体操作的指令接口名称Region: 地域参数，用来标识希望操作哪个地域的实例。Timestamp: 当前 UNIX 时间戳，可记录发起 API 请求的时间。Nonce: 用户可自定义随机正整数，与 Timestamp 联合起来， 用于防止重放攻击。SecretId: 在 云API密钥 上申请的标识身份的 SecretId，一个 SecretId 对应唯一的 SecretKey , 而 SecretKey 会用来生成请求签名 Signature。Signature: 请求签名，用来验证此次请求的合法性，需要用户根据实际的输入参数计算得出。# 可选参数SignatureMethod: 签名方式，目前支持 HmacSHA256 和 HmacSHA1。只有指定此参数为 HmacSHA256 时，才使用 HmacSHA256 算法验证签名，其他情况均使用 HmacSHA1 验证签名。Token: 临时证书所用的 Token，需要结合临时密钥一起使用。长期密钥不需要 Token。 接口请求参数 1接口请求参数与具体的接口有关，不同的接口支持的接口请求参数也不一样。接口请求参数的首字母均为小写，以此区分于公共请求参数。 最终请求形式 123456腾讯云 API 接口请求 URL 的拼接规则为：https:// + 请求域名 + 请求路径 + ? + 最终请求参数串# 组成部分说明：# 请求域名：请求域名由接口所属的产品或所属产品的模块决定，不同产品或不同产品的模块的请求域名会有不同，如腾讯云 CVM 的查询实例列表（DescribeInstances）的请求域名为：cvm.api.qcloud.com。具体产品请求域名详见各接口说明。# 请求路径： 腾讯云 API 对应产品的请求路径，一般是一个产品对应一个固定路径（如腾讯云 CVM 请求路径固定为 /v2/index.php）。# 最终请求参数串： 接口的请求参数串包括公共请求参数和接口请求参数。 API请求签名1腾讯云 API 会对每个访问的请求进行身份验证，即每个请求都需要在公共请求参数中包含签名信息（Signature），以验证用户身份。签名信息由用户所执有的安全凭证生成. 生成签名串有了安全凭证 SecretId 和 SecretKey 后，就可以生成签名串了。生成签名串的详细过程如下： 对参数排序 1首先对所有请求参数按参数名做字典序升序排列。（所谓字典序升序排列，直观上就如同在字典中排列单词一样排序，按照字母表或数字表里递增顺序的排列次序，即先考虑第一个“字母”，在相同的情况下考虑第二个“字母”，依此类推。） 拼接请求字符串 123此步骤将生成请求字符串。将把上一步排序好的请求参数格式化成“参数名称”=“参数值”的形式，如对 Action 参数，其参数名称为&quot;Action&quot;，参数值为&quot;DescribeInstances&quot;，因此格式化后就为 Action=DescribeInstances。然后将格式化后的各个参数用&quot;&amp;&quot;拼接在一起。 拼接签名原文字符串 12345678此步骤将生成签名原文字符串。签名原文字符串的拼接规则为：请求方法 + 请求主机 +请求路径 + ? + 请求字符串# 参数构成说明：# 请求方法： 支持 POST 和 GET 方式，这里使用 GET 请求， 注意方法为全大写。# 请求主机：即主机域名，请求域名由接口所属的产品或所属产品的模块决定，不同产品或不同产品的模块的请求域名会有不同。如腾讯云 CVM 的查询实例列表（DescribeInstances）的请求域名为：cvm.api.qcloud.com，具体产品请求域名详见各接口说明。# 请求路径： 腾讯云 API 对应产品的请求路径，一般是一个产品对应一个固定路径，如腾讯云 CVM 请求路径固定为/v2/index.php。# 请求字符串： 即上一步生成的请求字符串。 生成签名串 12此步骤生成签名串计算签名的方法有两种：HmacSHA256 和 HmacSHA1 这里要根据您指定的签名算法（即 SignatureMethod 参数）生成签名串。当指定 SignatureMethod 为 HmacSHA256 时，需要使用 HmacSHA256 计算签名，其他情况请使用 HmacSHA1 计算签名。 签名串编码 1生成的签名串并不能直接作为请求参数，需要对其进行 URL 编码。 代码演示12345678910111213141516171819202122232425262728293031323334353637383940import base64,hashlib,hmacimport time,randomimport requests# api密钥id = &apos;AKIDENN***********4U6&apos;key = &apos;e6Rac********Jt1&apos;# 根据请求公共参数，自定义数据字典data = &#123; # Action 定义操作（在API概览里查看），本例是用的云解析 &apos;Action&apos;: &apos;RecordList&apos;, &apos;Region&apos;: &apos;ap-shanghai&apos;, &apos;Nonce&apos;: random.randint(10000, 99999), &apos;SecretId&apos;: id, &apos;Timestamp&apos;: int(time.time()), &apos;domain&apos;: &apos;key1024.cn&apos;,&#125;# 请求接口地址url = &apos;cns.api.qcloud.com/v2/index.php&apos;# 对所有请求参数排序signature_old = &apos;&apos;for i in sorted(data): signature_old = signature_old + i + &quot;=&quot; + str(data[i]) + &quot;&amp;&quot;signature_old = signature_old[:-1]# 拼接请求字符串query = &apos;GET&apos; + url + &apos;?&apos; + signature_old# 使用加密算法生产请求签名hmac_str = hmac.new(key.encode(&apos;utf-8&apos;), query.encode(&apos;utf-8&apos;), hashlib.sha1).digest()signature = base64.b64encode(hmac_str)# print(signature)# 将signature加入data字典中data[&apos;Signature&apos;] = signatureresp = requests.get(&apos;https://&apos; + url ,params=data)print(resp.content.decode(&apos;utf-8&apos;)) 反思总结结合任务计划定时执行某些操作，而且可使用无服务函数，摆脱传统服务器环境去执行脚本。]]></content>
      <categories>
        <category>Case</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django数据库ORM模型]]></title>
    <url>%2F2019%2F03%2F02%2FDjango%E6%95%B0%E6%8D%AE%E5%BA%93ORM%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[ORM模型介绍随着项目越来越大，采用原生SQL的方式在代码中会出现大量的SQL语句，会出现很多问题： SQL语句重复利用率不高，越复杂的SQL语句条件越多，代码越长。会出现很多相近的SQL语句。 很多SQL语句是在业务逻辑中拼出来的，如果有数据库需要更改，就要去修改这些逻辑。会很容易漏掉对某些SQL语句的修改。 写SQL时容易忽略web安全问题，给未来造成安全隐患。 ORM，全称Object Relational Mapping，对象关系映射。通过ORM我们可以通过类的方式去操作数据库，而不再写原生的SQL语句。通过==把表映射成类，把行作为实例，把字段作为属性==，ORM在执行对象操作的时候最终还是会把对应的操作转换为数据库原生语句。 使用ORM优点： 易用性：使用ORM做数据库的开发可有效的减少重复SQL语句的概率，写出来的模型也更加直观、清晰。 性能损耗小：ORM转换成底层数据库操作指令确实会有一些开销，但这种性能损耗很小(不足5%)只要不是对性能有严苛要求，综合考虑开发效率、代码的阅读性，带来的好处要远远大于性能损耗，而且项目越大作用越明显。 设计灵活：可以轻松的写出复杂的查询。 可移植性：Django封装了底层的数据库实现，支持多个关系数据库引擎。包括流行的mysql、postgresql和sqlite，可以非常轻松的切换数据库。 创建ORM模型ORM模型一般都是放在app的models.py文件中，每个app都可以拥有自己的模型，并且如果这个模型想要映射到数据库中，那么这个app必须要放在settings.py的INSTALLED_APP中进行安装。 1234567from django.db import modelsclass Book(models.Model): name = models.CharField(max_length=20,null=False) author = models.CharField(max_length=20,null=False) pub_time = models.DateTimeField(default=datetime.now) price = models.FloadField(default=0)# 这个模型继承自django.db.models.Model，如果这个模型想要映射到数据库中，就必须继承自这个类。这个模型以后映射到数据库中，表名是模型名称的小写形式为book。 表中有四个字段 name：保存的是书的名称，varchar类型，最长不能超过20个字符，并且不能为空。 author：作者名字类型，varchar类型，长度不超过20个字符。 pub_time:出版时间，数据类型是datetime类型，默认是保存这本书时间。 price：书本价格，浮点类型。 在django中如果一个模型没有定义主键，那么将会自动生成一个自动增长的int类型的主键，并且这个主键的名字就叫做id。 映射模型到数据库：将ORM模型映射到数据库中，总结起来就是以下几步： 1.在settings.py中，配置好DATABASES，做好数据库相关的配置。 2.在app中的models.py中定义好模型，这个模型必须继承自django.db.models。 3.将这个app添加到setting.py的INSTALLED_APP中 4.在命令行终端，进入到项目所在的路径，然后执行命令python manage.py makemigrations来生成迁移脚本文件。 5.再执行命令python manage.py migrate来迁移脚本文件映射到数据库中。 如果要将一个普通的类变成一个可以映射到数据库中的ORM模型，那么必须要将父类设置为models.Model或者其他的子类。 123456789101112131415161718192021222324252627282930313233343536373839# settings.pyDATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, &apos;NAME&apos;:&apos;mysite&apos;, &apos;USER&apos;:&apos;root&apos;, &apos;PASSWORD&apos;:&apos;****&apos;, &apos;HOST&apos;:&apos;43.x.x.x&apos;, &apos;PORT&apos;:&apos;3306&apos;, &#125;&#125;INSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;front&apos;, &apos;book&apos;,]# models.pyfrom django.db import modelsclass Book(models.Model): # id,int类型，自增长的；不定义默认也是有的。 id = models.AutoField(primary_key=True) name = models.CharField(max_length=100,null=False) author = models.CharField(max_length=100,null=False) price = models.FloatField(null=False,default=0) def __str__(self): # &lt;Book:(name,author,price)&gt; return &quot;&lt;Book:(&#123;name&#125;,&#123;author&#125;,&#123;price&#125;)&gt;&quot;.format(name=self.name,author=self.author,price=self.price)class pulisher(models.Model): name = models.CharField(max_length=100,null=False) adderss = models.CharField(max_length=100,null=False) OMR对数据的基本操作添加数据123# 使用ORM模型创建一个对象，然后再调用这个ORM模型的save方法就可以保存了。book = Book(name=&apos;随记&apos;,author=&apos;哈哈&apos;,price=&apos;179&apos;)book.save() 查找数据123456# 所有的查找工作都是使用模型上的objects属性来完成的。1、根据主键进行查找，也可以使用objects.get方法。然后传递pk=xx的方式进行查找。book = Book.objects.get(pk=2)print(book)2、根据其他字段进行查找，objects.filter方法查找，返回一个类似列表的数据，可使用first来获取第一值。books = Book.objects.filter(name=&apos;西游记&apos;).first() 删除数据123# 首先查找到对应数据模型，然后再执行这个模型的delete，即可删除。book = Book.objects.get(pk=4)book.delete() 修改数据1234# 首先查找到对应的数据模型，然后修改这个模型上的属性的值，再执行save方法即可修改完成。book = Book.objects.get(pk=5)book.author = &apos;猪猪憨&apos;book.save()]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 外键和表关系]]></title>
    <url>%2F2019%2F03%2F02%2FDjango-%E5%A4%96%E9%94%AE%E5%92%8C%E8%A1%A8%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[外键mysql中常见2种引擎，一种是InnoDB，另外一个是MyIsam。InnoDB引擎是支持外键约束的，外键的存在使得ORM框架在处理表关系的时候异常的强大。 类定义为class Foreignkey(to,on_delete,**options) 第一个参数是引用的哪个模型 第二个参数是在使用外键引用的模型数据被删除了，这个字段如何处理。比如有CASCADE（默认的选项，级联删除）、SET_NULL（置空模式，删除的时候，外键字段被设置为空）等。 123456789101112# models.py# User模型在user这个app中calss User(models.Model): username = models.CharField(max_length=20) password = models.CahrField(max_length=200)# Article模型在article这个app中class Article(model.Molde): title = models.CharField(max_length=100) content = models.TextField() author = models.ForeignKey(&quot;user.User&quot;,on_delete=models.CASCADE) # category = models.ForeignKey(&quot;User&quot;,on_delete=models.CASCADE,null=True) # 如果外键是在其他app上，写法是：app.模型名 如果模型的外键引用的是本身自己这个模型，那么to参数可以为self，或者是这个模型的名字。在论坛开发中，定义模型就需要使用外键来引用自身。 12345class Comment(models.Model): content = models.TextField() origin_comment = models.ForeignKey(&apos;self&apos;,on_delete=models.CASCADE,null=True) # 或者 # models.ForeignKey(&apos;Comment&apos;,on_delete=models.CASCADE,null=True) 外键删除操作如果一个模型使用了外键，那么在对方那个模型被删除后，该进行什么样的操作。可以通过on_delete来指定，可以指定的类型如下： 12345678910111213# urls.pyfrom django.urls import pathfrom book import viewsurlpatterns = [ # path(&apos;&apos;,views.index), path(&apos;delete/&apos;,views.delete_view)]# views.pydef delete_view(request): category = Category.objects.get(pk=1) category.delete() return HttpResponse(&quot;delete success!&quot;) CASCADE：级联操作。如果外键对应的那条数据被删除了，那么这条数据也会被删除。 PROTECT：受保护。即只要这条数据引用了外键的那条数据，那么就不能删除外键的那条数据。 SET_NULL：设置为空。如果外键的那条数据被删除了，那么在本条数据上就将这个字段设置为空。如果设置这个选项，前提是要指定这个字段可以为空。 SET_DEFAULT：设置默认值。如果外键的那条数据被删除了，那么本条数据上就将这个字段设置为默认值。如果设置这个选项，前提是要指定这个字段一个默认值。 SET()：如果外键的那条数据被删除了。那么将会获取SET函数中的值来作为这个外键的值。SET函数可以接收一个可以调用的对象（比如函数或者方法），如果是可以调用的对象，那么会将这个对象调用后的结果作为值返回回去。 DO_NOTHING：不采取任何行为。一切全看数据库级别的约束。 表关系表之间的关系都是通过外键来进行关联的，包括三种关系：一对一、一对多(多对一)、多对多等。 一对多 应用场景：比如文章和作者之间的关系，一个文章只能由一个作者编写，但是一个作者可以写多篇文章。这种就是典型的一对多关系。 实现方式：一对多或者多对一，都是通过ForeignKey来实现的。 1234567class User(models.Model): username = models.CharField(max_length=20) password = models.CharField(max_length=100) class Article(models.Model): title = models.CahrField(max_length=100) content = models.TextField() author = models.ForeignKey(&quot;User&quot;,on_delete=models.CASCADE) 那么以后在给Article对象指定author就可以使用以下代码来完成 123456article = Article(title=&apos;abc&apos;,content=&apos;123&apos;)author = User(username=&apos;hahawa&apos;,password=&apos;123.com&apos;)# 要先保存到数据库中author.save()article.author = authorarticle.save() 123456789101112131415161718def one_to_many_view(request): # 1.一对多 # article = Article(title=&apos;钢铁是怎样炼成的&apos;,content=&apos;just do it&apos;) # category = Category.objects.first() # author = frontUser.objects.first() # article.category = category # article.author= author # article.save() # 2.获取某个分类下所有稳赚 # category = Category.objects.first() # articles = category.article_set.all() # for i in articles: # print(i) # 3.添加数据，bulk category = Category.objects.first() article = Article(title=&apos;111&apos;,content=&apos;222&apos;) article.author = frontUser.objects.first() category.article_set.add(article,bulk=False) 一对一 应用场景：比如一个用户表和一个用户信息表。如果把所有信息都存放到一张表中可能会影响查询效率，因此可以把用户的一些不常用的信息存放到另外一张表中我们叫做UserExtension。但是用户表User和用户信息表UserExtension就是典型的一对一了。 实现方式：Django为一对一提供了一个专门的Field叫做OneToOneField来实现一对一操作。 1234567class User(models.Model): username = models.CharField(max_length=20) password = models.CharField(max_length=100)class UserExtension(models.Model): birthday = models.DateTimeField(null=True) school = models.CharField(blank=True,max_length=50) user = models.OneToOneField(&quot;User&quot;, on_delete=models.CASCADE) 在UserExtension模型上增加了一个一对一的关系映射。其实底层是在UserExtension这个表上增加了一个user_id，来和user表进行关联，并且这个外键数据在表中必须是唯一的，来保证一对一。 多对多 应用场景：比如文章和标签的关系。一篇文章可以有多个标签，一个标签可以被多个文章所引用。因此标签和文章的关系是典型的多对多的关系。 实现方式：Django为这种多对多的实现提供了专门的Field。叫做ManyToManyField。 123456class Article(models.Model): title = models.CharField(max_length=100) content = models.TextField() tags = models.ManyToManyField(&quot;Tag&quot;,related_name=&quot;articles&quot;)class Tag(models.Model): name = models.CharField(max_length=50) 在数据库层面，实际上Django是为这种多对多的关系建立了一个中间表。这个中间表分别定义了两个外键，引用到article和tag两张表的主键。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django ORM模型迁移]]></title>
    <url>%2F2019%2F03%2F02%2FDjango%E8%BF%87%E6%BB%A4%E5%99%A8%E3%80%81%E6%A8%A1%E6%9D%BF%E7%BB%A7%E6%89%BF%E3%80%81%E5%8A%A0%E8%BD%BD%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[迁移命令12python manage.py makemigrationspython manage.py migrate makemigrations：将模型生成迁移脚本。模型所在的app，必须放在settings.py中的INSTALLED_APPS中。 app_label：后面可以跟一个或者多个app，那么就只会针对这几个app生成迁移脚本。如果没有任何的app_label，那么会检查INSTALLED_APPS中所有的app下的模型，针对每一个app都生成响应的迁移脚本。 name：给这个迁移脚本指定一个名字。 empty：生成一个空的迁移脚本。如果你想写自己的迁移脚本，可以使用这个命令来实现一个空的文件，然后自己再在文件中写迁移脚本。 migrate：将新生成的迁移脚本。映射到数据库中。创建新的表或者修改表的结构。以下一些常用的选项： app_label：将某个app下的迁移脚本映射到数据库中。如果没有指定，那么会将所有在INSTALLED_APPS中的app下的模型都映射到数据库中。 app_label migrationname：将某个app下指定名字的migration文件映射到数据库中。 –fake：可以将指定的迁移脚本名字添加到数据库中。但是并不会把迁移脚本转换为SQL语句，修改数据库中的表。 –fake-initial：将第一次生成的迁移文件版本号记录在数据库中。但并不会真正的执行迁移脚本。 showmigrations：查看某个app下的迁移文件。如果后面没有app，那么将查看INSTALLED_APPS中所有的迁移文件。 sqlmigrate：查看某个迁移文件在映射到数据库中的时候，转换的SQL语句。 migrations中的迁移版本和数据库中的迁移版本对不上 1、找到哪里不一致，然后使用python manage.py –fake [版本名字]，将这个版本标记为已经映射。 2、删除指定app下migrations和数据库表django_migrations中和这个app相关的版本号，然后将模型中的字段和数据库中的字段保持一致，再使用命令python manage.py makemigrations重新生成一个初始化的迁移脚本，之后再使用命令python manage.py makemigrations –fake-initial来将这个初始化的迁移脚本标记为已经映射。以后再修改就没有问题了。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python安装虚拟环境]]></title>
    <url>%2F2019%2F03%2F02%2Fpython%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[安装virtualenvvirtualenv是用来创建虚拟环境的软件工具，可通过pip来安装 1pip install virtualenv 创建虚拟环境12virtualenv xxx(虚拟环境的名字)#要注意，这个创建虚拟环境后会在当前路径下创建虚拟环境名的文件夹，故需要先进入指定文件夹再创建虚拟环境。 进入环境进入虚拟环境在不同的操作系统中有不同的方式，分为windows和linux： windows虚拟环境：进入到虚拟环境的scripts文件中，然后执行activate。 linux虚拟环境：source /path/to/virtualenv/bin/activate 一旦你进入到这个虚拟环境中，安装、卸载包都是在这个虚拟环境中，不会影响到外面的环境。 退出虚拟环境命令 deactivate 创建虚拟环境时指定python解释器1virtualenv -p C:\python36\python.exe [virutalenv name] virenvwrapper让管理虚拟环境变得更加简单，不用去到目录下通过virtualenv来创建虚拟环境、以及激活。 安装virtualenvwrapper12pip install virtualenvwrapper #linuxpip install virtualenvwrapper-win #windows virtualenvwrapper基本使用 创建虚拟环境 12mkvirtualenv my_env#会在当前用户下创建一个env文件夹，然后将这个虚拟环境安装到这个目录下。 切换到某个虚拟环境 1workon my_env 退出当前虚拟环境 1deactivate 删除某个虚拟环境 1rmvirtualenv my_env 列出所有虚拟环境 1lsvirtualenv 进入到虚拟环境所在的目录 1cdvirtualenv 修改mkvirtualenv的默认路径在系统环境变量里添加参数 WORKON_HOME，将这个参数的值设置为你需要的路径。 创建虚拟环境时指定python版本使用mkvirtualenv时，指定–python的参数来指定具体的python路径： 1mkvirtualenv --python==D:\Python36\python.exe haha_env]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django基础概念]]></title>
    <url>%2F2019%2F03%2F02%2FDjango%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Django遵循MVC设计模式的框架，MVC是Model、View、Controller的三个单词的简写。分别代表模型、视图、控制器。 创建项目 通过命令行方式，首先进入到安装了Django的虚拟环境中 通过pycharm方式1234567# 创建项目1、命令行：django-admin startproject [项目名称]2、pycharm方式：文件-&gt;创建项目-&gt;选择django。然后指定项目所在的路径，以及python解释器。#运行项目python manage.py runserver可添加端口 python manage.py runserver 8088对公访问 python manage.py runserver 0.0.0.0 8088 项目结构介绍 manage.py：和项目交互都基于这个文件，python manage.py [子命令] settings.py：本项目的设置项，所有和项目相关的配置都是放在这个里面 urls.py：用来配置URL路由的，比如访问http://127.0.0.1/news/ 是访问新闻列表页，这些东西就需要在这个文件中完成。 wsgi.py：项目与WSGI协议兼容的web服务器入口，部署的时候需要用到的，一般情况下也是不需要修改的。 创建app1python manage.py startapp django_1 视图函数 视图函数的第一个参数必须是request,这个参数不能少 视图函数的返回值必须是django.http.response.HttpResponseBase的子类的对象。 URL传递参数 url映射 要去urls.py文件中寻找映射是因为在settings.py文件中配置了ROOT_URLCONF为urls.py。所有的django会去urls.py中寻找。 在urls.py中我们所有的映射都应该放在urlpatterns这个变量中。 所有的映射不是随便写的，而是使用path函数或者是re_path函数进行包装的。 url传参数 采用在url中使用变量的方式：在path的第一个参数中，使用&lt;参数名&gt;的方式传递参数，然后在视图函数中也要写一个参数。视图函数中的参数必须和url中的参数名称保持一致，不然找不到这个参数。 采用查询字符串的方式：在url中不需要单独的匹配查询字符串的部分，只需要在视图函数中使用request.GET.get(&#39;参数名称&#39;)的方式来获取。代码如下： 1234def author_detail(request): author_id = request.GET[&apos;id&apos;] text = &apos;作者id是：%s&apos; % author_id return HttpResponse(text) 因为查询字符串使用的是GET请求，所以我们通过request.GET来获取参数，并且因为GET是一个类似于字典的数据类型，所有获取值跟字典的方式都是一样的。 url命名 需要url命名的原因 因为url是经常变化的，写死可能会经常修改代码。给url取个名字以后使用url的时候就使用它的名字进行反转就可以了。 如何给一个url指定名称 在path函数中，传递一个name参数就可以指定。 1234567891011121314151617# urls.pyfrom django.urls import pathfrom . import viewsurlpatterns = [ path(&apos;&apos;,views.index,name=&apos;index&apos;), path(&apos;signin/&apos;,views.login,name=&apos;login&apos;)]# views.pyfrom django.http import HttpResponsefrom django.shortcuts import redirect,reversedef index(request): username = request.GET.get(&apos;username&apos;) if username: return HttpResponse(&apos;前台首页&apos;) else: return redirect(reverse(&apos;login&apos;)) 应用命名空间在多个app之间，有可能产生同名的url。这时为了避免反转url的时候产生混淆，可以使用应用命名空间来做区分。定义应用命名空间非常简单，只要在app的urls.py中定义一个叫做app_name的变量来指定这个应用的命名空间即可。 1234567891011121314151617181920# urls.pyfrom django.urls import pathfrom . import viewsapp_name = &apos;front&apos;urlpatterns = [ path(&apos;&apos;,views.index,name=&apos;index&apos;), path(&apos;signin/&apos;,views.login,name=&apos;login&apos;)]# views.pyfrom django.http import HttpResponsefrom django.shortcuts import redirect,reversedef index(request): username = request.GET.get(&apos;username&apos;) if username: return HttpResponse(&apos;前台首页&apos;) else: return redirect(reverse(&apos;front:login&apos;)) 应用命名空间和实例名空间 12345678# 一个app可以创建多个实例，可以使用多个url映射同一个app。在做反转的时候使用应用命名空间，那么就会发生混淆，为了避免这个问题就可以使用实例命名空间。在include函数中传递一个namespace变量即可。# urls.py from django.urls import path,includeurlpattterns = [ path(&apos;&apos;,include(&apos;front.urls&apos;)), path(&apos;cms1/&apos;,include(&apos;cms.urls&apos;,namespace=&apos;cms1&apos;)), path(&apos;cms2/&apos;,include(&apos;cms.urls&apos;,namespace=&apos;cms2&apos;))] url分层模块化多个app后主app的urls.py里的urlpatterns会写入过多路径，可通过在app里创建自身app对应的urls.py来方便路径转发。 1234567891011121314151617# 主urls.py,使用include函数包含子urls.pyfrom django.urls import path,includeurlpattterns = [ path(&apos;book/&apos;,include(&apos;book.urls&apos;)) # 以book开头的url都会转到book app下的urls.py]# book app的 urls.pyfrom django.urls import pathfrom . import viewsurlpattterns = [ path(&apos;&apos;,views.book), path(&apos;detail/&lt;book_id&gt;&apos;,views.book_detail), path(&apos;list/&apos;,views.book_list),] reverse函数补充1、如果反转url的时候，需要添加参数，那么可以传递kwargs参数到reverse函数中。2、如果想要添加查询字符串的参数，则必须手动的进行拼接。 123456789101112131415161718192021222324252627282930# views.pyfrom django.http import HttpResponsefrom django.shortcuts import reverse,redirectdef index(request): username = request.GET.get(&apos;username&apos;) if username: return HttpResponse(&quot;首页&quot;) else: # login_url = reverse(&apos;login&apos;) + &quot;?next=/&quot; # return redirect(login_url) detail_url = reverse(&apos;detail&apos;,kwargs=&#123;&apos;article_id&apos;:1,&apos;page&apos;:2&#125;) return redirect(detail_url) def login(request): return HttpResponse(&quot;登录页面&quot;)def article_detail(request,article_id): text = &apos;您的文章id是:%s&apos; % article_id return HttpResponse(text) # urls.pyfrom django.urls import pathfrom front import viewsurlpatterns = [ path(&apos;&apos;,views.index,name=&apos;index&apos;), path(&apos;login/&apos;,views.login,name=&apos;login&apos;), path(&apos;detail/&lt;article_id&gt;/&lt;page&gt;/&apos;,views.article_detail,name=&apos;detail&apos;)] 创建项目 创建app：python manage.py startapp cms 将创建的app放入统一路径下，例如放入apps文件夹下 app中编写views.py 123from django.shortcuts import renderdef login_view(request): return render(request,&apos;cms/login.html&apos;) app中编写urls.py 12345678from django.urls import pathfrom . import viewsapp_name = &apos;cms&apos;urlpatterns = [ path(&apos;login/&apos;,views.login_view,name=&apos;login&apos;)] 默认app中添加urls12345from django.urls import path,includeurlpatterns = [ path(&apos;&apos;, include(&quot;apps.news.urls&quot;)), path(&apos;cms/&apos;,include(&quot;apps.cms.urls&quot;)),]]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下通过bat脚本自动重启apache服务]]></title>
    <url>%2F2019%2F02%2F14%2Fwindows%E4%B8%8Bbat%E8%84%9A%E6%9C%AC%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AFApache%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[问题描述 客户使用的集成环境phpstudy，上面运行的是php+apahce+mysql，经常无故apache异常导致网站无法打开，需要重启apache服务恢复。由于不方便变更基础环境，故考虑使用bat脚本自动检测网站是否正常判断是否需要重启apache服务。 准备工作12341、由于借助curl命令测试网站是否正常，需提前安装配置好curl命令。https://curl.haxx.se/download.html#Win64下载对应的版本安装，并配置好环境变量。(在cmd中能使用curl命令)2、最好将apache注册为系统服务，这样方便使用net重启服务。否则就要指定目录去重启。（注册系统服务推荐使用nssm） 注册系统服务12345678910# 如果已是系统服务跳过此步骤，比如phpstudy上可注册为系统服务1、下载，解压安装官网：http://nssm.cc/download根据操作系统选择32位或64位nssm，在该目录启动命令行窗口，建议写入path环境变量2、服务注册（此处是之前注册logstash的截图）nssm install logstash接下来会弹出一个框，在path处选择启动logstash的start.bat点击Install service即可填写应用程序的对应路径、设置Service name。最后单击install service按钮，执行安装。 编写脚本12345678910111213141516::Auto restart apache@echo offset url=http://www.gc1999.comecho %url%for /f %%z in (&apos;curl -so /dev/null -w %%&#123;http_code&#125; %url%&apos;) do (set result=%%zecho %%z)if %result% NEQ 200 (::echo %date%%time% %url% 无法打开 错误代码 %result% &gt;&gt;C:\log\%date:~0,4%%date:~5,2%%date:~8,2%ERROR.lognet stop apache2anet start apache2a) else (::echo %date%%time% %url% 网页可以打开 代码 %result% &gt;&gt;C:\log\%date:~0,4%%date:~5,2%%date:~8,2%SUCCESS.logecho %date%%time% %url% 网页可以打开 代码 %result%) 设置任务计划定期执行自定义任务计划，重复执行检测]]></content>
      <categories>
        <category>system</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql经典问题]]></title>
    <url>%2F2018%2F09%2F30%2FMysql%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、Mysql的复制原理以及流程 1234基本原理流程，3个线程以及之间的关联主：binlog线程 — 记录下所有改变了数据库数据的语句，放进master上的binlog中；从：io线程 — 在使用start slave之后负责从master上拉取binlog内容，放进自己的relay log中；从：sql执行线程 — 执行relay log中的语句。 2、Mysql中的MyIsam与InnoDB的区别，至少5点 a、问5点不同 12InnoDB |支持事务|支持行级锁| 支持MVCC |支持外键| 不支持全文索引MyISAM |不支持事务| 支持表级锁| 不支持MVCC| 不支持外键| 支持全文索引 b、innodb引擎的4大特性 1插入缓冲（insert buffer）、二次写（double write）、自适应哈希索引（ahi）、预读（read ahead） c、2者select count (*) from table哪个更快，为什么 1MyISAM更快，因为MyISAM内部维护一个计数器，可以直接调取。 3、Mysql中varchar与vhar的区别，以及varchar(50)中的50代表的含义 12345678a、varchar与char的区别char是一种固定长度的类型，varchar则是一种可变长度的类型b、varchar(50)中50的含义最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存。因为order by col采用fixed_length计算col长度（memory引擎也一样）c、int(20)中20的含义是指显示字符的长度。但要加参数的，最大为255，比如它是记录行数的id。20表示最大显示宽度为20，但仍占4字节存储，存储范围不变。d、mysql为什么这么设计对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1)和int(20)存储和计算均一样； 4、问innodb的事务与日志的实现方式 123456789101112131415161718a、有多少种日志错误日志（error log）：记录出错信息，也记录一些警告信息或者正确的信息。查询日志（general query log）：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。慢查询日志（-log-slow-queries）：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。二进制日志（binary log）：记录对数据库执行更改的所有操作。中继日志（relay log）：也是二进制日志，用来给slave库恢复（同步数据）事务日志：b、事务的4中隔离级别隔离级别读未提交（RU）读已提交（RC）可重复读（RR）串行c、事务是如何通过日志来实现的，说的越深入越好事务日志是通过redo和InnoDB的存储引擎日志缓冲（InnoDB log buffer）来实现的。当开始一个事务的时候，会记录该事务的lsn（log sequence number）号；当事务执行时，会往InnoDB存储引擎的日志缓存里插入事务日志；当事务提交时，必须将存储引擎的日志缓冲写入磁盘（通过innodb_flush_log_at_trx_commit来控制），也就是写数据前，需要先写日志。这种方式称为“预写日志方式”。d、事务的四个原则原子性、一致性、隔离性、持久性 5、问Mysql binlog的几种日志录入格式以及区别 12345678910a、binlog的日志格式的种类和区别三种：statement、MiXED以及ROWb、适用场景，结合第一个问题，每一种日志格式在复制中的优劣Statement：每一条会修改数据的sql都会记录在binlog中。优点：不需要记录每一行的变化，减少binlog日志量，节约了IO提高性能。缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在salve得到和在master端执行时候相同的结果。另外mysql的复制，像一些特定函数功能，slave可与master上保持一致会有很多相关问题（sleep()函数，last_insert_id()，以及user-defined functions(udf)会出现问题）。Row：不记录sql语句上下相关信息，仅保存哪条记录被修改。优点：binlog中可以不记录执行的sql语句的上下相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节，而且不会出现某些特定情况下的存储过程。如function、以及trigger的调用和触发无法被正确复制的问题。缺点：所有的执行的语句当记录到日志中的时候，都将每行记录的修改来记录，这样可能会产生大量的日志内容，比如一条update语句，修改多条记录。则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都会发生改变，那么该表每一条都会记录到日志中。Mixedlevel：是以上两种level的混合使用，一般的语句修改使用statement格式保存binlog，如一些函数statement无法完成主从复制的操作，则采用row格式保存binlog，mysql会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 6、问Mysql数据库CPU飙升到500%的话怎么处理 1列出所有进程show processlist，观察所有进程，几秒钟都没有状态变化的（干掉）。查看超时日志或者错误日志（一般会是查询以及大批量的插入会导致cpu与i/o上涨；也可能是网络状态突然断了，导致一个请求服务器只接受到一半，比如where子句或分页子句没有发送。） 7、SQL优化 1234567891011121314151617a、explain出来的各种item的意义select_type表示查询中每个select子句的类型type表示Mysql在表中找到所需行的方式，又称为“访问类型”possible_keys指出Mysql能使用哪个索引在表中找到行，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用。key显示Mysql能使用哪个索引在表中找到行，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。ref表示上述表的连接匹配条件，即哪些列或常量被用于查找索引上的值。Extra包含不适合在其他列中显示但十分重要的额外信息b、profile的意义以及使用场景查询到SQL会执行多少时间，并看出CPU/Memory使用量，执行过程中Systemlock，table lock花多少时间等等。 8、备份计划，mysqldump以及xtrabackup的实现原理 1234567891011121314a、备份计划每天增量备份，每月全备等b、备份恢复时间时间和硬盘的速率有关，以下列举几个仅供参考：20G的2分钟（mysqldump）80G的30分钟（mysqldump）110G的30分钟（mysqldump）290G的3个小时（xtra）3T的4小时（xtra）逻辑导入时间一般是备份时间的5倍以上c、xtrabackup实现原理在InnoDB内部会维护一个redo日志文件，也可叫做事务日志文件。事务日志会存储每一个InnoDB表数据的记录修改，当InnoDB启动时InnoDB会检查数据文件和事务日志，并执行两个步骤：它应用（前滚）已经提交的事务日志到数据文件，并将修改过但没有提交的数据进行回滚操作。 9、mysqldump中备份出来的sql，如果想sql文件中一行只有一个insert … value()的话怎么处理？如果备份需要带上master的复制点信息怎么处理？ 12--skip-extended-insert备份时添加这个：mysqldump -uroot -p haha(要备份的库) --skip-extended-insert 10、500台db，在最快时间之内重启。 12使用puppet，批量管理。dsh是专为在远程上运行shell命令涉及的，可以简化对大量计算机的操作。 11、InnoDB的读写参数优化 123456789101112a、读取参数global buffer pool 以及local buffer；b、写入参数innodb_flush_log_at_trx_commitinnodb_buffer_pool_sizec、与IO相关的参数innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_thread_concurrency = 0d、缓存参数以及缓存的适用场景query cache/query_cache_type 并不是所有表都适合使用query cache，造成query cache失效的原因主要是相应的table发生了变更。 12、你是如何监控你们的数据库的？你们的慢查询日志都是怎么查询的？ 1监控工具，例如zabbix，ngios，lepus等 13、你是否做过主从一致性校验，如果有怎么做的？如果没有，你打算怎么做？ 1主从一致性校验有多种工具，例如checksum、mysqldiff、pt-table-checksum等。 14、你们数据库是否支持emoji表情，如果不支持如何操作？ 123456789mysql数据库默认utf8，只能存储3个字节，标准的emoji表情是4个字节。如果是utf8字符集就需要升级至utf8_mb4方可支持。mb4的意思是most bytes 4，专门为兼容四个字节的。mysql需大于5.5.3版本。步骤：a、修改字段的字符集：ALTER table mb_touchpay_record modify clientName varchar(100) character set utf8mb4 collate utf8mb4_unicode_ci b、表：ALTER table mb_touchpay_record charset=utf8mb4; c、库： set names utf8mb4修改后可看到字符集设置：SHOW VARIABLES WHERE Variable_name LIKE &apos;character\_set\_%&apos; OR Variable_name LIKE &apos;collation%&apos;;不需要重启d、如果以上步骤操作后还不能保存成功，连接数据库时设置字符集如下： 15、你是如何维护数据库的数据字典的？ 1一般是直接在生产库进行注释，然后利用工具导出excel方便流通。 16、你们是否有开发规范，如果有如何执行的？ 17、表中有大字段X（例如：text类型），且字段X不会经常更新，以读为主，请问 12345a、你是选择拆成子表，还是继续放一起b、写出你这样选择的理由拆带来的问题：连接消耗+存储拆分空间；不拆可能带来的问题：查询性能； 如果能容忍拆分带来的空间问题：拆的话最好和经常要查询的表的主键在物理结构上放置一起，顺序IO减少连接消耗。 如果能容忍不拆分带来的查询性能损失的话：上面的方案在某个极致条件下肯定会出现问题，嘛呢不拆就是最好的选择。 18、Mysql中InnoDB引擎的行锁是通过加载什么上完成（或实现）的？为什么是这样子的？ 123InnoDB是基于索引来完成行锁的。例如：select * from tab_with_index where id = 1 for update;for update可以根据条件来完成行锁锁定，并且id是有索引键的列，如果id不是索引键那么InnoDB将完成表锁。并发将无从谈起。 19、如何从mysqldump产生的全库备份中只恢复某一个库、某一张表？ 123456全库备份：mysqldump -uroot -p --single-transaction -A --master-data=2 &gt; dump.sql只还原erp库的内容：mysql -uroot -p MANAGER erp --one-database &lt; dump.sql可以看出这里主要用到的参数是--one-database简写-o的参数，极大方便了我们的恢复灵活性。利用正则表达式来进行快速抽取：从全库备份中抽取出t表的表结构 sed -e&apos;/./&#123;H;$!d;&#125;&apos; -e &apos;x;/CREATE TABLE `t`/!d;q&apos; dump.sql从全库备份中抽取出t表的内容 grep&apos;INSERT INTO `t`&apos; dump.sql 20、开放性问题，一个6亿的表a，一个3亿的表b，通过外间tid关联。你是如何最快的查询出满足条件的第50000到第50200中的这200条数据记录。 1234a、如果A表TID是自增长，并且是连续的，B表的ID为索引select * from a,b where a.tid = b.tid and a.tid &gt; 50000 limit 200;b、如果A表的TID不是连续的，那么就需要使用覆盖索引，TID要么是主健，要么是辅助索引，B表ID也需要有索引。select * from b ,(select tid from a limit 50000,200) a where b.id = a.tid;]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群搭建]]></title>
    <url>%2F2018%2F09%2F03%2FKafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[zookeeper集群搭建kafka集群是把状态保存在zookeeper中的，首先要搭建zookeeper集群。 1、安装jdk12wget http://anchnet-script.oss-cn-shanghai.aliyuncs.com/oracle/jdk-8u171-linux-x64.rpmyum localinstall jdk-8u171-linux-x64.rpm -y 2、下载kafka安装包12wget http://anchnet-script.oss-cn-shanghai.aliyuncs.com/kafka/kafka_2.12-1.1.0.tgz官网下载链接：http://kafka.apache.org/downloads 解压kafka tar -zxvf kafka_2.12-1.1.0.tgzmv kafka_2.12-1.1.0 kafka 3、配置zk集群修改zookeeper.properties文件直接使用kafka自带的zookeeper建立zk集群 12cd /data/kafkavim conf/zookeeper.properties 123456789101112#tickTime：这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。#initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒#syncLimit：这个配置项标识 Leader 与Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是5*2000=10秒#dataDir：快照日志的存储路径#dataLogDir：需手动创建事物日志的存储路径，如果不配置这个那么事物日志会默认存储到dataDir制定的目录，这样会严重影响zk的性能，当zk吞吐量较大的时候，产生的事物日志、快照日志太多#clientPort：这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 创建myid文件进入dataDir目录，将三台服务器上的myid文件分别写入1、2、3。myid是zk集群用来发现彼此的标识，必须创建，且不能相同。 echo “1” &gt; /data/kafka/zk/myidecho “2” &gt; /data/kafka/zk/myidecho “3” &gt; /data/kafka/zk/myid 注意项zookeeper不会主动的清除旧的快照和日志文件，需要定期清理。 1234567891011#!/bin/bash #snapshot file dir dataDir=/data/kafka/zk/version-2#tran log dir dataLogDir=/data/kafka/log/zk/version-2#Leave 66 files count=66 count=$[$count+1] ls -t $dataLogDir/log.* | tail -n +$count | xargs rm -f ls -t $dataDir/snapshot.* | tail -n +$count | xargs rm -f #以上这个脚本定义了删除对应两个目录中的文件，保留最新的66个文件，可以将他写到crontab中，设置为每天凌晨2点执行一次就可以了。 4、启动zk服务进入kafka目录，执行zookeeper命令 12cd /data/kafkanohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp; 没有报错，而且jps查看有zk进程就说明启动成功了。 Kafka集群搭建1、修改server.properties配置文件 vim conf/server.properties 部分参数含义： 12345先每台设置host，listeners里要设置，否则后面消费消息会报错。 broker.id 每台都不能相同num.network.threads 设置为cpu核数num.partitions 分区数设置视情况而定，上面有讲分区数设置default.replication.factor kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务 2、启动kafka集群1nohup ./bin/kafka-server-start.sh config/server.properties &gt; logs/kafka.log 2&gt;&amp;1 &amp; 执行jps检查 3、创建topic验证1234./bin/kafka-topics.sh --create --zookeeper kafka1:2181,kafka2:2181,kafka3:2181 --replication-factor 2 --partitions 1 --topic test1--replication-factor 2 #复制两份--partitions 1 #创建1个分区--topic #主题为test1 4、创建生产者和消费者12345#模拟客户端去发送消息，生产者./bin/kafka-console-producer.sh --broker-list kafka1:9092,kafka2:9092,kafka3:9092 --topic test1#模拟客户端去接受消息，消费者./bin/kafka-console-consumer.sh --zookeeper kafka1:2181,kafka2:2181,kafka3:2181 --from-beginning --topic test1#然后在生产者处输入任意内容，在消费端查看内容。 5、其他命令1234567891011121314./bin/kafka-topics.sh --list --zookeeper xxxx：2181#显示创建的所有topic./bin/kafka-topics.sh --describe --zookeeper xxxx:2181 --topic test1#Topic:ssports PartitionCount:1 ReplicationFactor:2 Configs:# Topic: test1 Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1#分区为为1 复制因子为2 他的 test1的分区为0 #Replicas: 0,1 复制的为0，1``` ###### 6、删除topic&gt; 修改配置文件server.properties添加如下配置： &gt; delete.topic.enable=true &gt; 配置完重启kafka、zookeeper。 如果不想修改配置文件可删除topc及相关数据目录 #删除kafka topic./bin/kafka-topics.sh –delete –zookeeper xxxx:2181,xxxx:2181 –topic test1 #删除kafka相关数据目录rm -rf /data/kafka/log/kafka/test* #删除zookeeper相关路径rm -rf /data/kafka/zk/test*rm -rf /data/kafka/log/zk/test*```]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix简单故障自愈]]></title>
    <url>%2F2018%2F08%2F03%2FZabbix%E7%AE%80%E5%8D%95%E6%95%85%E9%9A%9C%E8%87%AA%E6%84%88%2F</url>
    <content type="text"><![CDATA[需求背景监控指标超过阈值后发出告警，大部分告警可通过触发脚本去自动处理，可一定程度上减低人肉运维的辛苦繁琐。例如某个服务down了、硬盘空间不足等等。 zabbix配置本例是检查某项服务（主要是端口）down了，在一定时间内未响应或者恢复，直接重启此服务。 登录上zabbix后台 配置监控项目–&gt;触发器(并关联监控项)–&gt;图形(可查看状态) 在对应的主机里创建监控项目 创建触发器 添加图形 创建动作 编辑操作操作细节：如果执行的命令需要sudo权限，一定要在前面加sudo 服务器上配置 visudo，添加zabbix相应的权限 注释visudo中的Defaults requiretty，因为zabbix执行这命令不是通过tty终端登录执行，所以关闭次设置。 编辑配置文件 12编辑/etc/zabbix/zabbix_agentd.conf开启EnableRemoteCommands=1，设置完需要重启zabbix_agent服务。 测试]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix发送邮件告警(含告警图片)]]></title>
    <url>%2F2018%2F08%2F03%2FZabbix%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E5%91%8A%E8%AD%A6-%E5%90%AB%E5%91%8A%E8%AD%A6%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[需求背景使用邮件发送zabbix告警，只能单纯的收到告警内容没有发生告警时的监控图片。使用python根据告警产生的itemid从数据库中获取到graphid然后下载图片，最后和邮件内容一同发出。 安装邮件发送工具 mailx 1234567yum -y install mailx# 填写发送邮件的账户信息,需要先开通邮箱账户的POP3/SMTP服务vim /etc/mail.rcset bsdcompatset from=cloud-ome@****.com smtp=smtp.xxx.comset smtp-auth-user=**** smtp-auth-password=****set smtp-auth=login postfix 1234yum install -y postfix输入“alternatives --display mta”查看当前MTA如显示当前MTA为sendmail，则输入“/usr/sbin/alternatives --set mta /usr/sbin/sendmail.postfix”修改为postfix编辑vim /etc/postfix/main.cf zabbix配置相关具体配置不再做过多说明，可自行网上查询 python脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#!/usr/bin/python#coding=utf-8from email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartfrom email.mime.image import MIMEImageimport MySQLdb,smtplib,sys,os,time,reuser=&apos;****&apos;#zabbix用户名password=&apos;****&apos;#zabbix密码url=&apos;http://172.81.x.x/zabbix/&apos;#zabbix首页period=&apos;900&apos;chart2_url=&apos;http://172.81.x.x/zabbix/chart2.php&apos;#zabbix获取图片url http://192.x.x.x/chart2.phpmysql_host=&apos;localhost&apos;mysql_user=&apos;zabbix&apos;mysql_pass=&apos;****&apos;mysql_db=&apos;zabbix&apos;#zabbix数据库相关信息graph_path=&apos;/data/zabbix/img/&apos;#图片保存路径def get_itemid(): #获取itemid pattern = re.compile(&apos;ITEM.*?:(\d+)&apos;,re.S) a = re.findall(pattern,sys.argv[3]) #a=re.findall(r&quot;ITEM ID: \d+&quot;,sys.argv[3]) i=str(a) itemid=re.findall(r&quot;\d+&quot;,i) return str(itemid).lstrip(&apos;[\&apos;&apos;).rstrip(&apos;\&apos;]&apos;)def get_graphid(itemid): #获取graphid conn =MySQLdb.connect(host=mysql_host,user=mysql_user,passwd=mysql_pass,db=mysql_db,connect_timeout=10) cur=conn.cursor() cur.execute(&quot;SELECT graphid FROM `graphs_items` WHERE `itemid`=%s;&quot; %itemid) result=cur.fetchone() cur.close() conn.close() graphid=re.findall(r&apos;\d+&apos;,str(result)) return str(graphid).lstrip(&apos;[\&apos;&apos;).rstrip(&apos;\&apos;]&apos;)def get_graph(): #拉取图片 time_tag=time.strftime(&quot;%Y%m%d%H%M%S&quot;, time.localtime()) os.system(&apos;curl -L -c /usr/lib/zabbix/alertscripts/cookie.txt --user-agent Mozilla/4.0 -d &quot;reauest=&amp;name=%s&amp;password=%s&amp;autologin=1&amp;enter=Sign+in&quot; %s&apos; %(user,password,url)) os.system(&apos;curl -c /usr/lib/zabbix/alertscripts/cookie.txt -b /usr/lib/zabbix/alertscripts/cookie.txt --user-agent Mozilla/4.0 -F &quot;graphid=%s&quot; -F &quot;period=%s&quot; -F &quot;width=900&quot; %s &gt; /data/zabbix/img/zabbix_%s_%s.png&apos; %(graphid,period,chart2_url,graphid,time_tag)) graph_name= &apos;/data/zabbix/img/&apos; + &apos;zabbix_&apos; + graphid + &apos;_&apos; + time_tag +&apos;.png&apos; return graph_namedef text_transfe_html(text): #将message转换为html d=text.splitlines() html_text=&apos;&apos; for i in d: i=&apos;&apos; + i + &apos;&lt;/br&gt;&apos; html_text+=i + &apos;\n&apos; return html_textdef send_mail(to_email,subject): #发送邮件 graph_name=get_graph() html_text=text_transfe_html(sys.argv[3]) smtp_host = &apos;smtp.xxxx&apos; from_email = &apos;liykxxxx&apos; #邮箱账户 passwd = &apos;xxxx&apos; #邮箱密码 msg=MIMEMultipart(&apos;related&apos;) fp=open(graph_name,&apos;rb&apos;) image=MIMEImage(fp.read()) fp.close() image.add_header(&apos;Content-ID&apos;,&apos;&lt;image1&gt;&apos;) msg.attach(image) html=&quot;&quot;&quot; &lt;html&gt; &lt;body&gt; &quot;&quot;&quot; html+=html_text html+=&apos;&lt;img src=&quot;cid:image1&quot;&gt;&lt;/br&gt;&apos; html+=&quot;&quot;&quot; &lt;/body&gt; &lt;/html&gt; &quot;&quot;&quot; html=MIMEText(html,&apos;html&apos;,&apos;utf-8&apos;) msg.attach(html) msg[&apos;Subject&apos;] = subject msg[&apos;From&apos;] = from_email smtp_server=smtplib.SMTP_SSL() smtp_server.connect(smtp_host,&apos;465&apos;) smtp_server.login(from_email,passwd) smtp_server.sendmail(from_email,to_email,msg.as_string()) smtp_server.quit()if __name__ == &apos;__main__&apos;: to=sys.argv[1] with open(&apos;/tmp/zabbix.log&apos;,&apos;w&apos;) as f: f.write(to) subject=sys.argv[2] itemid=get_itemid() graphid=get_graphid(itemid) for i in to.strip().split(&apos;,&apos;): send_mail(i,subject)]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix-sender实现秒级监控]]></title>
    <url>%2F2018%2F08%2F03%2FZabbix-sender%E5%AE%9E%E7%8E%B0%E7%A7%92%E7%BA%A7%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[背景客户需要提供秒级监控，使用agent发送获取是server取数据都会大大消耗性能，影响server和client的正常运行。zabbix-sender可解决这一问题，既能实现秒级监控也只是针对某一监控项单独定义，不会对整体造成太多的性能消耗。 安装zabbix-sender安装zabbix-sender，主动发数据实现秒级监控安装需与zabbix-server版本一致，例如zabbix-server是3.4，zabbix-sender也是3.4.x 12rpm -ivh https://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-2.el7.noarch.rpmyum install zabbix-sender -y 1234567其核心为利用zabbix_sender来发送数据zabbix_sender -z zabbix服务器的地址 -s 创建主机的主机名称 -k 键值名称 -o 上报的数据 -p zabbix 服务器的端口 编写脚本12345678910111213141516171819202122232425262728293031323334353637#!/bin/bash# ping可执行路径PINGCMD=/usr/bin/ping# zabbix_sender可执行文件路径SENDCMD=/usr/bin/zabbix_sender# ping测主机ip地址CHECKHOST=baidu.com# zabbix服务器ip地址ZABBIXSERVER=43.254.x.xx# zabbix服务器监听端口ZABBIXPORT=10051# zabbix添加这条监控主机名LOCALHOST=checkping_monitor# ping包的数量PAG_NUM=1# 添加监控项的键值ZAX_KEY=ping_response# 获取ping响应时间check_ping() &#123; $PINGCMD -c $PAG_NUM $CHECKHOST &gt;/dev/null 2&gt;&amp;1 if [ $? -eq 0 ];then RESPONSE_TIME=`$PINGCMD -c $PAG_NUM -w 1 $CHECKHOST |head -2 |tail -1|awk &apos;&#123;print $(NF-1)&#125;&apos;|cut -d= -f2` echo $RESPONSE_TIME else echo 0 fi&#125;# 发送数据到zabbixserversend_data() &#123; DATA=`check_ping` $SENDCMD -z $ZABBIXSERVER -s $LOCALHOST -k $ZAX_KEY -o $DATA&#125;while truedo send_data sleep 1done 123其中上报给zabbix server端的数据可以根据自己实际自定义需求上报即可在服务器上将脚本在后台运行即可nohup /bin/bash check_ping.sh &amp; Zabbix web界面配置 添加主机注意：此处的主机名称为脚本中的-LOCALHOST 添加监控项注意：此处类型选择zabbix采集器，键值为脚本中的ZAX_KEY,下面填写允许上报的主机IP 添加图形]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix-使用钉钉发送告警]]></title>
    <url>%2F2018%2F08%2F03%2FZabbix-%E4%BD%BF%E7%94%A8%E9%92%89%E9%92%89%E5%8F%91%E9%80%81%E5%91%8A%E8%AD%A6%2F</url>
    <content type="text"><![CDATA[概述群机器人是钉钉群的高级扩展功能。群机器人可以将第三方服务的信息聚合到群聊中，实现自动化的信息同步。例如：通过聚合GitHub，GitLab等源码管理服务，实现源码更新同步；通过聚合Trello，JIRA等项目协调服务，实现项目信息同步。不仅如此，群机器人支持Webhook协议的自定义接入，支持更多可能性，例如：你可将运维报警提醒通过自定义机器人聚合到钉钉群 钉钉机器人创建 在钉钉内发起群聊或创建群组，机器人管理–&gt;自定义 添加自定义机器人，记录webhook值。 zabbix配置 下载编写好的程序文件到zabbix的/usr/lib/zabbix/alertscripts目录：1https://www.appgao.com/files/192.html 1234解压更改名称为dingding.sh并且chmod +x dingding.sh &amp;&amp; chown zabbix.zabbix dingding.sh创建/tmp/dingding.log 并更改属主为zabbix测试程序是否能正常运行：./dingding.sh -webhook=&quot;https://oapi.dingtalk.com/robot/send?access_token=xxxxxxx 创建媒介类型 设置参数 1234-webhook=https://oapi.dingtalk.com/robot/send?access_token=6e582b91e0f8b2a800b38bb037d4512a8adff9d5b5e8398774e1114492a40c2d-msg=&#123;ALERT.MESSAGE&#125;-url=http://43.x.x.x/zabbix-log=/tmp/dingding.log 配置动作 配置操作和恢复操作，填写发送的用户和发送方式 1234567891011121314151617181920212223242526272829303132333435363738 报警信息： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;root&gt; &lt;from&gt;&#123;HOSTNAME1&#125;&lt;/from&gt; &lt;time&gt;&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;&lt;/time&gt; &lt;level&gt;&#123;TRIGGER.SEVERITY&#125;&lt;/level&gt; &lt;name&gt;&#123;TRIGGER.NAME&#125;&lt;/name&gt; &lt;key&gt;&#123;TRIGGER.KEY1&#125;&lt;/key&gt; &lt;value&gt;&#123;ITEM.VALUE&#125;&lt;/value&gt; &lt;now&gt;&#123;ITEM.LASTVALUE&#125;&lt;/now&gt; &lt;id&gt;&#123;EVENT.ID&#125;&lt;/id&gt; &lt;ip&gt;&#123;HOST.IP&#125;&lt;/ip&gt; &lt;url&gt;http://zabbix.gogen.cn&lt;/url&gt; &lt;age&gt;&#123;EVENT.AGE&#125;&lt;/age&gt; &lt;status&gt;&#123;EVENT.STATUS&#125;&lt;/status&gt;&lt;acknowledgement&gt; &#123;EVENT.ACK.STATUS&#125; &lt;/acknowledgement&gt;&lt;acknowledgementhistory&gt; &#123;EVENT.ACK.HISTORY&#125;&lt;/acknowledgementhistory&gt;&lt;/root&gt;恢复信息：&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;root&gt; &lt;from&gt;&#123;HOSTNAME1&#125;&lt;/from&gt; &lt;time&gt;&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;&lt;/time&gt; &lt;level&gt;&#123;TRIGGER.SEVERITY&#125;&lt;/level&gt; &lt;name&gt;&#123;TRIGGER.NAME&#125;&lt;/name&gt; &lt;key&gt;&#123;TRIGGER.KEY1&#125;&lt;/key&gt; &lt;value&gt;&#123;ITEM.VALUE&#125;&lt;/value&gt; &lt;now&gt;&#123;ITEM.LASTVALUE&#125;&lt;/now&gt; &lt;id&gt;&#123;EVENT.ID&#125;&lt;/id&gt; &lt;ip&gt;&#123;HOST.IP&#125;&lt;/ip&gt; &lt;color&gt;FF4A934A&lt;/color&gt; &lt;url&gt;http://zabbix.gogen.cn&lt;/url&gt; &lt;age&gt;&#123;EVENT.AGE&#125;&lt;/age&gt; &lt;recoveryTime&gt;&#123;EVENT.RECOVERY.DATE&#125; &#123;EVENT.RECOVERY.TIME&#125;&lt;/recoveryTime&gt; &lt;status&gt;OK&lt;/status&gt;&lt;/root&gt;注意：恢复信息说明：&lt;status&gt;OK&lt;/status&gt;默认为OK，这是3.4的zabbix server才这样做，如果你的zabbix server是3.4之前的版本请改为：&lt;status&gt;&#123;EVENT.RECOVERY.STATUS&#125;&lt;/status&gt; 设置用户报警媒介]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix安装graphtree]]></title>
    <url>%2F2018%2F08%2F01%2FZabbix%E5%AE%89%E8%A3%85graphtree%2F</url>
    <content type="text"><![CDATA[1、graphtree的功能1)集中展示所有分组设备2)集中展示一个分组图像3)集中展示一个设备图像4)展示设备下的Application5)展示每个Application下的图像6)展示每个Application下的日志7)对原生无图的监控项进行绘图 (注意问题:在组和主机级别，默认只显示系统配置的graph) 2、graphtree的安装配置 进入zabbix网页数据目录 1234cd /data/www/html/zabbixwget https://raw.githubusercontent.com/OneOaaS/graphtrees/master/graphtree3.2.x.patchyum install -y patchpatch -Np0 &lt; graphtree3.2.x.patch chown -R ${WEB_USER} oneoaas #注意此处的权限，必须和nginx或者apache的用户一致，我用的是apache，则此处为chown -R apache:apache oneoaas 3、graphtree的广告部分修改配置进入graphtree配置文件，进行相关修改 删除广告 1vim oneoaas/templates/graphtree/graphtree.tpl 修改图标 重启httpd服务然后查看效果]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基础用法]]></title>
    <url>%2F2018%2F07%2F01%2FDocker%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Docker Architecture（docker体系结构） Docker daemon 守护进程 containers：容器 images：镜像 Docker client Docker registries 镜像仓库 Docker Images Docker镜像含有启动容器所需要的文件系统及其内容，因此其用于创建并启动docker容器 采用分层构建机制，最底层为bootfs，其之为rootfs bootfs：用于系统引导的文件系统，包括bootloader和kernel，容器启动完成后会被卸载以节约内存资源。 rootfs：位于bootfs上，表现为docker容器的根文件系统。docker中rootfs由内核挂载为“只读”模式，而后通过“联合挂载”技术额外挂载一个“可写”层。 安装docker 依赖环境 64 bits CPU Linux Kernel 3.10+ Docker Daemon systemctl start docker.service Docker Client docker [OPTIONS] COMMAND [arg…] 123456781.下载yum源https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo2.修改baseurl地址底行模式下：%s/https:\/\/download.docker.com\//https:\/\/mirrors.tuna.tsinghua.edu.cn\/docker-ce\//g3.查看程序包 yum repolist4.安装yum install -y docker-ce 配置文件 docker-ce： 配置文件：/etc/docker/daemon.json ,docker启动之前不存在可自行定义创建，启动之前就自动生成。123456789&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;# 阿里云镜像&#123; &quot;registry-mirrors&quot;: [&quot;https://6ucotgoh.mirror.aliyuncs.com&quot;]&#125;# systemctl daemon-reload# systemcctl start docker 容器相关命令 docker image pull nginx:1.14-alpine : 下载镜像 docker run –name web1 -d nginx:1.14-alpine :运行容器 docker inspect web1 ：查看容器信息 docker container stop b1：停止容器 docker kill b1：强制关闭容器 docker rm b1：删除容器 docker rmi busybox：删除镜像 docker ps -a：查看容器状态 docker exec -it redis1 /bin/sh：交互式进入容器,并执行/bin/sh docker logs web1：查看容器日志 docker inspect mysql5.7 -f { {.Mounts} } ：查看容器指定key的键值如果不能自动补全docker命令，安装yum install -y bash-completion，然后退出当前终端重新连接。 Registry(repository and index) Repository 由某特定的docker镜像的所有迭代版本组成的镜像仓库。 一个Registry中可以存多个Repository Repository可分为“顶层仓库”和“用户仓库”，用户仓库名称格式为“用户名/仓库名” 每个仓库可以包含多个Tag(标签)，每个标签对应一个镜像。 Index 维护用户账户、镜像的校验以及公共命名空间的信息。 相当于为Registry提供了一个完成用户认证等功能的检索接口。 Docker Hub本地dckerfile变动发送到github，github推送到dockerhub自动构建为镜像。 Image Repositories：镜像仓库 Automated Builds：自动构建,dockerfile Webhooks：触发构建 Oraganizations：组织，创建工作组 GitHub and Bitbucket Integration：整合github、bitbucket 12# 下载镜像仓库指定镜像docker pull quay.io/coreos/flannel:v0.10.0-amd64 镜像生成方式 Dockerfile 基于容器制作 Docker Hub automated builds 基于容器制作123456789101112131415161、运行一个容器docker run --name b1 -it busybox (-it 交互式，-d 后台运行)2、修改容器如后台运行则可进入容器再操作docker exec -it b1 /bin/shmkdir -p /data/htmlvi /data/html/index.html3、提交镜像docker commit -p b1 (-p 暂停,防止数据有问题)docker image ls (REPOSITORY和TAG都是none,可以在commit时打上标签)4、给刚生成的打标签docker tag c7d5393e1bf2 liyk/boxhttp:v0.1再次打标签docker tag liyk/boxhttp:v0.1 liyk/boxhttp:latest5、运行命令打镜像docker commit -a &quot;Liyk &lt;liyk@anchnet.com&gt;&quot; -c &apos;CMD [&quot;/bin/httpd&quot;,&quot;-f&quot;,&quot;-h&quot;,&quot;/data/html&quot;]&apos; -p b1 liyk/httpd:v0.2 (-a 作者信息,-c 命令,-f 前台运行,-h 文件目录,-p 制作时暂停) 阿里云docker镜像仓库123456789101、控制台开通镜像仓库服务(设置仓库密码)2、创建镜像仓库，地域、命名空间(全局唯一)、仓库名称3、选择代码源，云code、github、私有gitlab等(需要绑定对应账号,提交代码后直接同步到镜像仓库)4、登录阿里云docker registry(用户名是阿里云账号全名，密码为开通服务时设置的密码)docker login --username=livcshiwo registry.cn-shanghai.aliyuncs.com5、从Registry中拉取镜像docker pull registry.cn-shanghai.aliyuncs.com/key1024/test:[镜像版本号]6、将镜像推送到Registrydocker tag [ImageId] registry.cn-shanghai.aliyuncs.com/key1024/test:[镜像版本号]docker push registry.cn-shanghai.aliyuncs.com/key1024/test:[镜像版本号] 镜像分享 镜像仓库：通过push到镜像仓库，需要镜像的再pull到服务器。 镜像打包：打包相关镜像，再scp到其他服务器。123456# 镜像打包docker save -o myimages.gz key1024/test:v0.1 key1024/test:v0.2# copy镜像scp myimages.gz node02:/data# load镜像docker load -i myimages.gz]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器网络]]></title>
    <url>%2F2018%2F07%2F01%2FDocker%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[6个命名空间User、Mount、Pid、UTS、Net、IPC 四种网络模式 host模式：使用–network=host指定 相当于Vmware中的桥接模式，与宿主机在同一个网络中，但没有独立IP地址。 容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。 容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。 bridge模式：使用–net=bridge指定 相当于Vmware中的Nat模式，容器使用独立network Namespace，并连接到docker0虚拟网卡（默认模式）。 通过docker0网桥以及Iptables nat表配置与宿主机通信。 Bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。 none模式：–net=none指定 容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。该模式关闭了容器的网络功能。 在以下两种情况下是有用的：容器并不需要网络（例如只需要写磁盘卷的批处理任务），overlay。 container模式：–net=container:NAME_or_ID指定 指定新创建的容器和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享。 新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。 两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信。 1234# 查看网络模式详情docker network inspect bridge# 查看容器详情docker container inspect myMysql Bridged containers –hostname HOSTNAME选项为容器指定主机名。docker run --rm --net bridge --hostname bbox.test.com busybox:latest nslookup bbox.test.com –dns DNS_SERVER_IP 为容器指定所使用的dns服务器地址。docker run --rm --dns 172.16.0.1 busybox:latest nslookup docker.com –add-host HOSTNAME:IP 为容器指定本地主机名解析项。docker run --rm --dns 172.16.0.1 --add-host &quot;docker.com:172.16.0.10&quot; busybox:latest nslookup docker.com Docker端口暴露 -p 将指定容器端口映射至主机所有地址的一个动态端口 -p : 将容器端口映射至指定的主机端口 -p :: 将指定的容器端口映射至主机指定的动态端口 -p :: 将指定的容器端口映射至主机指定的端口 -P 大写的P，暴露所有端口(指暴露镜像里的全部端口) docker守护进程123456# 远程操作一个容器dockerd守护进程的C/S,其默认仅监听Unix socket格式地址，/var/run/docker.sock;如果使用TCP套接字，/etc/docker/daemon.json: &quot;hosts&quot;: [&quot;tcp://0.0.0.0:2375&quot;,&quot;unix:///var/run/docker.sock&quot;]也可向dockerd直接传递&quot;-H|--host&quot;选项。docker -H 172.17.0.3:2375 image ls 自定义docker012345678910111213# docker0桥网络属性信息：/etc/docker/daemon.json&#123; &quot;bip&quot;: &quot;192.168.1.1/24&quot;, &quot;fixed-cidr&quot;: &quot;10.20.0.0/16&quot;, &quot;fixed-cidr-v6&quot;: &quot;2001:db8::/64&quot;, &quot;mut&quot;: 1500, &quot;default-gateway&quot;: &quot;10.20.1.1&quot;, &quot;default-gateway-v6&quot;: &quot;2001:db8:abcd::89&quot;, &quot;dns&quot;: [&quot;10.20.1.2&quot;,&quot;10.20.1.3&quot;]&#125;# 核心选项为bip，即bridge ip之意，用于指定docker0桥自身的IP地址；其他地址可通过此地址计算得出。# 创建自定义网络docker network create -d bridge --subnet &quot;172.18.0.0/16&quot; --gateway &quot;172.18.0.1&quot; mybr0]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker存储卷]]></title>
    <url>%2F2018%2F07%2F01%2FDocker%E5%AD%98%E5%82%A8%E5%8D%B7%2F</url>
    <content type="text"><![CDATA[Docker镜像由多个只读层叠加而成，启动容器时Docker会加载只读镜像层并在镜像栈顶部添加一个读写层。 如果运行中的容器修改了现有的一个已存在的文件，那该文件将会从读写层下面的只读层复制到读写层，该文件只读版本仍然存在。只是已经被读写层中该文件的副本所隐藏，此即“写时复制(COW)”机制。 Data Volume 关闭并重启容器其数据不受影响，但删除Docker容器则其更改将会全部丢失。 存在的问题： 存储于联合文件系统中，不易于宿主机访问； 容器间数据共享不便 删除容器其数据会丢失。 解决办法：卷(volume) 卷 是容器上的一个或多个目录，此类目录可绕过联合文件系统，与宿主机上的某种目录“绑定(关联)” Volume types Bind-mount volume：宿主机和容器上2个已知路径建立联系，必须双方都指定。 Docker-managed volume：指定容器挂载目录，存储上由docker自动指定挂载目录(/var/lib/docker/vfs/dir/&lt;容器长id&gt;)，只用指定容器目录，但是宿主机上是随机生成的不能指定。 容器中使用volumes Bind-mount volume docker run -it -v HOSTDIR:VOLUMEDIR –name bbox1 busybox docker inspect -f { {.Mounts} } bbox1 # 过滤数据 Docker-managed volume docker run -it –name bbox2 -v /data busybox docker inspect -f { {.Mounts} } bbox2 docker inspect -f { {.NetworkSettings.IPAddress} } bbox2 Sharing volumes 多个容器的卷使用同一个主机目录 docker run -it –name c1 -v /data/volumes/v1:/data busybox docker run -it –name c2 -v /data/volumes/v1:/data busybox 复制使用其他容器的卷，为docker run 命令使用–volumes-from选项 docker run -it –name b1 -v /data/volumes/v2:/data busybox docker run -it –name b2 –volumes-from b1 busybox]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor安装]]></title>
    <url>%2F2018%2F07%2F01%2FHarbor%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Harbor正是一个用于存储Docker镜像的企业级Registry服务。 Harbor核心组件 Proxy：他是一个nginx的前端代理，代理Harbor的registry,UI, token等服务。 db：负责储存用户权限、审计日志、Dockerimage分组信息等数据。 UI：提供图形化界面，帮助用户管理registry上的镜像, 并对用户进行授权。 jobsevice：jobsevice是负责镜像复制工作的，他和registry通信，从一个registry pull镜像然后push到另一个registry，并记录job_log。 Adminserver：是系统的配置管理中心附带检查存储用量，ui和jobserver启动时候回需要加载adminserver的配置。 Registry：镜像仓库，负责存储镜像文件。 Log：为了帮助监控Harbor运行，负责收集其他组件的log，供日后进行分析。 安装和配置安装要求 最小配置：2核、4G内存、40G硬盘 Docker引擎：17.03.0-ce +或更高版本 Docker compose：1.18.0或更高版本 安装Openssl：为Harbor生成证书和密钥 网络端口 端口 协议 描述 443 HPPTS Harbor端口和核心API将接受此端口上的https协议请求，此端口可以在配置文件中更改 8443 HTTPS 只有在启用“公证”时才需要连接到Dock的Docker Content Trust服务，此端口可以在配置文件中更改 80 HTTP Harbor端口和核心API将接受此端口上的http协议请求 安装步骤1.下载安装程序 12wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.1.tgztar -zxvf harbor-online-installer-v1.8.1.tgz 2.配置harbor.yml有两类参数，必需参数和可选参数 必需参数： hostname：目标主机的主机名，用于访问Portal和注册表服务。例如192.168.1.10或reg.yourdomain.com。不要使用localhost或127.0.0.1作为主机名 - 外部客户端需要访问注册表服务！ data_volume：本地存储harbor数据的位置 harbor_admin_password：管理员的初始密码。此密码仅在Harbor首次启动时生效。之后，将忽略此设置，并且应在Portal中设置管理员密码。请注意，默认用户名/密码为admin / Harbor12345。 database：与本地数据库相关的配置，password：用于db_auth的PostgreSQL数据库的root密码。 jobservice：jobservice相关服务，max_job_workers：作业服务中的最大复制工作者数。对于每个映像复制作业，工作程序将存储库的所有标记同步到远程目标。增加此数量可以在系统中执行更多并发复制作业。但是，由于每个工作者都消耗一定量的网络/ CPU / IO资源，请根据主机的硬件资源仔细选择该属性的值。 log：log相关的url； 3.运行install.sh安装并启动Harbor 1234567891011121314151617181920212223# 安装docker1.下载yum源https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo2.修改baseurl地址底行模式下：%s/https:\/\/download.docker.com\//https:\/\/mirrors.tuna.tsinghua.edu.cn\/docker-ce\//g3.查看程序包 yum repolist4.安装yum install -y docker-ce5.配置文件：/etc/docker/daemon.json ,docker启动之前不存在可自行定义创建，启动之前就自动生成。&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;# 阿里云镜像&#123; &quot;registry-mirrors&quot;: [&quot;https://6ucotgoh.mirror.aliyuncs.com&quot;]&#125;# systemctl daemon-reload# systemcctl start docker# 安装docker-compose，如果提前安装过忽略此步骤yum install docker-compose -y # 安装harbor./install.sh]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx日志切割]]></title>
    <url>%2F2018%2F07%2F01%2Fnginx%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2%2F</url>
    <content type="text"><![CDATA[nginx日志自动切割有利于分析查看日志 1.日志配置1234567access.log 记录哪些用户,哪些页面以及用户浏览器,IP等访问信息；error.log 记录服务器错误的日志；配置日志存储路径location / &#123; access_log /usr/local/nginx/logs/access.log; error_log /usr/local/nginx/logs/error.log; &#125; nginx日志配置方法，默认已自动设置 2.日志切割 12345678#!/bin/bash LOGS_PATH=/var/log/nginx/ YESTERDAY=$(date -d yesterday +&quot;%Y%m%d&quot;) #,%Y-%m-%d等于%Fmv $&#123;LOGS_PATH&#125;/access.log $&#123;LOGS_PATH&#125;/logs/access_$&#123;YESTERDAY&#125;.log #mv $&#123;LOGS_PATH&#125;/error.log $&#123;LOGS_PATH&#125;/error_$&#123;YESTERDAY&#125;.log ## 向 Nginx 主进程发送 USR1 信号。USR1 信号是重新打开日志文件 kill -USR1 `cat $&#123;LOGS_PATH&#125;/nginx.pid` #kill -USR1 `cat /var/log/nginx/nginx.pid` 3.设置定时任务 1234crontab -e每天凌晨1分执行01 0 * * * /var/log/nginx/splitLog.sh重启定时任务 补充方式一： nginx cronolog日志分割配置文档，根据下面方法，每分钟分割一次NGINX访问日志。123456781.nginx日志配置 access_log access_log /data/access_log_pipe main;2.先创建一个命名管道mkfifo /www/log/access_log_pipe3.配置cronolog：nohup cat /data/access_log_pipe | /usr/local/sbin/cronolog /data/log/domain.access_%Y%m%d%H%M.log &amp;4.启动Nginx/usr/local/nginx/sbin/nginx 123456789注意：cronolog必须在nginx启动前启动没有安装cronolog的话，需要先安装wget http://cronolog.org/download/cronolog-1.6.2.tar.gztar zxvf cronolog-1.6.2.tar.gzcd cronolog-1.6.2./configure makemake install 方式二： 定时任务中每小时添加定时任务，执行一下脚本，可以实现小时日志分割。12345log_dir=&quot;/var/log/nginx&quot;date_dir=`date +%Y/%m/%d/%H`/bin/mkdir -p $&#123;log_dir&#125;/$&#123;date_dir&#125; &gt; /dev/null 2&gt;&amp;1/bin/mv $&#123;log_dir&#125;/access.log $&#123;log_dir&#125;/$&#123;date_dir&#125;/access.logkill -USR1 `cat /opt/nginx/logs/nginx.pid` 方式三： 使用logrotate做nginx日志轮询12345678910logrotate看名字就知道是专门做日志轮询的，只把任务配置放在/etc/logrotate.d/下，任务就会自动完成，而且无需安装，系统自带，比较推荐使用.vim /etc/logrotate.d/nginx /usr/local/nginx/logs/www.willko.cn.log /usr/local/nginx/logs/nginx_error.log &#123; notifempty daily sharedscripts postrotate /bin/kill -USR1 `/bin/cat /usr/local/nginx/nginx.pid` endscript &#125; 多个日志以空格分开，notifempty 如果日志为空则不做轮询daily 每天执行一次postrotate 日志轮询后执行的脚本这样，每天都会自动轮询，生成nginx.log.1-n]]></content>
      <categories>
        <category>WEB</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下的jq命令处理json数据]]></title>
    <url>%2F2018%2F07%2F01%2FLinux%E4%B8%8B%E7%9A%84jq%E5%91%BD%E4%BB%A4%E5%A4%84%E7%90%86json%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[jq命令可以过滤json格式数据1234安装 https://stedolan.github.io/jq/wget -O jq https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64chmod +x ./jqcp jq /usr/bin 操作演示cat test.json | jq cat 1.json |jq -r &#39;.timeDelay[].duration&#39; 过滤出duration里的数据 1234cat 1.json |jq -r &apos;.timeDelay[] |select (.url | contains(“http:/as/index”))&apos;contains包含这个值的全部匹配cat 1.json |jq -r &apos;.timeDelay[] |select (.url == “http:/as/index”)&apos;== 等于这个值的匹配 有个弊端就是不支持key是数字的 1234567891011# 批量过滤数据脚本#!/bin/bashcat api_url.txt |while read linedo id=$(echo $line |awk -F&apos;/&apos; &apos;&#123;print $2&quot;-&quot;$3&#125;&apos;) if [ $id == &quot;-&quot; ];then id=1 fi# cat 1.json |jq -r &apos;.timeDelay[] |select (.url | contains(&quot;&apos;$line&apos;&quot;))&apos; &gt; /data/script/api_url/$&#123;id&#125;.json cat 1.json |jq -r &apos;.timeDelay[] |select (.url == &quot;&apos;$line&apos;&quot;)&apos; &gt; /data/script/api_url/$&#123;id&#125;.jsondone]]></content>
      <categories>
        <category>system</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker部署lnmp]]></title>
    <url>%2F2018%2F06%2F15%2FDocker%E9%83%A8%E7%BD%B2lnmp%2F</url>
    <content type="text"><![CDATA[安装docker 安装 123456781.下载yum源https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo2.修改baseurl地址底行模式下：%s/https:\/\/download.docker.com\//https:\/\/mirrors.tuna.tsinghua.edu.cn\/docker-ce\//g3.查看程序包 yum repolist4.安装yum install -y docker-ce 配置 12345678910配置文件：/etc/docker/daemon.json ,docker启动之前不存在可自行定义创建，启动之前就自动生成。&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;# 阿里云镜像&#123; &quot;registry-mirrors&quot;: [&quot;https://6ucotgoh.mirror.aliyuncs.com&quot;]&#125;# systemctl daemon-reload# systemcctl start docker 下载需要的镜像123docker pull nginx:alpinedocker pull php:5.6-fpmdocker pull mysql:5.7 相关部署 启动php 1234docker run --name myphp -v /data/nginx/www:/www -d php:5.6-fpm# --name myphp :将容器命名为 myphp# -v /data/nginx/www:/www :将主机中项目的目录/data/nginx/www 挂载到容器的/www# -d :后台运行 添加nginx配置文件 12345678910111213141516171819202122232425262728# 创建 /data/nginx/conf/conf.d 目录：mkdir -p /data/nginx/conf/conf.d在/data/nginx/conf/conf.d/test-php.conf 文件，内容如下：server &#123; listen 80; server_name localhost; location / &#123; root /usr/share/nginx/html; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; location ~ \.php$ &#123; fastcgi_pass 172.17.0.4:9000; # 这里为php容器的ip fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /www/$fastcgi_script_name; include fastcgi_params; &#125;&#125;# fastcgi_pass ：填写php器的ip，使用docker inspect myphp -f `&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;`查看IP 启动nginx 1docker run --name mynginx -p 80:80 -d -v /data/nginx/www:/usr/share/nginx/html:ro -v /data/nginx/conf/conf.d:/etc/nginx/conf.d:ro --link myphp5.6:php nginx:alpine 测试php 12345# 在/data/nginx/www目录下创建test-php.php，代码如下：&lt;?phpecho phpinfo();?&gt;# 浏览器访问测试：IP/test-php.php 启动mysql 1docker run --name myMysql -P -e MYSQL_ROOT_PASSWORD=Xxzx@789 -d mysql 测试php连接mysql 12345678910111213141516171819&lt;?php$dbms=&apos;mysql&apos;; //数据库类型$host=&apos;172.17.0.2&apos;; //数据库主机名,此处写mysql 容器的名字$dbport = &apos;3306&apos;;$dbName=&apos;emlog&apos;; //使用的数据库$user=&apos;root&apos;; //数据库连接用户名$pass=&apos;X****&apos;; //对应的密码$dsn=&quot;$dbms:host=$host;port=$dbport;dbname=$dbName&quot;;try &#123; $dbh = new PDO($dsn, $user, $pass); //初始化一个PDO对象 echo &quot;successful!&lt;br/&gt;&quot;; //你还可以进行一次搜索操作 // foreach ($dbh-&gt;query(&apos;SELECT * from user&apos;) as $row) &#123; // print_r($row); //你可以用 echo($GLOBAL); 来看到这些值 // &#125; $dbh = null;&#125; catch (PDOException $e) &#123; die (&quot;Error!: &quot; . $e-&gt;getMessage() . &quot;&lt;br/&gt;&quot;);&#125; docker容器安装vim等命令 1234567# 进入容器docker exec -it 容器名 /bin/bash（或者/bin/sh）# 查看容器的系统版本cat /etc/issue# 此处以ubuntu为例,先更新软件包，否则安装可能会提示Unable to locate package vimapt-get updateapt-get install vim php连接mysql报错：could not find driver 12345678910# 默认官方php是没添加支持mysql，需自行安装配置1、进入php容器docker exec -it myphp /bin/bash2、进入安装扩展目录下whereis php # 查看在/usr/local/bin下# 安装扩展 ：docker-php-ext-install ./docker-php-ext-install pdo pdo_mysql./docker-php-ext-install mysqli3、查看php.iniphp --ini 安装gd扩展：验证码要用 1234567891011121314151、进入php容器docker exec -it myphp /bin/bash2、安装依赖apt-get install libpng-dev3、进入安装扩展目录./docker-php-ext-install -j$(nproc) gd4、重启php容器，测试gd&lt;?if(extension_loaded(&apos;gd&apos;)) &#123; echo &apos;您可以使用gd&lt;br&gt;&apos;; foreach(gd_info() as $cate=&gt;$value) echo &quot;$cate: $value&lt;br&gt;&quot;;&#125; else echo &apos;没安装gd扩展!&apos;?&gt;]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
</search>
